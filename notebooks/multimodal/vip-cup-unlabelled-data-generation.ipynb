{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport sys\nimport time\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n\ncsv_path_unlabelled = \"/kaggle/input/olives-training-labels/unlabelled_images.csv\"\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-19T13:34:46.866773Z","iopub.execute_input":"2023-08-19T13:34:46.867206Z","iopub.status.idle":"2023-08-19T13:34:46.874553Z","shell.execute_reply.started":"2023-08-19T13:34:46.867170Z","shell.execute_reply":"2023-08-19T13:34:46.873300Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# model.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision\n\nclass ResNet(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=2):\n        super(ResNet, self).__init__()\n        if (name == 'resnet50'):\n            self.encoder = torchvision.models.resnet50(zero_init_residual=True, pretrained = True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(2048, num_classes)\n        else:\n            self.encoder = torchvision.models.resnet18(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(512, num_classes)\n    def forward(self, x):\n\n        return self.fc(self.encoder(x))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:23:27.594131Z","iopub.execute_input":"2023-08-19T13:23:27.594483Z","iopub.status.idle":"2023-08-19T13:23:29.681688Z","shell.execute_reply.started":"2023-08-19T13:23:27.594452Z","shell.execute_reply":"2023-08-19T13:23:29.680402Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# datasets.py\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport os\n\nclass OLIVES(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        return image, bio_tensor\n\n\nclass RECOVERY(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        return image\n\n\n\nclass RECOVERY_TEST(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        return image, bio_tensor\n","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:24:52.524822Z","iopub.execute_input":"2023-08-19T13:24:52.525292Z","iopub.status.idle":"2023-08-19T13:24:52.546425Z","shell.execute_reply.started":"2023-08-19T13:24:52.525255Z","shell.execute_reply":"2023-08-19T13:24:52.544655Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# data_preprocessing.py\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport glob\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef combine_excel(csv_dir):\n    filenames = glob.glob(csv_dir + \"/*.xlsx\")\n    outputxlsx = pd.DataFrame()\n\n    for file in filenames:\n        df = pd.concat(pd.read_excel(file, sheet_name=None), ignore_index=True, sort=False)\n        outputxlsx = outputxlsx.append(df, ignore_index=True)\n\n    outputxlsx.to_csv('test_set_labels.csv',index=False)\n\ndef analyze_dataframe(csv_dir):\n    pass\n\ndef process_images(csv_dir):\n    df = pd.read_csv(csv_dir)\n\n    for i in tqdm(range(0,len(df))):\n        path = df.iloc[i,0]\n        im = Image.open(path).convert('L')\n\n\ndef numpy_submission(sub_dir,np_dir):\n    np_file  = np.load(np_dir)\n    print(len(np_file))\n    sub_dir = pd.read_csv(sub_dir)\n    print(len(sub_dir))\n    for i in range(0,len(sub_dir)):\n        sub_dir.iloc[i,1] = np_file[i,0]\n        sub_dir.iloc[i, 2] = np_file[i, 1]\n        sub_dir.iloc[i, 3] = np_file[i, 2]\n        sub_dir.iloc[i, 4] = np_file[i, 3]\n        sub_dir.iloc[i, 5] = np_file[i, 4]\n        sub_dir.iloc[i, 6] = np_file[i, 5]\n    print(sub_dir.head())\n    sub_dir.to_csv('baseline_result.csv',index=False)\n\n\n\n    #process_images(csv_dir)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:25:21.714080Z","iopub.execute_input":"2023-08-19T13:25:21.714558Z","iopub.status.idle":"2023-08-19T13:25:21.728599Z","shell.execute_reply.started":"2023-08-19T13:25:21.714520Z","shell.execute_reply":"2023-08-19T13:25:21.727132Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\n\nimport math\nimport numpy as np\nimport torch.optim as optim\nimport os\nfrom sklearn.metrics import f1_score\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import random_split\n\nimport torch.nn as nn\ndef set_model(opt, freeze = False):\n\n\n    device = opt.device\n    model = ResNet(name=opt.model,num_classes = opt.ncls)\n    if freeze:\n        model.encoder.requires_grad_(False)\n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n\n    return model, criterion\n\n\n\n\n\n\ndef set_loader(opt):\n    # construct data loader\n    if opt.dataset == 'OLIVES' or opt.dataset == 'RECOVERY':\n        mean = (.1706)\n        std = (.2112)\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n\n    normalize = transforms.Normalize(mean=mean, std=std)\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=224, scale=(0.2, 1.)),\n        transforms.RandomHorizontalFlip(),\n\n        transforms.RandomApply([\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n        ], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n\n    if opt.dataset =='OLIVES':\n        csv_path_train = opt.train_csv_path\n        csv_path_test = opt.test_csv_path\n        data_path_train = opt.train_image_path\n        data_path_test = opt.test_image_path\n        train_dataset = OLIVES(csv_path_train,data_path_train,transforms = train_transform)\n        unlabelled_train_dataset = RECOVERY(csv_path_unlabelled,data_path_train,transforms = val_transform)\n        test_dataset = RECOVERY(csv_path_test,data_path_test,transforms = val_transform)\n        train_dataset, val_dataset = random_split(train_dataset, [0.95, 0.05], generator=torch.Generator().manual_seed(42))\n    else:\n        raise ValueError(opt.dataset)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=opt.batch_size, shuffle=True,\n        num_workers=opt.num_workers, pin_memory=True)\n    \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n    \n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n    \n    unlabelled_train_loader = torch.utils.data.DataLoader(\n        unlabelled_train_dataset, batch_size=opt.batch_size, shuffle=True,\n        num_workers=opt.num_workers, pin_memory=True)\n\n    return train_loader, val_loader, test_loader, unlabelled_train_loader\n\n\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef adjust_learning_rate(args, optimizer, epoch):\n    lr = args.learning_rate\n    if args.cosine:\n        eta_min = lr * (args.lr_decay_rate ** 3)\n        lr = eta_min + (lr - eta_min) * (\n                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n    else:\n        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n        if steps > 0:\n            lr = lr * (args.lr_decay_rate ** steps)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n    if args.warm and epoch <= args.warm_epochs:\n        p = (batch_id + (epoch - 1) * total_batches) / \\\n            (args.warm_epochs * total_batches)\n        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n\ndef set_optimizer(opt, model):\n\n    optimizer = optim.SGD(model.parameters(),\n                          lr=opt.learning_rate,\n                          momentum=opt.momentum,\n                          weight_decay=opt.weight_decay)\n\n\n    return optimizer\n\n\ndef save_model(model, optimizer, opt, epoch, save_file):\n    print('==> Saving...')\n    state = {\n        'opt': opt,\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'epoch': epoch,\n    }\n    torch.save(state, save_file)\n    del state","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:34:22.509177Z","iopub.execute_input":"2023-08-19T13:34:22.509580Z","iopub.status.idle":"2023-08-19T13:34:22.543550Z","shell.execute_reply.started":"2023-08-19T13:34:22.509548Z","shell.execute_reply":"2023-08-19T13:34:22.542230Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# config.py\n\nimport argparse\nimport math\nimport os\n\ndef parse_option(string):\n    parser = argparse.ArgumentParser('argument for training')\n\n    parser.add_argument('--print_freq', type=int, default=10,\n                        help='print frequency')\n    parser.add_argument('--save_freq', type=int, default=50,\n                        help='save frequency')\n    parser.add_argument('--batch_size', type=int, default=128,\n                        help='batch_size')\n    parser.add_argument('--num_workers', type=int, default=8,\n                        help='num of workers to use')\n    parser.add_argument('--epochs', type=int, default=100,\n                        help='number of training epochs')\n    parser.add_argument('--device', type=str, default='cuda:0')\n    # optimization\n    parser.add_argument('--learning_rate', type=float, default=0.05,\n                        help='learning rate')\n    parser.add_argument('--patient_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--cluster_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--lr_decay_epochs', type=str, default='100',\n                        help='where to decay lr, can be a list')\n    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n                        help='decay rate for learning rate')\n    parser.add_argument('--weight_decay', type=float, default=1e-4,\n                        help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='momentum')\n    parser.add_argument('--train_csv_path', type=str, default='train data csv')\n    parser.add_argument('--test_csv_path', type=str, default='test data csv')\n    parser.add_argument('--train_image_path', type=str, default='train data csv')\n    parser.add_argument('--test_image_path', type=str, default='test data csv')\n\n    parser.add_argument('--parallel', type=int, default=1, help='data parallel')\n    parser.add_argument('--ncls', type=int, default=6, help='Number of Classes')\n    # model dataset\n    parser.add_argument('--model', type=str, default='resnet50')\n    parser.add_argument('--dataset', type=str, default='TREX_DME',\n                        choices=[ 'OLIVES'], help='dataset')\n    parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n    parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n    parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n    parser.add_argument('--size', type=int, default=128, help='parameter for RandomResizedCrop')\n\n    # temperature\n    parser.add_argument('--temp', type=float, default=0.07,\n                        help='temperature for loss function')\n\n\n\n    opt = parser.parse_args(string)\n\n    # check if dataset is path that passed required arguments\n    if opt.dataset == 'path':\n        assert opt.data_folder is not None \\\n               and opt.mean is not None \\\n               and opt.std is not None\n\n    # set the path according to the environment\n    if opt.data_folder is None:\n        opt.data_folder = './datasets/'\n    opt.model_path = './save/{}_models'.format(opt.dataset)\n\n    iterations = opt.lr_decay_epochs.split(',')\n    opt.lr_decay_epochs = list([])\n    for it in iterations:\n        opt.lr_decay_epochs.append(int(it))\n\n    opt.model_name = '{}_lr_{}_decay_{}_bsz_{}_temp_{}'. \\\n        format(opt.model, opt.learning_rate,\n               opt.weight_decay, opt.batch_size, opt.temp)\n\n\n    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n    if not os.path.isdir(opt.save_folder):\n        os.makedirs(opt.save_folder)\n\n    return opt","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:34:22.928812Z","iopub.execute_input":"2023-08-19T13:34:22.929251Z","iopub.status.idle":"2023-08-19T13:34:22.952577Z","shell.execute_reply.started":"2023-08-19T13:34:22.929219Z","shell.execute_reply":"2023-08-19T13:34:22.951115Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def train_supervised(train_loader, val_loader, model,criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    device = opt.device\n    end = time.time()\n    correct_predictions = 0\n\n    for idx, (image, bio_tensor) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        images = image.to(device)\n\n        labels = bio_tensor.float()\n\n        labels = labels.to(device)\n        bsz = labels.shape[0]\n\n        # compute loss\n        output = model(images)\n        loss = criterion(output, labels)\n        \n        # Calculate training accuracy\n        predicted_labels = torch.round(torch.sigmoid(output)) \n        correct_predictions += (predicted_labels == labels).sum().item()\n\n        # update metric\n        losses.update(loss.item(), bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % opt.print_freq == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(\n                epoch, idx + 1, len(train_loader)))\n\n            sys.stdout.flush()\n\n    total_values = len(train_loader.dataset) * 6\n    training_accuracy = (correct_predictions / total_values) * 100.0\n    print(f\"Training Accuracy: {training_accuracy:.2f}%\")\n    print(losses.avg)\n    sample_evaluation(val_loader, model, opt)\n    \n    return losses.avg\n","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:34:23.371310Z","iopub.execute_input":"2023-08-19T13:34:23.371737Z","iopub.status.idle":"2023-08-19T13:34:23.385076Z","shell.execute_reply.started":"2023-08-19T13:34:23.371699Z","shell.execute_reply":"2023-08-19T13:34:23.383592Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def submission_generate(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    with torch.no_grad():\n        for idx, image in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n\n            # forward\n            output = model(images)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n\n\n    out_submisison = np.array(out_list)\n    np.save('output',out_submisison)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:34:23.870026Z","iopub.execute_input":"2023-08-19T13:34:23.870433Z","iopub.status.idle":"2023-08-19T13:34:23.879126Z","shell.execute_reply.started":"2023-08-19T13:34:23.870402Z","shell.execute_reply":"2023-08-19T13:34:23.877867Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def sample_evaluation(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    label_list = []\n    correct_count = 0\n    total_count = 0\n\n    with torch.no_grad():\n        for idx, (image,bio_tensor) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            labels = bio_tensor.float().to(device)\n\n            labels = labels.float()\n\n            #label_list.append(labels.squeeze().detach().cpu().numpy())\n            # forward\n            output = model(images)\n            output = torch.round(torch.sigmoid(output))\n            #out_list.append(output.squeeze().detach().cpu().numpy())\n            \n            correct_count += (labels == output).sum().item()\n            total_count += len(labels) * 6\n        \n    print(\"Validation accuracy: \",(correct_count / total_count) * 100, \"%\")\n\n    #label_array = np.array(label_list)\n    #out_array = np.array(out_list)\n    #f = f1_score(label_array,out_array,average='macro')\n    #print(\"Validation F1:\", f)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:34:24.363490Z","iopub.execute_input":"2023-08-19T13:34:24.364644Z","iopub.status.idle":"2023-08-19T13:34:24.372757Z","shell.execute_reply.started":"2023-08-19T13:34:24.364601Z","shell.execute_reply":"2023-08-19T13:34:24.371838Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def sample_evaluation_full(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    label_list = []\n    correct_count = 0\n    total_count = 0\n\n    with torch.no_grad():\n        for idx, (image,bio_tensor) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            labels = bio_tensor.float().to(device)\n\n            labels = labels.float()\n\n            label_list.append(labels.squeeze().detach().cpu().numpy())\n            # forward\n            output = model(images)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n            \n            correct_count += (labels == output).sum().item()\n            total_count += len(labels) * 6\n        \n    print(\"Validation accuracy: \",(correct_count / total_count) * 100, \"%\")\n\n    label_array = np.array(label_list)\n    out_array = np.array(out_list)\n    f = f1_score(label_array,out_array,average='macro')\n    print(\"Validation F1:\", f)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:34:24.860117Z","iopub.execute_input":"2023-08-19T13:34:24.860519Z","iopub.status.idle":"2023-08-19T13:34:24.872354Z","shell.execute_reply.started":"2023-08-19T13:34:24.860487Z","shell.execute_reply":"2023-08-19T13:34:24.871031Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"args = args = ['--batch_size', '64', '--model', \"resnet50\", '--dataset', 'OLIVES', '--epochs', '100', '--device', 'cuda:0', '--train_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TRAIN/OLIVES', '--test_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/', '--test_csv_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv', '--train_csv_path', '/kaggle/input/olives-training-labels/Training_Biomarker_Data.csv']\nopt = parse_option(args)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:34:35.644586Z","iopub.execute_input":"2023-08-19T13:34:35.644996Z","iopub.status.idle":"2023-08-19T13:34:35.654339Z","shell.execute_reply.started":"2023-08-19T13:34:35.644965Z","shell.execute_reply":"2023-08-19T13:34:35.652863Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# build data loader\ntrain_loader, val_loader, test_loader, unlabelled_train_loader = set_loader(opt)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:35:05.432990Z","iopub.execute_input":"2023-08-19T13:35:05.433448Z","iopub.status.idle":"2023-08-19T13:35:05.653609Z","shell.execute_reply.started":"2023-08-19T13:35:05.433413Z","shell.execute_reply":"2023-08-19T13:35:05.652391Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"# build model and criterion\nmodel, criterion = set_model(opt)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:35:52.529194Z","iopub.execute_input":"2023-08-19T13:35:52.529827Z","iopub.status.idle":"2023-08-19T13:35:55.656828Z","shell.execute_reply.started":"2023-08-19T13:35:52.529783Z","shell.execute_reply":"2023-08-19T13:35:55.655032Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:01<00:00, 69.9MB/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# build model and criterion\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model, criterion \u001b[38;5;241m=\u001b[39m \u001b[43mset_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[8], line 22\u001b[0m, in \u001b[0;36mset_model\u001b[0;34m(opt, freeze)\u001b[0m\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m criterion \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, criterion\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"],"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error"}]},{"cell_type":"code","source":"# build optimizer\noptimizer = set_optimizer(opt, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training routine\n# model.encoder.requires_grad_(True)\nfor epoch in range(1, 5 + 1):\n    train_supervised(train_loader, val_loader, model, criterion, optimizer, epoch, opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_file = os.path.join(opt.save_folder, 'last.pth')\nsave_model(model, optimizer, opt, opt.epochs, save_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation\nsample_evaluation_full(val_loader, model, opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_generate(test_loader, model, opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = np.load('/kaggle/working/output.npy')\nsubmission = pd.read_csv(\"/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv\")\nsubmission.iloc[:, 1:] = output\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\n\ndef save_all_image_paths():\n    filepathre = re.compile(r'/TRAIN/OLIVES(.*)')\n    files = []\n    count = 0\n    for dirname, test, filenames in os.walk('/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TRAIN/OLIVES'):\n        for filename in filenames:\n            if filename[-3:] != \"tif\": \n                continue\n            filename = filepathre.findall(os.path.join(dirname, filename))[0]\n            files.append(filename)\n            count += 1\n            if count % 5000 == 0:\n                print(count, filename)\n                \n    df = pd.DataFrame({\"FilePath\": files})\n    df.to_csv(\"unlabelled_images.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}