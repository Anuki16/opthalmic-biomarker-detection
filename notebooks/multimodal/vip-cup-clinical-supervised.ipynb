{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport sys\nimport time\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport re\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\nclinical_path_train = \"/kaggle/input/olives-training-labels/Training_Unlabeled_Clinical_Data.csv\"\nclinical_path_test = \"/kaggle/input/olives-training-labels/test_with_clinical.csv\"\nmin_cst, max_cst = 150, 723\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-30T19:59:55.505864Z","iopub.execute_input":"2023-08-30T19:59:55.506376Z","iopub.status.idle":"2023-08-30T19:59:56.455731Z","shell.execute_reply.started":"2023-08-30T19:59:55.506340Z","shell.execute_reply":"2023-08-30T19:59:56.454810Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# model.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision\n\nclass ResNet(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=2):\n        super(ResNet, self).__init__()\n        if (name == 'resnet50'):\n            self.encoder = torchvision.models.resnet50(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(2048, num_classes)\n        else:\n            self.encoder = torchvision.models.resnet18(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(512, num_classes)\n    def forward(self, x):\n\n        return self.fc(self.encoder(x))\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-08-30T19:59:56.458024Z","iopub.execute_input":"2023-08-30T19:59:56.458377Z","iopub.status.idle":"2023-08-30T20:00:00.479213Z","shell.execute_reply.started":"2023-08-30T19:59:56.458345Z","shell.execute_reply":"2023-08-30T20:00:00.478284Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class ResNetModified(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=2):\n        super(ResNetModified, self).__init__()\n        if (name == 'resnet50'):\n            self.encoder = torchvision.models.resnet50(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(9, 9), stride=(2, 2), padding=(4, 4), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(2048, num_classes)\n        else:\n            self.encoder = torchvision.models.resnet18(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(9, 9), stride=(2, 2), padding=(4, 4), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(512, num_classes)\n    def forward(self, x):\n\n        return self.fc(self.encoder(x))","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-08-30T20:00:00.484066Z","iopub.execute_input":"2023-08-30T20:00:00.488838Z","iopub.status.idle":"2023-08-30T20:00:00.525345Z","shell.execute_reply.started":"2023-08-30T20:00:00.488799Z","shell.execute_reply":"2023-08-30T20:00:00.521405Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# For clinical labels\nclass MLP(nn.Module):\n    # define model elements\n    def __init__(self, n_inputs, n_outputs):\n        super(MLP, self).__init__()\n        self.layer = nn.Sequential(\n\n            # for clinical label only baseline\n            nn.Linear(n_inputs, n_inputs),\n            nn.LeakyReLU(0.2, True),\n            nn.Linear(n_inputs, n_outputs)\n        )\n\n    # forward propagate input\n    def forward(self, X):\n        X = self.layer(X)\n        return X","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-08-30T20:00:00.538020Z","iopub.execute_input":"2023-08-30T20:00:00.545473Z","iopub.status.idle":"2023-08-30T20:00:00.559002Z","shell.execute_reply.started":"2023-08-30T20:00:00.545436Z","shell.execute_reply":"2023-08-30T20:00:00.557965Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass HybridModel(nn.Module):\n    def __init__(self, name, num_classes, num_clinical):\n        super(HybridModel, self).__init__()\n\n        if (name == 'resnet50'):\n            self.encoder = torchvision.models.resnet50(zero_init_residual=True, pretrained = True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.encoder_fc = nn.Linear(2048, 10)\n        else:\n            self.encoder = torchvision.models.resnet18(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.encoder_fc = nn.Linear(512, 10)\n        \n        # Combine the outputs of CNN and numeric layers\n        combined_features_dim = 10 + num_clinical\n        self.fc = nn.Sequential(\n            nn.LeakyReLU(0.2, True),\n            nn.Linear(combined_features_dim, num_classes)\n        )\n\n    def forward(self, image_data, numeric_data):\n        # Process image data using CNN layers\n        image_features = self.encoder_fc(self.encoder(image_data))\n\n        # Concatenate image and numeric features\n        combined_features = torch.cat((image_features.view(image_features.size(0), -1), numeric_data), dim=1)\n\n        # Final prediction using combined features\n        output = self.fc(combined_features)\n        return output\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-08-30T20:00:00.563695Z","iopub.execute_input":"2023-08-30T20:00:00.564450Z","iopub.status.idle":"2023-08-30T20:00:00.582719Z","shell.execute_reply.started":"2023-08-30T20:00:00.564418Z","shell.execute_reply":"2023-08-30T20:00:00.581590Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=2):\n        super(Encoder, self).__init__()\n        self.encoder = torchvision.models.resnet50(pretrained=True, zero_init_residual=True)\n        self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n        self.encoder.fc = nn.Identity()\n        self.fc = nn.Linear(2048, 512)\n\n    def forward(self, x):\n\n        return self.fc(self.encoder(x))\n\n\nclass ProjectionHead(nn.Module):\n    def __init__(self,in_dim,feature_dim):\n        super(ProjectionHead, self).__init__()\n        \n        self.g1 = nn.Sequential(nn.Linear(in_dim, 1024, bias=False),\n                               nn.BatchNorm1d(1024),\n                               nn.ReLU(inplace=True)\n                               )\n        self.g2 = nn.Sequential(nn.Linear(1024, 512, bias=False),\n                                nn.BatchNorm1d(512),\n                                nn.ReLU(inplace=True)\n                                )\n        self.g3=nn.Linear(512, feature_dim, bias=True)\n    def forward(self, x):\n        # print(x.shape)\n        x = torch.flatten(x, start_dim=1, end_dim=- 1) \n        x = self.g1(x)\n        x = self.g2(x)\n        x = self.g3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:00:00.587056Z","iopub.execute_input":"2023-08-30T20:00:00.587462Z","iopub.status.idle":"2023-08-30T20:00:00.605173Z","shell.execute_reply.started":"2023-08-30T20:00:00.587429Z","shell.execute_reply":"2023-08-30T20:00:00.603942Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def prepare_clinical_data_train(bio_data_path, clinical_data_path):\n    \n    df = pd.read_csv(bio_data_path)\n    clinical_df = pd.read_excel(clinical_data_path)\n    filtered_df = pd.DataFrame(columns = clinical_df.columns)\n    file_names = list(df.iloc[:, 0])\n\n    for i in range(len(clinical_df)):\n        if clinical_df.iloc[i].File_Path in file_names:\n            filtered_df.loc[len(filtered_df.index)] = clinical_df.loc[i]\n    print(\"Created clinical data frame\")\n\n    new_df = df.copy()\n    file_names = list(filtered_df.iloc[:, 0])\n    for i in range(len(df)):\n        if df.iloc[i,0] not in file_names:\n            new_df = new_df.drop(i)\n    print(\"Filtered unlabelled data\")\n\n    assert len(new_df) == len(filtered_df), (len(new_df), len(filtered_df))\n\n    filtered_df = filtered_df.sort_values(by = [filtered_df.columns[0]])\n    new_df = new_df.sort_values(by = [new_df.columns[0]])\n    assert new_df.iloc[0, 0] == filtered_df.iloc[0, 0], (new_df.iloc[0, 0], filtered_df.iloc[0, 0])\n    \n    bcva_mean = np.mean(filtered_df.BCVA)\n    bcva_std = np.std(filtered_df.BCVA)\n    cst_mean = np.mean(filtered_df.CST)\n    cst_std = np.std(filtered_df.CST)\n    filtered_df.BCVA = (filtered_df.BCVA - bcva_mean) / bcva_std\n    filtered_df.CST = (filtered_df.CST - cst_mean) / cst_std\n    \n    return new_df, filtered_df","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:00:00.607132Z","iopub.execute_input":"2023-08-30T20:00:00.608949Z","iopub.status.idle":"2023-08-30T20:00:00.620368Z","shell.execute_reply.started":"2023-08-30T20:00:00.608916Z","shell.execute_reply":"2023-08-30T20:00:00.619550Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def prepare_clinical_data_test(bio_data_path, clinical_data_path):\n    \n    df = pd.read_csv(bio_data_path)\n    clinical_df = pd.read_excel(clinical_data_path)\n    regex = re.compile(r\"RECOVERY/OCT/([0-9\\-]+)/W([0-9]+)\")\n    \n    cols = [\"Patient ID\", \"BCVA\", \"CST\"]\n    id_to_row = dict(zip(clinical_df['Patient ID'], clinical_df.index))\n    for col in cols:\n        df[col] = np.nan\n        \n    clinical_df.iloc[28, [3, 4]] = clinical_df.iloc[28, [1, 2]]\n\n    for i in range(len(df)):\n        patient_id, week = regex.findall(df.iloc[i, 0])[0]\n        week = int(week)\n        idx = id_to_row[patient_id]\n        df.loc[i, cols[0]] = clinical_df.iloc[idx, 0]\n        if week < 50:   # Use test result that is closest to the week of the OCT\n            df.loc[i, cols[1]] = clinical_df.loc[idx, \"Week 0 BCVA\"]\n            df.loc[i, cols[2]] = clinical_df.loc[idx, \"Week 0 CST\"]\n        else:\n            df.loc[i, cols[1]] = clinical_df.loc[idx, \"Final Week BCVA\"]\n            df.loc[i, cols[2]] = clinical_df.loc[idx, \"Final Week CST\"]\n\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:00:00.621778Z","iopub.execute_input":"2023-08-30T20:00:00.622371Z","iopub.status.idle":"2023-08-30T20:00:00.637339Z","shell.execute_reply.started":"2023-08-30T20:00:00.622340Z","shell.execute_reply":"2023-08-30T20:00:00.636215Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"bcva_mean = np.mean(df.BCVA)\nbcva_std = np.std(df.BCVA)\ncst_mean = np.mean(df.CST)\ncst_std = np.std(df.CST)\ndf.BCVA = (df.BCVA - bcva_mean) / bcva_std\ndf.CST = (df.CST - cst_mean) / cst_std","metadata":{}},{"cell_type":"code","source":"def scale_labels(df):\n    df.BCVA = df.BCVA / 100\n    df.CST = (df.CST - min_cst) / (max_cst - min_cst)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:07:32.553067Z","iopub.execute_input":"2023-08-30T20:07:32.553424Z","iopub.status.idle":"2023-08-30T20:07:32.558541Z","shell.execute_reply.started":"2023-08-30T20:07:32.553393Z","shell.execute_reply":"2023-08-30T20:07:32.557452Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# datasets.py\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport os\n\nclass OLIVES(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        return image, bio_tensor\n    \nclass OLIVES_CLINICAL(data.Dataset):\n    def __init__(self, bio_df, clinical_df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df, self.clinical_df = prepare_clinical_data_train(bio_df, clinical_df)\n        \n    def __len__(self):\n        return len(self.clinical_df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        \n        bcva = self.clinical_df.iloc[idx, 1]\n        cst = self.clinical_df.iloc[idx, 2]\n        clinical = torch.tensor([bcva, cst])\n        \n        return image, clinical, bio_tensor\n\n\n\n\nclass RECOVERY(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        return image\n    \nclass RECOVERY_CLINICAL(data.Dataset):\n    def __init__(self,df, clinical_df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = prepare_clinical_data_test(df, clinical_df)\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        \n        bcva = self.df.loc[idx, \"BCVA\"]\n        cst = self.df.loc[idx, \"CST\"]\n        clinical = torch.tensor([bcva, cst])\n        \n        return image, clinical\n\nclass CLINICAL(data.Dataset):\n    def __init__(self, df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n        scale_labels(self.df)\n        if len(self.df) > 1000:\n            self.df = self.df.iloc[:1000]\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        \n        bcva = self.df.loc[idx, \"BCVA\"]\n        cst = self.df.loc[idx, \"CST\"]\n        clinical = torch.tensor([bcva, cst])\n        \n        return image, clinical\n\nclass RECOVERY_TEST(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        return image, bio_tensor\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:07:08.995469Z","iopub.execute_input":"2023-08-30T20:07:08.995848Z","iopub.status.idle":"2023-08-30T20:07:09.028091Z","shell.execute_reply.started":"2023-08-30T20:07:08.995817Z","shell.execute_reply":"2023-08-30T20:07:09.026855Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# data_preprocessing.py\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport glob\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef combine_excel(csv_dir):\n    filenames = glob.glob(csv_dir + \"/*.xlsx\")\n    outputxlsx = pd.DataFrame()\n\n    for file in filenames:\n        df = pd.concat(pd.read_excel(file, sheet_name=None), ignore_index=True, sort=False)\n        outputxlsx = outputxlsx.append(df, ignore_index=True)\n\n    outputxlsx.to_csv('test_set_labels.csv',index=False)\n\ndef analyze_dataframe(csv_dir):\n    pass\n\ndef process_images(csv_dir):\n    df = pd.read_csv(csv_dir)\n\n    for i in tqdm(range(0,len(df))):\n        path = df.iloc[i,0]\n        im = Image.open(path).convert('L')\n\n\ndef numpy_submission(sub_dir,np_dir):\n    np_file  = np.load(np_dir)\n    print(len(np_file))\n    sub_dir = pd.read_csv(sub_dir)\n    print(len(sub_dir))\n    for i in range(0,len(sub_dir)):\n        sub_dir.iloc[i,1] = np_file[i,0]\n        sub_dir.iloc[i, 2] = np_file[i, 1]\n        sub_dir.iloc[i, 3] = np_file[i, 2]\n        sub_dir.iloc[i, 4] = np_file[i, 3]\n        sub_dir.iloc[i, 5] = np_file[i, 4]\n        sub_dir.iloc[i, 6] = np_file[i, 5]\n    print(sub_dir.head())\n    sub_dir.to_csv('baseline_result.csv',index=False)\n\n\n\n    #process_images(csv_dir)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-08-30T20:00:00.711379Z","iopub.execute_input":"2023-08-30T20:00:00.711902Z","iopub.status.idle":"2023-08-30T20:00:00.730833Z","shell.execute_reply.started":"2023-08-30T20:00:00.711870Z","shell.execute_reply":"2023-08-30T20:00:00.729846Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def load_model(model, load_file):\n    print('==> Loading...')\n    checkpoint = torch.load(load_file)\n    model.load_state_dict(checkpoint['model'])\n    return model\n\nload_file = '/kaggle/input/models/diff_kernel.pth'  # Path to the saved model file","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:00:00.732301Z","iopub.execute_input":"2023-08-30T20:00:00.733296Z","iopub.status.idle":"2023-08-30T20:00:00.742819Z","shell.execute_reply.started":"2023-08-30T20:00:00.733264Z","shell.execute_reply":"2023-08-30T20:00:00.741646Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\n\nimport math\nimport numpy as np\nimport torch.optim as optim\nimport os\nfrom sklearn.metrics import f1_score\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import random_split\n\nimport torch.nn as nn\ndef set_model(opt):\n\n\n    device = opt.device\n    model = ResNet(name=opt.model,num_classes = opt.ncls)\n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n\n    return model, criterion\n\ndef set_model_hybrid(opt):\n\n    device = opt.device\n    model = HybridModel(name=opt.model,num_classes = opt.ncls, num_clinical = 2)\n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n\n    return model, criterion\n\ndef set_model_hybrid_pretrained(opt, encoder_type = ResNet):\n\n    device = opt.device\n    pre_model = encoder_type(name=opt.model,num_classes = opt.ncls)\n    criterion = torch.nn.BCEWithLogitsLoss()\n    \n    pre_model = load_model(pre_model, load_file)\n    model = HybridPretrainedModel(pre_model.encoder, opt.model, opt.ncls, 2)\n\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n\n    return model, criterion\n\ndef set_model_both(opt):\n\n    device = opt.device\n    model1 = ResNet(name=opt.model,num_classes = opt.ncls)\n    criterion1 = torch.nn.BCEWithLogitsLoss()\n    model1 = model1.to(device)\n    criterion1 = criterion1.to(device)\n\n    model2 = MLP(n_inputs = 2, n_outputs = opt.ncls)\n    criterion2 = torch.nn.BCEWithLogitsLoss()\n    model2 = model2.to(device)\n    criterion2 = criterion2.to(device)\n\n    return model1, criterion1, model2, criterion2\n\n\n\n\ndef set_loader(opt):\n    # construct data loader\n    if opt.dataset == 'OLIVES' or opt.dataset == 'RECOVERY':\n        mean = (.1706)\n        std = (.2112)\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n\n    normalize = transforms.Normalize(mean=mean, std=std)\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=224, scale=(0.2, 1.)),\n        transforms.RandomHorizontalFlip(),\n\n        transforms.RandomApply([\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n        ], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n\n    if opt.dataset =='OLIVES':\n        data_path_train = opt.train_image_path\n        data_path_test = opt.test_image_path\n        train_dataset = OLIVES(csv_path_train,data_path_train,transforms = train_transform)\n        unlabelled_train_dataset = RECOVERY(csv_path_unlabelled,data_path_train,transforms = val_transform)\n        val_dataset = OLIVES(csv_path_valid,data_path_train,transforms = val_transform)\n        test_dataset = RECOVERY(csv_path_test,data_path_test,transforms = val_transform)\n    else:\n        raise ValueError(opt.dataset)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=opt.batch_size, shuffle=True,\n        num_workers=opt.num_workers, pin_memory=True)\n    \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n    \n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n\n    return train_loader, val_loader, test_loader\n\n\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef adjust_learning_rate(args, optimizer, epoch):\n    lr = args.learning_rate\n    if args.cosine:\n        eta_min = lr * (args.lr_decay_rate ** 3)\n        lr = eta_min + (lr - eta_min) * (\n                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n    else:\n        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n        if steps > 0:\n            lr = lr * (args.lr_decay_rate ** steps)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n    if args.warm and epoch <= args.warm_epochs:\n        p = (batch_id + (epoch - 1) * total_batches) / \\\n            (args.warm_epochs * total_batches)\n        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n\ndef set_optimizer(opt, model):\n\n    optimizer = optim.SGD(model.parameters(),\n                          lr=opt.learning_rate,\n                          momentum=opt.momentum,\n                          weight_decay=opt.weight_decay)\n\n\n    return optimizer\n\ndef set_optimizer_both(opt, model1, model2):\n\n    optimizer1 = optim.SGD(model1.parameters(),\n                          lr=opt.learning_rate,\n                          momentum=opt.momentum,\n                          weight_decay=opt.weight_decay)\n\n    optimizer2 = optim.Adam(model2.parameters(), lr=0.1, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n\n\n    return optimizer1, optimizer2\n\ndef save_model(model, optimizer, opt, epoch, save_file):\n    print('==> Saving...')\n    state = {\n        'opt': opt,\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'epoch': epoch,\n    }\n    torch.save(state, save_file)\n    del state","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:00:00.745179Z","iopub.execute_input":"2023-08-30T20:00:00.745512Z","iopub.status.idle":"2023-08-30T20:00:00.794803Z","shell.execute_reply.started":"2023-08-30T20:00:00.745482Z","shell.execute_reply":"2023-08-30T20:00:00.793527Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def set_loader_clinical(opt):\n    # construct data loader\n    if opt.dataset == 'OLIVES' or opt.dataset == 'RECOVERY':\n        mean = (.1706)\n        std = (.2112)\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n\n    normalize = transforms.Normalize(mean=mean, std=std)\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=224, scale=(0.8, 1.)),\n\n        transforms.RandomApply([\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n        ], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n\n    if opt.dataset =='OLIVES':\n        csv_path_train = opt.train_csv_path\n        csv_path_test = opt.test_csv_path\n        data_path_train = opt.train_image_path\n        data_path_test = opt.test_image_path\n        train_dataset = CLINICAL(clinical_path_train, data_path_train,transforms = train_transform)\n        test_dataset = CLINICAL(clinical_path_test, data_path_test,transforms = val_transform)\n        \n    else:\n        raise ValueError(opt.dataset)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=opt.batch_size, shuffle=True,\n        num_workers=opt.num_workers, pin_memory=True)\n    \n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n\n    return train_loader, test_loader\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:00:00.798771Z","iopub.execute_input":"2023-08-30T20:00:00.801676Z","iopub.status.idle":"2023-08-30T20:00:00.814938Z","shell.execute_reply.started":"2023-08-30T20:00:00.801642Z","shell.execute_reply":"2023-08-30T20:00:00.813950Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# config.py\n\nimport argparse\nimport math\nimport os\n\ndef parse_option(string):\n    parser = argparse.ArgumentParser('argument for training')\n\n    parser.add_argument('--print_freq', type=int, default=10,\n                        help='print frequency')\n    parser.add_argument('--save_freq', type=int, default=50,\n                        help='save frequency')\n    parser.add_argument('--batch_size', type=int, default=128,\n                        help='batch_size')\n    parser.add_argument('--num_workers', type=int, default=8,\n                        help='num of workers to use')\n    parser.add_argument('--epochs', type=int, default=100,\n                        help='number of training epochs')\n    parser.add_argument('--device', type=str, default='cuda:0')\n    # optimization\n    parser.add_argument('--learning_rate', type=float, default=0.05,\n                        help='learning rate')\n    parser.add_argument('--patient_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--cluster_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--lr_decay_epochs', type=str, default='100',\n                        help='where to decay lr, can be a list')\n    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n                        help='decay rate for learning rate')\n    parser.add_argument('--weight_decay', type=float, default=1e-4,\n                        help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='momentum')\n    parser.add_argument('--train_csv_path', type=str, default='train data csv')\n    parser.add_argument('--test_csv_path', type=str, default='test data csv')\n    parser.add_argument('--train_image_path', type=str, default='train data csv')\n    parser.add_argument('--test_image_path', type=str, default='test data csv')\n\n    parser.add_argument('--parallel', type=int, default=1, help='data parallel')\n    parser.add_argument('--ncls', type=int, default=6, help='Number of Classes')\n    # model dataset\n    parser.add_argument('--model', type=str, default='resnet50')\n    parser.add_argument('--dataset', type=str, default='TREX_DME',\n                        choices=[ 'OLIVES'], help='dataset')\n    parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n    parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n    parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n    parser.add_argument('--size', type=int, default=128, help='parameter for RandomResizedCrop')\n\n    # temperature\n    parser.add_argument('--temp', type=float, default=0.07,\n                        help='temperature for loss function')\n\n\n\n    opt = parser.parse_args(string)\n\n    # check if dataset is path that passed required arguments\n    if opt.dataset == 'path':\n        assert opt.data_folder is not None \\\n               and opt.mean is not None \\\n               and opt.std is not None\n\n    # set the path according to the environment\n    if opt.data_folder is None:\n        opt.data_folder = './datasets/'\n    opt.model_path = './save/{}_models'.format(opt.dataset)\n\n    iterations = opt.lr_decay_epochs.split(',')\n    opt.lr_decay_epochs = list([])\n    for it in iterations:\n        opt.lr_decay_epochs.append(int(it))\n\n    opt.model_name = '{}_lr_{}_decay_{}_bsz_{}_temp_{}'. \\\n        format(opt.model, opt.learning_rate,\n               opt.weight_decay, opt.batch_size, opt.temp)\n\n\n    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n    if not os.path.isdir(opt.save_folder):\n        os.makedirs(opt.save_folder)\n\n    return opt","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-08-30T20:00:00.816770Z","iopub.execute_input":"2023-08-30T20:00:00.817458Z","iopub.status.idle":"2023-08-30T20:00:00.841963Z","shell.execute_reply.started":"2023-08-30T20:00:00.817425Z","shell.execute_reply":"2023-08-30T20:00:00.840927Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def train_supervised_clinical(train_loader, model, criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    device = opt.device\n    end = time.time()\n\n    for idx, (image, clinical) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        images = image.to(device)\n        labels = clinical.to(torch.float32).to(device)\n        bsz = labels.shape[0]\n\n        # compute loss\n        output = model(images)\n        loss = criterion(output, labels)\n\n        # update metric\n        losses.update(loss.item(), bsz)\n        #print(loss.item(), output.mean(axis = 0), labels.mean(axis = 0))\n        \n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % opt.print_freq == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(\n                epoch, idx + 1, len(train_loader)))\n\n            sys.stdout.flush()\n    print(\"Training MSE Loss:\", losses.avg)\n    \n    return losses.avg\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:22:05.571658Z","iopub.execute_input":"2023-08-30T20:22:05.572011Z","iopub.status.idle":"2023-08-30T20:22:05.583353Z","shell.execute_reply.started":"2023-08-30T20:22:05.571983Z","shell.execute_reply":"2023-08-30T20:22:05.581740Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def train_supervised(train_loader, model,criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    device = opt.device\n    end = time.time()\n    correct_predictions = 0\n\n    for idx, (image, bio_tensor) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        images = image.to(device)\n\n        labels = bio_tensor.float()\n\n        labels = labels.to(device)\n        bsz = labels.shape[0]\n\n        # compute loss\n        output = model(images)\n        loss = criterion(output, labels)\n        \n        # Calculate training accuracy\n        predicted_labels = torch.round(torch.sigmoid(output)) \n        correct_predictions += (predicted_labels == labels).sum().item()\n\n        # update metric\n        losses.update(loss.item(), bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % opt.print_freq == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(\n                epoch, idx + 1, len(train_loader)))\n\n            sys.stdout.flush()\n\n    total_values = len(train_loader.dataset) * 6\n    training_accuracy = (correct_predictions / total_values) * 100.0\n    print(f\"Training Accuracy: {training_accuracy:.2f}%\")\n    \n    return losses.avg\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-08-30T20:00:00.858703Z","iopub.execute_input":"2023-08-30T20:00:00.859515Z","iopub.status.idle":"2023-08-30T20:00:00.873024Z","shell.execute_reply.started":"2023-08-30T20:00:00.859433Z","shell.execute_reply":"2023-08-30T20:00:00.872094Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def train_supervised_hybrid(train_loader, val_loader, model, criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    device = opt.device\n    end = time.time()\n    correct_predictions = 0\n\n    for idx, (image, clinical, bio_tensor) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        images = image.to(torch.float32).to(device)\n        clinical = clinical.to(torch.float32).to(device)\n\n        labels = bio_tensor.to(torch.float32)\n\n        labels = labels.to(device)\n        bsz = labels.shape[0]\n\n        # compute loss\n        output = model(images, clinical)\n        loss = criterion(output, labels)\n        \n        # Calculate training accuracy\n        predicted_labels = torch.round(torch.sigmoid(output)) \n        correct_predictions += (predicted_labels == labels).sum().item()\n\n        # update metric\n        losses.update(loss.item(), bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % opt.print_freq == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(\n                epoch, idx + 1, len(train_loader)))\n\n            sys.stdout.flush()\n\n    total_values = len(train_loader.dataset) * 6\n    training_accuracy = (correct_predictions / total_values) * 100.0\n    print(f\"Training Accuracy: {training_accuracy:.2f}%\")\n    print(losses.avg)\n    \n    return losses.avg\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-08-30T20:00:00.874761Z","iopub.execute_input":"2023-08-30T20:00:00.875604Z","iopub.status.idle":"2023-08-30T20:00:00.889930Z","shell.execute_reply.started":"2023-08-30T20:00:00.875548Z","shell.execute_reply":"2023-08-30T20:00:00.888906Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def train_supervised_multimodal(train_loader, model1, model2, criterion1, criterion2, optimizer1, optimizer2, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model1.train()\n    model2.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    device = opt.device\n    end = time.time()\n    correct_predictions = 0\n\n    for idx, (image, clinical, bio_tensor) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n        \n        model1.zero_grad()\n        optimizer1.zero_grad()\n        model2.zero_grad()\n        optimizer2.zero_grad()\n\n        images = image.to(device)\n        clinical = clinical.to(device)\n\n        labels = bio_tensor.float()\n\n        labels = labels.to(device)\n        bsz = labels.shape[0]\n\n        # compute loss\n        output1 = model1(images)\n        loss1 = criterion1(output1, labels)\n        \n        output2 = model2(clinical)\n        loss2 = criterion2(output2, labels)\n        \n        # Calculate training accuracy\n        pred1 = torch.round(torch.sigmoid(output1)) \n        pred2 = torch.round(torch.sigmoid(output2)) \n        correct_predictions += (pred1 == labels).sum().item()\n\n        comp_criterion = nn.MSELoss()\n\n        loss3 = comp_criterion(output1[pred2 == labels], output2[pred2 == labels])\n        if torch.isnan(loss3):\n            loss3 = comp_criterion(output1[pred1 == labels], output2[pred1 == labels])\n        if torch.isnan(loss3):\n            loss = loss1 + loss2\n        else:\n            loss = loss1 + loss2 + loss3\n        # update metric\n        losses.update(loss.item(), bsz)\n\n        # SGD\n        loss.backward()\n        optimizer1.step()\n        optimizer2.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % opt.print_freq == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(\n                epoch, idx + 1, len(train_loader)))\n\n            sys.stdout.flush()\n\n    total_values = len(train_loader.dataset) * 6\n    training_accuracy = (correct_predictions / total_values) * 100.0\n    print(f\"Training Accuracy: {training_accuracy:.2f}%\")\n    print(losses.avg)\n    \n    \n    return losses.avg","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-08-30T20:00:00.891854Z","iopub.execute_input":"2023-08-30T20:00:00.892656Z","iopub.status.idle":"2023-08-30T20:00:00.910614Z","shell.execute_reply.started":"2023-08-30T20:00:00.892624Z","shell.execute_reply":"2023-08-30T20:00:00.909662Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def submission_generate(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    with torch.no_grad():\n        for idx, (image, clinical) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            clinical = clinical.to(torch.float32).to(device)\n\n            # forward\n            output = model(images, clinical)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n\n\n    out_submisison = np.array(out_list)\n    np.save('output',out_submisison)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:00:00.912159Z","iopub.execute_input":"2023-08-30T20:00:00.912970Z","iopub.status.idle":"2023-08-30T20:00:00.922238Z","shell.execute_reply.started":"2023-08-30T20:00:00.912938Z","shell.execute_reply":"2023-08-30T20:00:00.921275Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def validation_clinical(val_loader, model, criterion, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    losses = AverageMeter()\n\n    with torch.no_grad():\n        for idx, (image, clinical) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            labels = clinical.to(torch.float32).to(device)\n            bsz = labels.shape[0]\n\n            output = model(images)\n            loss = criterion(output, labels)\n            \n            losses.update(loss.item(), bsz)\n    print(\"Test MSE Loss:\", losses.avg)\n    return losses.avg\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:00:00.923806Z","iopub.execute_input":"2023-08-30T20:00:00.924458Z","iopub.status.idle":"2023-08-30T20:00:00.944669Z","shell.execute_reply.started":"2023-08-30T20:00:00.924427Z","shell.execute_reply":"2023-08-30T20:00:00.943564Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def sample_evaluation_acc(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    correct_count = 0\n    total_count = 0\n\n    with torch.no_grad():\n        for idx, (image,clinical, bio_tensor) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            clinical = clinical.to(torch.float32).to(device)\n            labels = bio_tensor.float().to(device)\n\n            labels = labels.float()\n\n            output = model(images, clinical)\n            output = torch.round(torch.sigmoid(output))\n            \n            correct_count += (labels == output).sum().item()\n        \n    total_count = len(val_loader.dataset) * 6\n    val_acc = (correct_count / total_count) * 100\n    print(val_acc, \"%\")\n    return val_acc\n","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:00:00.949533Z","iopub.execute_input":"2023-08-30T20:00:00.951272Z","iopub.status.idle":"2023-08-30T20:00:00.971078Z","shell.execute_reply.started":"2023-08-30T20:00:00.951236Z","shell.execute_reply":"2023-08-30T20:00:00.970163Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def sample_evaluation(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    label_list = []\n    correct_count = 0\n    total_count = 0\n\n    with torch.no_grad():\n        for idx, (image,clinical, bio_tensor) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            clinical = clinical.to(torch.float32).to(device)\n            labels = bio_tensor.float().to(device)\n\n            labels = labels.float()\n\n            label_list.append(labels.squeeze().detach().cpu().numpy())\n            # forward\n            output = model(images, clinical)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n            \n            correct_count += (labels == output).sum().item()\n        \n    total_count = len(val_loader.dataset) * 6\n    print((correct_count / total_count) * 100, \"%\")\n\n    label_array = np.array(label_list)\n    out_array = np.array(out_list)\n    f = f1_score(label_array,out_array,average='macro')\n    print(f)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:00:00.978179Z","iopub.execute_input":"2023-08-30T20:00:00.979625Z","iopub.status.idle":"2023-08-30T20:00:01.020723Z","shell.execute_reply.started":"2023-08-30T20:00:00.979591Z","shell.execute_reply":"2023-08-30T20:00:01.019835Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = args = ['--batch_size', '64', '--model', \"resnet50\", '--dataset', 'OLIVES', '--epochs', '100', '--device', 'cuda:0', '--train_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TRAIN/OLIVES', '--test_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/', '--test_csv_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv', '--train_csv_path', '/kaggle/input/olives-training-labels/Training_Biomarker_Data.csv']\nopt = parse_option(args)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:00:01.030773Z","iopub.execute_input":"2023-08-30T20:00:01.032238Z","iopub.status.idle":"2023-08-30T20:00:01.057191Z","shell.execute_reply.started":"2023-08-30T20:00:01.032206Z","shell.execute_reply":"2023-08-30T20:00:01.056046Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# build data loader\nopt.batch_size = 10\ntrain_loader, test_loader = set_loader_clinical(opt)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:16:51.896556Z","iopub.execute_input":"2023-08-30T20:16:51.896955Z","iopub.status.idle":"2023-08-30T20:16:52.021798Z","shell.execute_reply.started":"2023-08-30T20:16:51.896926Z","shell.execute_reply":"2023-08-30T20:16:52.020833Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"train_loader.dataset[0]","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:09:51.128718Z","iopub.execute_input":"2023-08-30T20:09:51.129669Z","iopub.status.idle":"2023-08-30T20:09:51.143062Z","shell.execute_reply.started":"2023-08-30T20:09:51.129628Z","shell.execute_reply":"2023-08-30T20:09:51.142025Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"(tensor([[[-0.8078, -0.8078, -0.8078,  ..., -0.6778, -0.6221, -0.6407],\n          [-0.8078, -0.8078, -0.8078,  ..., -0.6221, -0.4178, -0.5107],\n          [-0.8078, -0.8078, -0.8078,  ..., -0.5664, -0.4178, -0.6778],\n          ...,\n          [-0.7706, -0.5849, -0.4735,  ..., -0.8078, -0.8078, -0.8078],\n          [-0.7521, -0.7706, -0.7892,  ..., -0.8078, -0.8078, -0.8078],\n          [-0.7892, -0.8078, -0.8078,  ..., -0.8078, -0.8078, -0.8078]]]),\n tensor([0.7300, 0.6405], dtype=torch.float64))"},"metadata":{}}]},{"cell_type":"code","source":"net = Encoder()\nprojection_head = ProjectionHead(512, 2)\nclinical_model = nn.Sequential(net, projection_head)\ncriterion = nn.MSELoss()\n\nclinical_model = clinical_model.to(opt.device)\ncriterion = criterion.to(opt.device)\n\noptimizer = set_optimizer(opt, clinical_model)\noptimizer = torch.optim.Adam(clinical_model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:25:53.607048Z","iopub.execute_input":"2023-08-30T20:25:53.607399Z","iopub.status.idle":"2023-08-30T20:25:54.139631Z","shell.execute_reply.started":"2023-08-30T20:25:53.607369Z","shell.execute_reply":"2023-08-30T20:25:54.138607Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"epochs = 10\nopt.print_freq = 10\n\nwith torch.autograd.set_detect_anomaly(True):\n    for epoch in range(1, epochs + 1):\n        train_supervised_clinical(train_loader, clinical_model, criterion, optimizer, epoch, opt)\n        #loss = validation_clinical(test_loader, clinical_model, criterion, opt)\n        scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:42:01.231497Z","iopub.execute_input":"2023-08-30T20:42:01.232460Z","iopub.status.idle":"2023-08-30T20:45:18.053938Z","shell.execute_reply.started":"2023-08-30T20:42:01.232427Z","shell.execute_reply":"2023-08-30T20:45:18.052720Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Train: [1][10/100]\t\nTrain: [1][20/100]\t\nTrain: [1][30/100]\t\nTrain: [1][40/100]\t\nTrain: [1][50/100]\t\nTrain: [1][60/100]\t\nTrain: [1][70/100]\t\nTrain: [1][80/100]\t\nTrain: [1][90/100]\t\nTrain: [1][100/100]\t\nTraining MSE Loss: 0.002808182584121823\nTrain: [2][10/100]\t\nTrain: [2][20/100]\t\nTrain: [2][30/100]\t\nTrain: [2][40/100]\t\nTrain: [2][50/100]\t\nTrain: [2][60/100]\t\nTrain: [2][70/100]\t\nTrain: [2][80/100]\t\nTrain: [2][90/100]\t\nTrain: [2][100/100]\t\nTraining MSE Loss: 0.0025240870914421975\nTrain: [3][10/100]\t\nTrain: [3][20/100]\t\nTrain: [3][30/100]\t\nTrain: [3][40/100]\t\nTrain: [3][50/100]\t\nTrain: [3][60/100]\t\nTrain: [3][70/100]\t\nTrain: [3][80/100]\t\nTrain: [3][90/100]\t\nTrain: [3][100/100]\t\nTraining MSE Loss: 0.0028604165452998133\nTrain: [4][10/100]\t\nTrain: [4][20/100]\t\nTrain: [4][30/100]\t\nTrain: [4][40/100]\t\nTrain: [4][50/100]\t\nTrain: [4][60/100]\t\nTrain: [4][70/100]\t\nTrain: [4][80/100]\t\nTrain: [4][90/100]\t\nTrain: [4][100/100]\t\nTraining MSE Loss: 0.0028761536750243977\nTrain: [5][10/100]\t\nTrain: [5][20/100]\t\nTrain: [5][30/100]\t\nTrain: [5][40/100]\t\nTrain: [5][50/100]\t\nTrain: [5][60/100]\t\nTrain: [5][70/100]\t\nTrain: [5][80/100]\t\nTrain: [5][90/100]\t\nTrain: [5][100/100]\t\nTraining MSE Loss: 0.0028635080822277813\nTrain: [6][10/100]\t\nTrain: [6][20/100]\t\nTrain: [6][30/100]\t\nTrain: [6][40/100]\t\nTrain: [6][50/100]\t\nTrain: [6][60/100]\t\nTrain: [6][70/100]\t\nTrain: [6][80/100]\t\nTrain: [6][90/100]\t\nTrain: [6][100/100]\t\nTraining MSE Loss: 0.002494159189518541\nTrain: [7][10/100]\t\nTrain: [7][20/100]\t\nTrain: [7][30/100]\t\nTrain: [7][40/100]\t\nTrain: [7][50/100]\t\nTrain: [7][60/100]\t\nTrain: [7][70/100]\t\nTrain: [7][80/100]\t\nTrain: [7][90/100]\t\nTrain: [7][100/100]\t\nTraining MSE Loss: 0.0027032612316543235\nTrain: [8][10/100]\t\nTrain: [8][20/100]\t\nTrain: [8][30/100]\t\nTrain: [8][40/100]\t\nTrain: [8][50/100]\t\nTrain: [8][60/100]\t\nTrain: [8][70/100]\t\nTrain: [8][80/100]\t\nTrain: [8][90/100]\t\nTrain: [8][100/100]\t\nTraining MSE Loss: 0.0021760880961664954\nTrain: [9][10/100]\t\nTrain: [9][20/100]\t\nTrain: [9][30/100]\t\nTrain: [9][40/100]\t\nTrain: [9][50/100]\t\nTrain: [9][60/100]\t\nTrain: [9][70/100]\t\nTrain: [9][80/100]\t\nTrain: [9][90/100]\t\nTrain: [9][100/100]\t\nTraining MSE Loss: 0.002452706628537271\nTrain: [10][10/100]\t\nTrain: [10][20/100]\t\nTrain: [10][30/100]\t\nTrain: [10][40/100]\t\nTrain: [10][50/100]\t\nTrain: [10][60/100]\t\nTrain: [10][70/100]\t\nTrain: [10][80/100]\t\nTrain: [10][90/100]\t\nTrain: [10][100/100]\t\nTraining MSE Loss: 0.002639914279279765\n","output_type":"stream"}]},{"cell_type":"code","source":"clinical_model.eval()\nimage, label = train_loader.dataset[13]\nimage = image.reshape((1, ) + image.shape).to(opt.device)\nclinical_model(image), label","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:45:27.276871Z","iopub.execute_input":"2023-08-30T20:45:27.277257Z","iopub.status.idle":"2023-08-30T20:45:27.308469Z","shell.execute_reply.started":"2023-08-30T20:45:27.277225Z","shell.execute_reply":"2023-08-30T20:45:27.307545Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"(tensor([[0.6921, 0.6226]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([0.7300, 0.6405], dtype=torch.float64))"},"metadata":{}}]},{"cell_type":"code","source":"clinical_model.train()\nimage,label = next(iter(train_loader))\nimage = image.to(opt.device)\nclinical_model(image)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:08:55.654054Z","iopub.execute_input":"2023-08-30T20:08:55.654415Z","iopub.status.idle":"2023-08-30T20:08:59.202735Z","shell.execute_reply.started":"2023-08-30T20:08:55.654384Z","shell.execute_reply":"2023-08-30T20:08:59.201499Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"tensor([[0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8065, 3.7140],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109],\n        [0.8049, 3.7109]], device='cuda:0', grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"label.mean(axis = 0)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T20:09:16.993294Z","iopub.execute_input":"2023-08-30T20:09:16.993688Z","iopub.status.idle":"2023-08-30T20:09:17.002974Z","shell.execute_reply.started":"2023-08-30T20:09:16.993657Z","shell.execute_reply":"2023-08-30T20:09:17.001877Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"tensor([0.7958, 3.4642], dtype=torch.float64)"},"metadata":{}}]},{"cell_type":"code","source":"((label - output) ** 2).mean()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T18:47:23.392182Z","iopub.execute_input":"2023-08-30T18:47:23.392575Z","iopub.status.idle":"2023-08-30T18:47:23.404432Z","shell.execute_reply.started":"2023-08-30T18:47:23.392540Z","shell.execute_reply":"2023-08-30T18:47:23.403336Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"tensor(0.2867, device='cuda:0', grad_fn=<MeanBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T18:54:54.253993Z","iopub.execute_input":"2023-08-30T18:54:54.254388Z","iopub.status.idle":"2023-08-30T18:54:54.635917Z","shell.execute_reply.started":"2023-08-30T18:54:54.254350Z","shell.execute_reply":"2023-08-30T18:54:54.634638Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# build data loader\ntrain_loader, val_loader, test_loader = set_loader(opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build model and criterion\nmodel, criterion = set_model_hybrid(opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build optimizer\noptimizer = set_optimizer(opt, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt.learning_rate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training routine\nbest = 0\nfor epoch in range(1, 80 + 1):\n    train_supervised_hybrid(train_loader, val_loader, model, criterion, optimizer, epoch, opt)\n    val_acc = sample_evaluation_acc(val_loader, model, opt)\n    if val_acc > best:\n        best = val_acc\n        save_model(model, optimizer, opt, opt.epochs, \"/kaggle/working/last_model.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_file = os.path.join(opt.save_folder, 'last2.pth')\nsave_model(model, optimizer, opt, opt.epochs, save_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation\nsample_evaluation(val_loader, model, opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_generate(test_loader, model, opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = np.load('/kaggle/working/output.npy')\nsubmission = pd.read_csv(\"/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv\")\nsubmission.iloc[:, 1:] = output\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_loader.dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, clin, bio = val_loader.dataset[25]\nimage = image.reshape((1, ) + image.shape).to(opt.device)\nclin = clin.reshape((1, ) + clin.shape).to(opt.device)\noutput = model(image, clin)\ntorch.round(torch.sigmoid(output)), bio, clin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clin = torch.tensor([-0.5, 0.6]).reshape(clin.shape).to(opt.device)\noutput = model(image, clin)\ntorch.round(torch.sigmoid(output))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TRAIN/Training_Biomarker_Data.csv\")\nclinical_df = pd.read_excel(\"/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TRAIN/Training_Unlabeled_Clinical_Data.xlsx\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clinical_df.iloc[1].File_Path in df.iloc[:, 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_df = pd.DataFrame(columns = clinical_df.columns)\nfile_names = list(df.iloc[:, 0])\n\nfor i in range(len(clinical_df)):\n    if clinical_df.iloc[i].File_Path in file_names:\n        filtered_df.loc[len(filtered_df.index)] = clinical_df.loc[i]\n        \nfiltered_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sort_values(by = df.columns[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, cln, bio = train_loader.dataset[0]\nimage.to(torch.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/olives-training-labels/Training_Unlabeled_Clinical_Data.csv\")\n\ndf.BCVA.isnull()","metadata":{"execution":{"iopub.status.busy":"2023-08-30T19:56:05.040772Z","iopub.execute_input":"2023-08-30T19:56:05.041599Z","iopub.status.idle":"2023-08-30T19:56:05.207666Z","shell.execute_reply.started":"2023-08-30T19:56:05.041553Z","shell.execute_reply":"2023-08-30T19:56:05.206666Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n78131    False\n78132    False\n78133    False\n78134    False\n78135    False\nName: BCVA, Length: 78136, dtype: bool"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(len(df)):\n    if np.isnan(df.BCVA[i]) or np.isnan(df.CST[i]):\n        print(i)","metadata":{"execution":{"iopub.status.busy":"2023-08-30T19:04:34.926137Z","iopub.execute_input":"2023-08-30T19:04:34.926879Z","iopub.status.idle":"2023-08-30T19:04:37.708544Z","shell.execute_reply.started":"2023-08-30T19:04:34.926844Z","shell.execute_reply":"2023-08-30T19:04:37.707444Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"df.iloc[:10]","metadata":{"execution":{"iopub.status.busy":"2023-08-30T19:56:14.889506Z","iopub.execute_input":"2023-08-30T19:56:14.890443Z","iopub.status.idle":"2023-08-30T19:56:14.907055Z","shell.execute_reply.started":"2023-08-30T19:56:14.890392Z","shell.execute_reply":"2023-08-30T19:56:14.905974Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"                                           File_Path  BCVA  CST  Eye_ID  \\\n0  /TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...    73  517      12   \n1  /TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...    73  517      12   \n2  /TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...    73  517      12   \n3  /TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...    73  517      12   \n4  /TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...    73  517      12   \n5  /TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...    73  517      12   \n6  /TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...    73  517      12   \n7  /TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...    73  517      12   \n8  /TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...    73  517      12   \n9  /TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...    73  517      12   \n\n   Patient_ID  \n0         234  \n1         234  \n2         234  \n3         234  \n4         234  \n5         234  \n6         234  \n7         234  \n8         234  \n9         234  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File_Path</th>\n      <th>BCVA</th>\n      <th>CST</th>\n      <th>Eye_ID</th>\n      <th>Patient_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...</td>\n      <td>73</td>\n      <td>517</td>\n      <td>12</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...</td>\n      <td>73</td>\n      <td>517</td>\n      <td>12</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...</td>\n      <td>73</td>\n      <td>517</td>\n      <td>12</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...</td>\n      <td>73</td>\n      <td>517</td>\n      <td>12</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...</td>\n      <td>73</td>\n      <td>517</td>\n      <td>12</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>/TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...</td>\n      <td>73</td>\n      <td>517</td>\n      <td>12</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>/TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...</td>\n      <td>73</td>\n      <td>517</td>\n      <td>12</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>/TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...</td>\n      <td>73</td>\n      <td>517</td>\n      <td>12</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>/TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...</td>\n      <td>73</td>\n      <td>517</td>\n      <td>12</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>/TREX_DME/TREX DME/GILA/0234GOD/V1/OD/TREXW_00...</td>\n      <td>73</td>\n      <td>517</td>\n      <td>12</td>\n      <td>234</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"clinical_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nregex = re.compile(r\"RECOVERY/OCT/([0-9\\-]+)/W([0-9]+)/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_to_row = dict(zip(clinical_df['Patient ID'], clinical_df.index))\ncols = [\"Patient ID\", \"BCVA\", \"CST\"]\nfor col in cols:\n    df[col] = np.nan\n\nfor i in range(len(df)):\n    patient_id, week = regex.findall(df.iloc[i, 0])[0]\n    week = int(week)\n    idx = id_to_row[patient_id]\n    df.loc[i, cols[0]] = clinical_df.iloc[idx, 0]\n    if week < 50:\n        df.loc[i, cols[1]] = clinical_df.loc[idx, \"Week 0 BCVA\"]\n        df.loc[i, cols[2]] = clinical_df.loc[idx, \"Week 0 CST\"]\n    else:\n        df.loc[i, cols[1]] = clinical_df.loc[idx, \"Final Week BCVA\"]\n        df.loc[i, cols[2]] = clinical_df.loc[idx, \"Final Week CST\"]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"((df.BCVA - df.BCVA.mean())/df.BCVA.std()).unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[i, clinical_df.columns] = clinical_df.iloc[idx]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clinical_df.iloc[28, [3, 4]] = clinical_df.iloc[28, [1, 2]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clinical_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\narr = np.array([1,2,3,4,5,6,7])\ntest = np.array([1,2,3,1,2])\ntest[test == np.array([1,5,3,1,1])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bio_df = pd.read_csv(opt.test_csv_path)\nclin_df = pd.read_excel(clinical_path_test)\n\ndf = prepare_clinical_data_test(opt.test_csv_path, clinical_path_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop([\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\"], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"test_with_clinical.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}