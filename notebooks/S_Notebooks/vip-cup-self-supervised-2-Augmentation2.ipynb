{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport sys\nimport time\nimport numpy as np\nfrom sklearn.metrics import f1_score\nfrom skimage import exposure\nimport random\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-metric-learning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision\n\nclass ResNet(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=2):\n        super(ResNet, self).__init__()\n        if (name == 'resnet50'):\n            self.encoder = torchvision.models.resnet50(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(2048, num_classes)\n        else:\n            self.encoder = torchvision.models.resnet18(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(512, num_classes)\n    def forward(self, x):\n\n        return self.fc(self.encoder(x))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Prj_Head(nn.Module):\n    def __init__(self,in_dim,feature_dim):\n        super(Prj_Head, self).__init__()\n        \n        self.g1 = nn.Sequential(nn.Linear(in_dim, 1024, bias=False),\n                               nn.BatchNorm1d(1024),\n                               nn.ReLU(inplace=True)\n                               )\n        self.g2 = nn.Sequential(nn.Linear(1024, 512, bias=False),\n                                nn.BatchNorm1d(512),\n                                nn.ReLU(inplace=True)\n                                )\n        self.g3=nn.Linear(512, feature_dim, bias=True)\n    def forward(self, x):\n        # print(x.shape)\n        x = torch.flatten(x, start_dim=1, end_dim=- 1) \n        x = self.g1(x)\n        x = self.g2(x)\n        x = self.g3(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encdr(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=2):\n        super(Encdr, self).__init__()\n        self.encoder = torchvision.models.resnet50(pretrained=True, zero_init_residual=True)\n        self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n        self.encoder.fc = nn.Identity()\n        self.fc = nn.Linear(2048, 512)\n\n    def forward(self, x):\n\n        return self.fc(self.encoder(x))\n    \n    def add_feature(self):\n        self.fc1=nn.Linear(512,2)\n\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datasets.py\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport os\n\nclass OLIVES(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        return image, bio_tensor\n\n\n\n\nclass RECOVERY(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        return image\n\n\n\nclass RECOVERY_TEST(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        return image, bio_tensor\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_preprocessing.py\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport glob\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef combine_excel(csv_dir):\n    filenames = glob.glob(csv_dir + \"/*.xlsx\")\n    outputxlsx = pd.DataFrame()\n\n    for file in filenames:\n        df = pd.concat(pd.read_excel(file, sheet_name=None), ignore_index=True, sort=False)\n        outputxlsx = outputxlsx.append(df, ignore_index=True)\n\n    outputxlsx.to_csv('test_set_labels.csv',index=False)\n\ndef analyze_dataframe(csv_dir):\n    pass\n\ndef process_images(csv_dir):\n    df = pd.read_csv(csv_dir)\n\n    for i in tqdm(range(0,len(df))):\n        path = df.iloc[i,0]\n        im = Image.open(path).convert('L')\n\n\ndef numpy_submission(sub_dir,np_dir):\n    np_file  = np.load(np_dir)\n    print(len(np_file))\n    sub_dir = pd.read_csv(sub_dir)\n    print(len(sub_dir))\n    for i in range(0,len(sub_dir)):\n        sub_dir.iloc[i,1] = np_file[i,0]\n        sub_dir.iloc[i, 2] = np_file[i, 1]\n        sub_dir.iloc[i, 3] = np_file[i, 2]\n        sub_dir.iloc[i, 4] = np_file[i, 3]\n        sub_dir.iloc[i, 5] = np_file[i, 4]\n        sub_dir.iloc[i, 6] = np_file[i, 5]\n    print(sub_dir.head())\n    sub_dir.to_csv('baseline_result.csv',index=False)\n\n\n\n    #process_images(csv_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\n\nimport math\nimport numpy as np\nimport torch.optim as optim\nimport os\nfrom sklearn.metrics import f1_score\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import random_split, Subset\n\nimport torch.nn as nn\ndef set_model(opt, freeze = False):\n\n\n    device = opt.device\n    model = ResNet(name=opt.model,num_classes = opt.ncls)\n    if freeze:\n        model.encoder.requires_grad_(False)\n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n\n    return model, criterion\n\n\n# model for self supervised training\n\ndef set_model_st(opt,Net):\n\n\n    device = opt.device\n    #model = Encdr(name=opt.model,num_classes = opt.ncls)\n    model = nn.Sequential(\n    Net, \n    nn.Linear(512, 1024, bias=False),\n    nn.BatchNorm1d(1024),\n    nn.ReLU(inplace=True),\n    nn.Linear(1024, 512, bias=False),\n    nn.BatchNorm1d(512),\n    nn.ReLU(inplace=True),\n    nn.Linear(512, 6, bias=True))\n    \n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n\n    return model, criterion\n\n\n\n\ndef set_loader(opt):\n    # construct data loader\n    if opt.dataset == 'OLIVES' or opt.dataset == 'RECOVERY':\n        mean = (.1706)\n        std = (.2112)\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n\n    normalize = transforms.Normalize(mean=mean, std=std)\n\n    train_transform = transforms.Compose([\n         transforms.RandomResizedCrop(size=224, scale=(0.2, 1.)),\n         transforms.RandomHorizontalFlip(),\n        \n        \n        #################################################\n        #transforms.Resize((224,224)),\n        \n#         transforms.RandomApply([\n#             transforms.Lambda(lambda x: x ** random.uniform(0.8, 1.2))\n#         ], p=0.8),\n    \n##########################################################\n        \n        transforms.RandomApply([\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.4)\n        ], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n\n    if opt.dataset =='OLIVES':\n        csv_path_train = opt.train_csv_path\n        csv_path_test = opt.test_csv_path\n        data_path_train = opt.train_image_path\n        data_path_test = opt.test_image_path\n        train_dataset = OLIVES(csv_path_train,data_path_train,transforms = train_transform)\n        unlabelled_train_dataset = RECOVERY(csv_path_unlabelled,data_path_train,transforms = val_transform)\n        test_dataset = RECOVERY(csv_path_test,data_path_test,transforms = val_transform)\n        train_dataset, val_dataset = random_split(train_dataset, [0.95, 0.05], generator=torch.Generator().manual_seed(42))\n        unlabelled_train_dataset = Subset(unlabelled_train_dataset, range(unlabel_count))\n    else:\n        raise ValueError(opt.dataset)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=opt.batch_size, shuffle=True,\n        num_workers=opt.num_workers, pin_memory=True)\n    \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n    \n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n    \n    unlabelled_train_loader = torch.utils.data.DataLoader(\n        unlabelled_train_dataset, batch_size=opt.batch_size, shuffle=True,\n        num_workers=opt.num_workers, pin_memory=True)\n\n    return train_loader, val_loader, test_loader, unlabelled_train_loader\n\n\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef adjust_learning_rate(args, optimizer, epoch):\n    lr = args.learning_rate\n    if args.cosine:\n        eta_min = lr * (args.lr_decay_rate ** 3)\n        lr = eta_min + (lr - eta_min) * (\n                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n    else:\n        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n        if steps > 0:\n            lr = lr * (args.lr_decay_rate ** steps)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n    if args.warm and epoch <= args.warm_epochs:\n        p = (batch_id + (epoch - 1) * total_batches) / \\\n            (args.warm_epochs * total_batches)\n        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n\ndef set_optimizer(opt, model):\n\n    optimizer = optim.SGD(model.parameters(),\n                          lr=opt.learning_rate,\n                          momentum=opt.momentum,\n                          weight_decay=opt.weight_decay)\n    #optimizer = torch.optim.Adam(model.parameters(), lr=opt.learning_rate)\n\n    return optimizer\n\n\ndef save_model(model, optimizer, opt, epoch, save_file):\n    print('==> Saving...')\n    state = {\n        'opt': opt,\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'epoch': epoch,\n    }\n    torch.save(state, save_file)\n    del state","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#----------------------------------------------------------------------------------------------------\n# Augmentations\nfrom torchvision import transforms\nclass GaussianBlur(object):\n    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n    \"\"\"Borrowed from MoCo implementation\"\"\"\n\n    def __init__(self, sigma=[.1, 2.]):\n        self.sigma = sigma\n\n    def __call__(self, x):\n        sigma = random.uniform(self.sigma[0], self.sigma[1])\n        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n        return x\n    \nclass FixedRandomRotation:\n    \"\"\"Rotate by one of the given angles.\"\"\"\n    def __init__(self, angles):\n        self.angles = angles\n\n    def __call__(self, x):\n        angle = random.choice(self.angles)\n        return transforms.functional.rotate(x, angle)\n    \n    \n  \ndef torchvision_transforms(eval=False, aug=None):\n\n    trans = []\n\n    if aug[\"resize\"]:\n        trans.append(transforms.Resize(aug[\"resize\"]))\n\n    if aug[\"randcrop\"] and aug[\"scale\"] and not eval:\n        trans.append(transforms.RandomResizedCrop(aug[\"randcrop\"], scale=aug[\"scale\"]))\n\n    if aug[\"randcrop\"] and eval:\n        trans.append(transforms.CenterCrop(aug[\"randcrop\"]))\n\n    if aug[\"flip\"] and not eval:\n        trans.append(transforms.RandomHorizontalFlip(p=0.5))\n        trans.append(transforms.RandomVerticalFlip(p=0.5))\n\n    if aug[\"jitter_d\"] and not eval:\n        trans.append(transforms.RandomApply(\n            [transforms.ColorJitter(0.8*aug[\"jitter_d\"], 0.8*aug[\"jitter_d\"], 0.8*aug[\"jitter_d\"], 0.2*aug[\"jitter_d\"])],\n             p=aug[\"jitter_p\"]))\n\n    if aug[\"gaussian_blur\"] and not eval:\n        trans.append(transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1,.2))], p=aug[\"gaussian_blur\"]))\n\n    if aug[\"rotation\"] and not eval:\n        # rotation_transform = FixedRandomRotation(angles=[0, 90, 180, 270])\n        trans.append(FixedRandomRotation(angles=[0, 10, 20, 30]))\n  \n\n    trans = transforms.Compose(trans)\n   \n    return trans\naug = {\"resize\":0,\n    \"randcrop\":224,     \n      \"scale\": (0.25, 1.0),\n      \"flip\":0,       # change 1 to 0\n      \"jitter_d\":0.3,\n       \"jitter_p\":0.3,\n       \"gaussian_blur\":0.5,\n       \"rotation\":1,\n       \n      }\naugmentations = torchvision_transforms(aug = aug)\nprint(augmentations)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# config.py\n\nimport argparse\nimport math\nimport os\n\ndef parse_option(string):\n    parser = argparse.ArgumentParser('argument for training')\n\n    parser.add_argument('--print_freq', type=int, default=10,\n                        help='print frequency')\n    parser.add_argument('--save_freq', type=int, default=50,\n                        help='save frequency')\n    parser.add_argument('--batch_size', type=int, default=128,\n                        help='batch_size')\n    parser.add_argument('--num_workers', type=int, default=8,\n                        help='num of workers to use')\n    parser.add_argument('--epochs', type=int, default=100,\n                        help='number of training epochs')\n    parser.add_argument('--device', type=str, default='cuda:0')\n    # optimization\n    parser.add_argument('--learning_rate', type=float, default=0.05,\n                        help='learning rate')\n    parser.add_argument('--patient_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--cluster_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--lr_decay_epochs', type=str, default='100',\n                        help='where to decay lr, can be a list')\n    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n                        help='decay rate for learning rate')\n    parser.add_argument('--weight_decay', type=float, default=1e-4,\n                        help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='momentum')\n    parser.add_argument('--train_csv_path', type=str, default='train data csv')\n    parser.add_argument('--test_csv_path', type=str, default='test data csv')\n    parser.add_argument('--train_image_path', type=str, default='train data csv')\n    parser.add_argument('--test_image_path', type=str, default='test data csv')\n\n    parser.add_argument('--parallel', type=int, default=1, help='data parallel')\n    parser.add_argument('--ncls', type=int, default=6, help='Number of Classes')\n    # model dataset\n    parser.add_argument('--model', type=str, default='resnet50')\n    parser.add_argument('--dataset', type=str, default='TREX_DME',\n                        choices=[ 'OLIVES'], help='dataset')\n    parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n    parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n    parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n    parser.add_argument('--size', type=int, default=128, help='parameter for RandomResizedCrop')\n\n    # temperature\n    parser.add_argument('--temp', type=float, default=0.07,\n                        help='temperature for loss function')\n\n\n\n    opt = parser.parse_args(string)\n\n    # check if dataset is path that passed required arguments\n    if opt.dataset == 'path':\n        assert opt.data_folder is not None \\\n               and opt.mean is not None \\\n               and opt.std is not None\n\n    # set the path according to the environment\n    if opt.data_folder is None:\n        opt.data_folder = './datasets/'\n    opt.model_path = './save/{}_models'.format(opt.dataset)\n\n    iterations = opt.lr_decay_epochs.split(',')\n    opt.lr_decay_epochs = list([])\n    for it in iterations:\n        opt.lr_decay_epochs.append(int(it))\n\n    opt.model_name = '{}_lr_{}_decay_{}_bsz_{}_temp_{}'. \\\n        format(opt.model, opt.learning_rate,\n               opt.weight_decay, opt.batch_size, opt.temp)\n\n\n    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n    if not os.path.isdir(opt.save_folder):\n        os.makedirs(opt.save_folder)\n\n    return opt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_metric_learning.losses import NTXentLoss\nss_loss_func = NTXentLoss(temperature=0.10)\n\ndef train_ss(Net,projection_head,data_loader, epoch):\n    Net.train()\n    projection_head.train()\n    total_loss = AverageMeter()\n    for idx, x in enumerate(data_loader): \n        # print(batch_idx)\n        optimizer.zero_grad()\n        x = x.to(device)\n        # Get data representations\n        \n        A1 = augmentations(x)\n        A2 = augmentations(x)\n        \n        h1 = Net(A1)\n        z1 = projection_head(h1)\n        \n        h2 = Net(A2)\n        z2 = projection_head(h2)\n        \n        # Prepare for loss\n        embeddings = torch.cat((z1, z2))\n        # The same index corresponds to a positive pair\n        indices = torch.arange(0, z1.size(0), device=z2.device)\n        labels = torch.cat((indices, indices))\n        loss = ss_loss_func(embeddings, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss.update(loss.data.item())\n        \n        # print info\n        if (idx + 1) % 50 == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(\n                epoch, idx + 1, len(data_loader)))\n            \n    return total_loss.avg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_supervised(train_loader, val_loader, model,criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    device = opt.device\n    end = time.time()\n    correct_predictions = 0\n\n    for idx, (image, bio_tensor) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        images = image.to(device)\n\n        labels = bio_tensor.float()\n\n        labels = labels.to(device)\n        bsz = labels.shape[0]\n\n        # compute loss\n        output = model(images)\n        loss = criterion(output, labels)\n        \n        # Calculate training accuracy\n        predicted_labels = torch.round(torch.sigmoid(output)) \n        correct_predictions += (predicted_labels == labels).sum().item()\n\n        # update metric\n        losses.update(loss.item(), bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % 10 == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(\n                epoch, idx + 1, len(train_loader)))\n\n            sys.stdout.flush()\n\n    total_values = len(train_loader.dataset) * 6\n    training_accuracy = (correct_predictions / total_values) * 100.0\n    print(f\"Training Accuracy: {training_accuracy:.2f}%\")\n    print(losses.avg)\n    print(sample_evaluation_acc(val_loader, model, opt))\n    \n    return losses.avg\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submission_generate(val_loader, model, opt, epoch = 0):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    with torch.no_grad():\n        for idx, image in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n\n            # forward\n            output = model(images)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n\n\n    out_submisison = np.array(out_list)\n    np.save('output',out_submisison)\n    \n    output = np.load('/kaggle/working/output.npy')\n    submission = pd.read_csv(\"/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv\")\n    submission.iloc[:, 1:] = output\n    submission.to_csv(f\"/kaggle/working/submission{epoch}.csv\", index = False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample_evaluation(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    label_list = []\n    correct_count = 0\n    total_count = 0\n\n    with torch.no_grad():\n        for idx, (image,bio_tensor) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            labels = bio_tensor.float().to(device)\n\n            labels = labels.float()\n\n            label_list.append(labels.squeeze().detach().cpu().numpy())\n            # forward\n            output = model(images)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n            \n            correct_count += (labels == output).sum().item()\n            total_count += len(labels) * 6\n        \n    print((correct_count / total_count) * 100, \"%\")\n\n    label_array = np.array(label_list)\n    out_array = np.array(out_list)\n    f = f1_score(label_array,out_array,average='macro')\n    print(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample_evaluation_acc(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    label_list = []\n    correct_count = 0\n    total_count = 0\n\n    with torch.no_grad():\n        for idx, (image,bio_tensor) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            labels = bio_tensor.float().to(device)\n\n            labels = labels.float()\n\n            #label_list.append(labels.squeeze().detach().cpu().numpy())\n            # forward\n            output = model(images)\n            output = torch.round(torch.sigmoid(output))\n            #out_list.append(output.squeeze().detach().cpu().numpy())\n            \n            correct_count += (labels == output).sum().item()\n            total_count += len(labels) * 6\n        \n    print((correct_count / total_count) * 100, \"%\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(model, load_file, key = 'model'):\n    print('==> Loading...')\n    checkpoint = torch.load(load_file)\n    model.load_state_dict(checkpoint[key])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.isdir('/kaggle/working/supervised'):\n    os.makedirs('/kaggle/working/supervised')\nif not os.path.isdir('/kaggle/working/unsupervised'):\n    os.makedirs('/kaggle/working/unsupervised')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = args = ['--batch_size', '64', '--model', \"resnet50\", '--dataset', 'OLIVES', '--epochs', '2', '--device', 'cuda:0', '--train_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TRAIN/OLIVES', '--test_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/', '--test_csv_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv', '--train_csv_path', '/kaggle/input/olives-training-labels/Training_Biomarker_Data.csv']\nopt = parse_option(args)\ncsv_path_unlabelled = \"/kaggle/input/olives-training-labels/unlabelled_images.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build data loader\nunlabel_count = 20000\ntrain_loader, val_loader, test_loader, unlabelled_train_loader = set_loader(opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(unlabelled_train_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build optimizer\n#optimizer = set_optimizer(opt, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### UNSUPERVISED LEARNING PART","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--------------------------------------------------------------------------\ndevice = torch.device(\"cuda:0\" )\nNet = Encdr().to(device)\nprojection_head = Prj_Head(512,128).to(device)\n\noptimizer = torch.optim.Adam(list(Net.parameters())+list(projection_head.parameters()), lr=0.001)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n\n#train(Net,projection_head,train_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epoch = 20\nloss_list = []\nbest_loss = 10000\nsave_file = os.path.join(opt.save_folder + 'models', 'last.pth')\n#save_model(model, optimizer, opt, opt.epochs, save_file)\n\nfor epoch in range(1, n_epoch):\n    loss = train_ss(Net,projection_head,unlabelled_train_loader, epoch)\n    print(f'Epoch {epoch:3d}, Loss: {loss:.4f}')\n    scheduler.step()\n    loss_list.append(loss)\n    if loss<best_loss:\n        best_loss = loss\n        #torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_best_loss.pth.tar') \n        save_model(Net, optimizer, opt, opt.epochs, '/kaggle/working/unsupervised/best_loss.pth')\n    elif epoch%10 ==0:\n        #best_loss = loss\n        #torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_checkpoint.pth.tar') \n        save_model(Net, optimizer, opt, opt.epochs, f'/kaggle/working/unsupervised/epoch{epoch}.pth')\n#torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_final_checkpoint.pth.tar') \nplt.figure()\nplt.plot(loss_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### SUPERVISED LEARNING PART","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If loading pretrained\nload_file = \"/kaggle/input/models/pretrain_unsupervised.pth\"  # change to model path\ndevice = torch.device(\"cuda:0\" )\nNet = Encdr().to(device)\nNet = load_model(Net, load_file, \"model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Net.add_feature()\nmodel, criterion = set_model_st(opt, Net)    \noptimizer = set_optimizer(opt, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To check performance without the pre training\nnew_model, new_criterion = set_model(opt)    \nnew_optimizer = set_optimizer(opt, new_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training routine with freezing\n#model[0].requires_grad_(False)\n#opt.learning_rate = 0.05\n#for epoch in range(1, 15+1):\n#    train_supervised(train_loader, val_loader, model, criterion, optimizer, epoch, opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training routine without freezing\nmodel[0].requires_grad_(True)\nopt.learning_rate = 0.05\nfor epoch in range(1, 20+1):\n    train_supervised(train_loader, val_loader, model, criterion, optimizer, epoch, opt)\n    if epoch % 10 == 0:\n        submission_generate(test_loader, model, opt, epoch)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"if epochs > 0 and epochs % 10 == 0:\n    submission_generate(test_loader, model, opt, epoch)","metadata":{}},{"cell_type":"code","source":"save_file = os.path.join('/kaggle/working/supervised/last.pth')\nsave_model(model, optimizer, opt, opt.epochs, save_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation\nsample_evaluation(val_loader, model, opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_generate(test_loader, model, opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}