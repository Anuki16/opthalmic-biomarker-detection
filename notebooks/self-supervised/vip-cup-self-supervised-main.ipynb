{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a6c8bd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-26T19:24:42.501593Z",
     "iopub.status.busy": "2023-08-26T19:24:42.501077Z",
     "iopub.status.idle": "2023-08-26T19:24:43.610412Z",
     "shell.execute_reply": "2023-08-26T19:24:43.609356Z"
    },
    "papermill": {
     "duration": 1.129012,
     "end_time": "2023-08-26T19:24:43.613063",
     "exception": false,
     "start_time": "2023-08-26T19:24:42.484051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "'''\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "'''\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f081eda3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:24:43.643698Z",
     "iopub.status.busy": "2023-08-26T19:24:43.642016Z",
     "iopub.status.idle": "2023-08-26T19:24:57.270409Z",
     "shell.execute_reply": "2023-08-26T19:24:57.269158Z"
    },
    "papermill": {
     "duration": 13.646825,
     "end_time": "2023-08-26T19:24:57.273343",
     "exception": false,
     "start_time": "2023-08-26T19:24:43.626518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-metric-learning\r\n",
      "  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (1.23.5)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (1.2.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (4.65.0)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (2.0.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (4.6.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (1.11.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\r\n",
      "Installing collected packages: pytorch-metric-learning\r\n",
      "Successfully installed pytorch-metric-learning-2.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-metric-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d067487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:24:57.303826Z",
     "iopub.status.busy": "2023-08-26T19:24:57.303410Z",
     "iopub.status.idle": "2023-08-26T19:25:01.148765Z",
     "shell.execute_reply": "2023-08-26T19:25:01.147584Z"
    },
    "papermill": {
     "duration": 3.863254,
     "end_time": "2023-08-26T19:25:01.151326",
     "exception": false,
     "start_time": "2023-08-26T19:24:57.288072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"encoder + classifier\"\"\"\n",
    "    def __init__(self, name='resnet50', num_classes=2):\n",
    "        super(ResNet, self).__init__()\n",
    "        if (name == 'resnet50'):\n",
    "            self.encoder = torchvision.models.resnet50(zero_init_residual=True)\n",
    "            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "            self.encoder.fc = nn.Identity()\n",
    "            self.fc = nn.Linear(2048, num_classes)\n",
    "        else:\n",
    "            self.encoder = torchvision.models.resnet18(zero_init_residual=True)\n",
    "            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "            self.encoder.fc = nn.Identity()\n",
    "            self.fc = nn.Linear(512, num_classes)\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.fc(self.encoder(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31e53bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.179732Z",
     "iopub.status.busy": "2023-08-26T19:25:01.179167Z",
     "iopub.status.idle": "2023-08-26T19:25:01.188024Z",
     "shell.execute_reply": "2023-08-26T19:25:01.187119Z"
    },
    "papermill": {
     "duration": 0.025262,
     "end_time": "2023-08-26T19:25:01.190118",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.164856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Prj_Head(nn.Module):\n",
    "    def __init__(self,in_dim,feature_dim):\n",
    "        super(Prj_Head, self).__init__()\n",
    "        \n",
    "        self.g1 = nn.Sequential(nn.Linear(in_dim, 1024, bias=False),\n",
    "                               nn.BatchNorm1d(1024),\n",
    "                               nn.ReLU(inplace=True)\n",
    "                               )\n",
    "        self.g2 = nn.Sequential(nn.Linear(1024, 512, bias=False),\n",
    "                                nn.BatchNorm1d(512),\n",
    "                                nn.ReLU(inplace=True)\n",
    "                                )\n",
    "        self.g3=nn.Linear(512, feature_dim, bias=True)\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=- 1) \n",
    "        x = self.g1(x)\n",
    "        x = self.g2(x)\n",
    "        x = self.g3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "968032d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.218366Z",
     "iopub.status.busy": "2023-08-26T19:25:01.217579Z",
     "iopub.status.idle": "2023-08-26T19:25:01.225612Z",
     "shell.execute_reply": "2023-08-26T19:25:01.224757Z"
    },
    "papermill": {
     "duration": 0.024167,
     "end_time": "2023-08-26T19:25:01.227638",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.203471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encdr(nn.Module):\n",
    "    \"\"\"encoder + classifier\"\"\"\n",
    "    def __init__(self, name='resnet50', num_classes=2):\n",
    "        super(Encdr, self).__init__()\n",
    "        self.encoder = torchvision.models.resnet50(pretrained=True, zero_init_residual=True)\n",
    "        self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.fc = nn.Linear(2048, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.fc(self.encoder(x))\n",
    "    \n",
    "    def add_feature(self):\n",
    "        self.fc1=nn.Linear(512,2)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39599930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.256609Z",
     "iopub.status.busy": "2023-08-26T19:25:01.255853Z",
     "iopub.status.idle": "2023-08-26T19:25:01.272776Z",
     "shell.execute_reply": "2023-08-26T19:25:01.271807Z"
    },
    "papermill": {
     "duration": 0.03429,
     "end_time": "2023-08-26T19:25:01.274892",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.240602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datasets.py\n",
    "\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class OLIVES(data.Dataset):\n",
    "    def __init__(self,df, img_dir, transforms):\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "        self.df = pd.read_csv(df)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.img_dir + self.df.iloc[idx,0]\n",
    "        image = Image.open(path).convert(\"L\")\n",
    "        image = np.array(image)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transforms(image)\n",
    "        b1 = self.df.iloc[idx,1]\n",
    "        b2 = self.df.iloc[idx,2]\n",
    "        b3 = self.df.iloc[idx,3]\n",
    "        b4 = self.df.iloc[idx, 4]\n",
    "        b5 = self.df.iloc[idx, 5]\n",
    "        b6 = self.df.iloc[idx, 6]\n",
    "        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n",
    "        return image, bio_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RECOVERY(data.Dataset):\n",
    "    def __init__(self,df, img_dir, transforms):\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "        self.df = pd.read_csv(df)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.img_dir + self.df.iloc[idx,0]\n",
    "        image = Image.open(path).convert(\"L\")\n",
    "        image = np.array(image)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transforms(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "class RECOVERY_TEST(data.Dataset):\n",
    "    def __init__(self,df, img_dir, transforms):\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "        self.df = pd.read_csv(df)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.img_dir + self.df.iloc[idx,0]\n",
    "        image = Image.open(path).convert(\"L\")\n",
    "        image = np.array(image)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transforms(image)\n",
    "        b1 = self.df.iloc[idx,1]\n",
    "        b2 = self.df.iloc[idx,2]\n",
    "        b3 = self.df.iloc[idx,3]\n",
    "        b4 = self.df.iloc[idx, 4]\n",
    "        b5 = self.df.iloc[idx, 5]\n",
    "        b6 = self.df.iloc[idx, 6]\n",
    "        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n",
    "        return image, bio_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849c047b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.305724Z",
     "iopub.status.busy": "2023-08-26T19:25:01.304213Z",
     "iopub.status.idle": "2023-08-26T19:25:01.316662Z",
     "shell.execute_reply": "2023-08-26T19:25:01.315659Z"
    },
    "papermill": {
     "duration": 0.029522,
     "end_time": "2023-08-26T19:25:01.318729",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.289207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "def combine_excel(csv_dir):\n",
    "    filenames = glob.glob(csv_dir + \"/*.xlsx\")\n",
    "    outputxlsx = pd.DataFrame()\n",
    "\n",
    "    for file in filenames:\n",
    "        df = pd.concat(pd.read_excel(file, sheet_name=None), ignore_index=True, sort=False)\n",
    "        outputxlsx = outputxlsx.append(df, ignore_index=True)\n",
    "\n",
    "    outputxlsx.to_csv('test_set_labels.csv',index=False)\n",
    "\n",
    "def analyze_dataframe(csv_dir):\n",
    "    pass\n",
    "\n",
    "def process_images(csv_dir):\n",
    "    df = pd.read_csv(csv_dir)\n",
    "\n",
    "    for i in tqdm(range(0,len(df))):\n",
    "        path = df.iloc[i,0]\n",
    "        im = Image.open(path).convert('L')\n",
    "\n",
    "\n",
    "def numpy_submission(sub_dir,np_dir):\n",
    "    np_file  = np.load(np_dir)\n",
    "    print(len(np_file))\n",
    "    sub_dir = pd.read_csv(sub_dir)\n",
    "    print(len(sub_dir))\n",
    "    for i in range(0,len(sub_dir)):\n",
    "        sub_dir.iloc[i,1] = np_file[i,0]\n",
    "        sub_dir.iloc[i, 2] = np_file[i, 1]\n",
    "        sub_dir.iloc[i, 3] = np_file[i, 2]\n",
    "        sub_dir.iloc[i, 4] = np_file[i, 3]\n",
    "        sub_dir.iloc[i, 5] = np_file[i, 4]\n",
    "        sub_dir.iloc[i, 6] = np_file[i, 5]\n",
    "    print(sub_dir.head())\n",
    "    sub_dir.to_csv('baseline_result.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "    #process_images(csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db6f778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.347262Z",
     "iopub.status.busy": "2023-08-26T19:25:01.347000Z",
     "iopub.status.idle": "2023-08-26T19:25:01.378771Z",
     "shell.execute_reply": "2023-08-26T19:25:01.377894Z"
    },
    "papermill": {
     "duration": 0.048423,
     "end_time": "2023-08-26T19:25:01.380818",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.332395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import random_split, Subset, SubsetRandomSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "def set_model(opt, freeze = False):\n",
    "\n",
    "\n",
    "    device = opt.device\n",
    "    model = ResNet(name=opt.model,num_classes = opt.ncls)\n",
    "    if freeze:\n",
    "        model.encoder.requires_grad_(False)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "    return model, criterion\n",
    "\n",
    "\n",
    "# model for self supervised training\n",
    "\n",
    "def set_model_st(opt,Net):\n",
    "\n",
    "\n",
    "    device = opt.device\n",
    "    #model = Encdr(name=opt.model,num_classes = opt.ncls)\n",
    "    model = nn.Sequential(\n",
    "    Net, \n",
    "    nn.Linear(512, 1024, bias=False),\n",
    "    nn.BatchNorm1d(1024),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(1024, 512, bias=False),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(512, 6, bias=True))\n",
    "    \n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "    return model, criterion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_loader(opt):\n",
    "    # construct data loader\n",
    "    if opt.dataset == 'OLIVES' or opt.dataset == 'RECOVERY':\n",
    "        mean = (.1706)\n",
    "        std = (.2112)\n",
    "    else:\n",
    "        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n",
    "\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=224, scale=(0.85, 1.)),\n",
    "        transforms.RandomRotation(30),\n",
    "\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "        ], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "\n",
    "    if opt.dataset =='OLIVES':\n",
    "        data_path_train = opt.train_image_path\n",
    "        data_path_test = opt.test_image_path\n",
    "        train_dataset = OLIVES(csv_path_train,data_path_train,transforms = train_transform)\n",
    "        unlabelled_train_dataset = RECOVERY(csv_path_unlabelled,data_path_train,transforms = val_transform)\n",
    "        val_dataset = OLIVES(csv_path_valid,data_path_train,transforms = val_transform)\n",
    "        test_dataset = RECOVERY(csv_path_test,data_path_test,transforms = val_transform)\n",
    "        \n",
    "        # Create a random sampler for the subset\n",
    "        np.random.seed(121)\n",
    "        random_indices = np.random.choice(len(unlabelled_train_dataset), unlabel_count, replace=False)\n",
    "        subset_sampler = SubsetRandomSampler(random_indices)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(opt.dataset)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=opt.batch_size, shuffle=True,\n",
    "        num_workers=opt.num_workers, pin_memory=True)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=1, shuffle=False,\n",
    "        num_workers=0, pin_memory=True,drop_last=False)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=1, shuffle=False,\n",
    "        num_workers=0, pin_memory=True,drop_last=False)\n",
    "    \n",
    "    unlabelled_train_loader = torch.utils.data.DataLoader(unlabelled_train_dataset,\n",
    "        sampler=subset_sampler, batch_size=opt.batch_size,\n",
    "        num_workers=opt.num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, unlabelled_train_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def adjust_learning_rate(args, optimizer, epoch):\n",
    "    lr = args.learning_rate\n",
    "    if args.cosine:\n",
    "        eta_min = lr * (args.lr_decay_rate ** 3)\n",
    "        lr = eta_min + (lr - eta_min) * (\n",
    "                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n",
    "    else:\n",
    "        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n",
    "        if steps > 0:\n",
    "            lr = lr * (args.lr_decay_rate ** steps)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n",
    "    if args.warm and epoch <= args.warm_epochs:\n",
    "        p = (batch_id + (epoch - 1) * total_batches) / \\\n",
    "            (args.warm_epochs * total_batches)\n",
    "        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def set_optimizer(opt, model):\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=opt.learning_rate,\n",
    "                          momentum=opt.momentum,\n",
    "                          weight_decay=opt.weight_decay)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, opt, epoch, save_file):\n",
    "    print('==> Saving...')\n",
    "    state = {\n",
    "        'opt': opt,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    torch.save(state, save_file)\n",
    "    del state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56c4e1e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.408809Z",
     "iopub.status.busy": "2023-08-26T19:25:01.408520Z",
     "iopub.status.idle": "2023-08-26T19:25:01.424304Z",
     "shell.execute_reply": "2023-08-26T19:25:01.423432Z"
    },
    "papermill": {
     "duration": 0.032702,
     "end_time": "2023-08-26T19:25:01.426799",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.394097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.25, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=warn)\n",
      "    RandomApply(\n",
      "    p=0.3\n",
      "    ColorJitter(brightness=(0.76, 1.24), contrast=(0.76, 1.24), saturation=(0.76, 1.24), hue=(-0.06, 0.06))\n",
      ")\n",
      "    RandomApply(\n",
      "    p=0.5\n",
      "    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.2))\n",
      ")\n",
      "    <__main__.FixedRandomRotation object at 0x7fc4203726e0>\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# Augmentations\n",
    "from torchvision import transforms\n",
    "class GaussianBlur(object):\n",
    "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
    "    \"\"\"Borrowed from MoCo implementation\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=[.1, 2.]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
    "        return x\n",
    "    \n",
    "class FixedRandomRotation:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.choice(self.angles)\n",
    "        return transforms.functional.rotate(x, angle)\n",
    "    \n",
    "def torchvision_transforms(eval=False, aug=None):\n",
    "\n",
    "    trans = []\n",
    "\n",
    "    if aug[\"resize\"]:\n",
    "        trans.append(transforms.Resize(aug[\"resize\"]))\n",
    "\n",
    "    if aug[\"randcrop\"] and aug[\"scale\"] and not eval:\n",
    "        trans.append(transforms.RandomResizedCrop(aug[\"randcrop\"], scale=aug[\"scale\"]))\n",
    "\n",
    "    if aug[\"randcrop\"] and eval:\n",
    "        trans.append(transforms.CenterCrop(aug[\"randcrop\"]))\n",
    "\n",
    "    if aug[\"flip\"] and not eval:\n",
    "        trans.append(transforms.RandomHorizontalFlip(p=0.5))\n",
    "        trans.append(transforms.RandomVerticalFlip(p=0.5))\n",
    "\n",
    "    if aug[\"jitter_d\"] and not eval:\n",
    "        trans.append(transforms.RandomApply(\n",
    "            [transforms.ColorJitter(0.8*aug[\"jitter_d\"], 0.8*aug[\"jitter_d\"], 0.8*aug[\"jitter_d\"], 0.2*aug[\"jitter_d\"])],\n",
    "             p=aug[\"jitter_p\"]))\n",
    "\n",
    "    if aug[\"gaussian_blur\"] and not eval:\n",
    "        trans.append(transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1,.2))], p=aug[\"gaussian_blur\"]))\n",
    "\n",
    "    if aug[\"rotation\"] and not eval:\n",
    "        # rotation_transform = FixedRandomRotation(angles=[0, 90, 180, 270])\n",
    "        trans.append(FixedRandomRotation(angles=[0, 90, 180, 270]))\n",
    "\n",
    "\n",
    "    trans = transforms.Compose(trans)\n",
    "   \n",
    "    return trans\n",
    "aug = {\"resize\":0,\n",
    "    \"randcrop\":224,\n",
    "      \"scale\": (0.25, 1.0),\n",
    "      \"flip\":0,\n",
    "      \"jitter_d\":0.3,\n",
    "       \"jitter_p\":0.3,\n",
    "       \"gaussian_blur\":0.5,\n",
    "       \"rotation\":1\n",
    "      }\n",
    "augmentations = torchvision_transforms(aug = aug)\n",
    "print(augmentations)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03278075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.455207Z",
     "iopub.status.busy": "2023-08-26T19:25:01.454839Z",
     "iopub.status.idle": "2023-08-26T19:25:01.473214Z",
     "shell.execute_reply": "2023-08-26T19:25:01.472293Z"
    },
    "papermill": {
     "duration": 0.035046,
     "end_time": "2023-08-26T19:25:01.475280",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.440234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config.py\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "\n",
    "def parse_option(string):\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "    parser.add_argument('--print_freq', type=int, default=10,\n",
    "                        help='print frequency')\n",
    "    parser.add_argument('--save_freq', type=int, default=50,\n",
    "                        help='save frequency')\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "                        help='batch_size')\n",
    "    parser.add_argument('--num_workers', type=int, default=8,\n",
    "                        help='num of workers to use')\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='number of training epochs')\n",
    "    parser.add_argument('--device', type=str, default='cuda:0')\n",
    "    # optimization\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.05,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--patient_lambda', type=float, default=1,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--cluster_lambda', type=float, default=1,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--lr_decay_epochs', type=str, default='100',\n",
    "                        help='where to decay lr, can be a list')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n",
    "                        help='decay rate for learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--train_csv_path', type=str, default='train data csv')\n",
    "    parser.add_argument('--test_csv_path', type=str, default='test data csv')\n",
    "    parser.add_argument('--train_image_path', type=str, default='train data csv')\n",
    "    parser.add_argument('--test_image_path', type=str, default='test data csv')\n",
    "\n",
    "    parser.add_argument('--parallel', type=int, default=1, help='data parallel')\n",
    "    parser.add_argument('--ncls', type=int, default=6, help='Number of Classes')\n",
    "    # model dataset\n",
    "    parser.add_argument('--model', type=str, default='resnet50')\n",
    "    parser.add_argument('--dataset', type=str, default='TREX_DME',\n",
    "                        choices=[ 'OLIVES'], help='dataset')\n",
    "    parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n",
    "    parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n",
    "    parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n",
    "    parser.add_argument('--size', type=int, default=128, help='parameter for RandomResizedCrop')\n",
    "\n",
    "    # temperature\n",
    "    parser.add_argument('--temp', type=float, default=0.07,\n",
    "                        help='temperature for loss function')\n",
    "\n",
    "\n",
    "\n",
    "    opt = parser.parse_args(string)\n",
    "\n",
    "    # check if dataset is path that passed required arguments\n",
    "    if opt.dataset == 'path':\n",
    "        assert opt.data_folder is not None \\\n",
    "               and opt.mean is not None \\\n",
    "               and opt.std is not None\n",
    "\n",
    "    # set the path according to the environment\n",
    "    if opt.data_folder is None:\n",
    "        opt.data_folder = './datasets/'\n",
    "    opt.model_path = './save/{}_models'.format(opt.dataset)\n",
    "\n",
    "    iterations = opt.lr_decay_epochs.split(',')\n",
    "    opt.lr_decay_epochs = list([])\n",
    "    for it in iterations:\n",
    "        opt.lr_decay_epochs.append(int(it))\n",
    "\n",
    "    opt.model_name = '{}_lr_{}_decay_{}_bsz_{}_temp_{}'. \\\n",
    "        format(opt.model, opt.learning_rate,\n",
    "               opt.weight_decay, opt.batch_size, opt.temp)\n",
    "\n",
    "\n",
    "    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.save_folder):\n",
    "        os.makedirs(opt.save_folder)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d841f8a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.505075Z",
     "iopub.status.busy": "2023-08-26T19:25:01.504135Z",
     "iopub.status.idle": "2023-08-26T19:25:01.528624Z",
     "shell.execute_reply": "2023-08-26T19:25:01.527715Z"
    },
    "papermill": {
     "duration": 0.041741,
     "end_time": "2023-08-26T19:25:01.530908",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.489167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_metric_learning.losses import NTXentLoss\n",
    "ss_loss_func = NTXentLoss(temperature=0.10)\n",
    "\n",
    "def train_ss(Net,projection_head,data_loader, epoch):\n",
    "    Net.train()\n",
    "    projection_head.train()\n",
    "    total_loss = AverageMeter()\n",
    "    for idx, x in enumerate(data_loader): \n",
    "        # print(batch_idx)\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        # Get data representations\n",
    "        \n",
    "        A1 = augmentations(x)\n",
    "        A2 = augmentations(x)\n",
    "        \n",
    "        h1 = Net(A1)\n",
    "        z1 = projection_head(h1)\n",
    "        \n",
    "        h2 = Net(A2)\n",
    "        z2 = projection_head(h2)\n",
    "        \n",
    "        # Prepare for loss\n",
    "        embeddings = torch.cat((z1, z2))\n",
    "        # The same index corresponds to a positive pair\n",
    "        indices = torch.arange(0, z1.size(0), device=z2.device)\n",
    "        labels = torch.cat((indices, indices))\n",
    "        loss = ss_loss_func(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss.update(loss.data.item())\n",
    "        \n",
    "        # print info\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'.format(\n",
    "                epoch, idx + 1, len(data_loader)))\n",
    "            \n",
    "        del x, A1, A2\n",
    "            \n",
    "    return total_loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b291fbdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.559307Z",
     "iopub.status.busy": "2023-08-26T19:25:01.558526Z",
     "iopub.status.idle": "2023-08-26T19:25:01.569465Z",
     "shell.execute_reply": "2023-08-26T19:25:01.568596Z"
    },
    "papermill": {
     "duration": 0.026832,
     "end_time": "2023-08-26T19:25:01.571443",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.544611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_supervised(train_loader, val_loader, model,criterion, optimizer, epoch, opt):\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    device = opt.device\n",
    "    end = time.time()\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for idx, (image, bio_tensor) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        images = image.to(device)\n",
    "\n",
    "        labels = bio_tensor.float()\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        bsz = labels.shape[0]\n",
    "\n",
    "        # compute loss\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        predicted_labels = torch.round(torch.sigmoid(output)) \n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "\n",
    "        # update metric\n",
    "        losses.update(loss.item(), bsz)\n",
    "\n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print info\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'.format(\n",
    "                epoch, idx + 1, len(train_loader)))\n",
    "\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    total_values = len(train_loader.dataset) * 6\n",
    "    training_accuracy = (correct_predictions / total_values) * 100.0\n",
    "    print(f\"Training Accuracy: {training_accuracy:.2f}%\")\n",
    "    print(\"Training loss:\", losses.avg)\n",
    "    \n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f446c7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.605513Z",
     "iopub.status.busy": "2023-08-26T19:25:01.604772Z",
     "iopub.status.idle": "2023-08-26T19:25:01.612812Z",
     "shell.execute_reply": "2023-08-26T19:25:01.611868Z"
    },
    "papermill": {
     "duration": 0.024429,
     "end_time": "2023-08-26T19:25:01.614890",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.590461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submission_generate(val_loader, model, opt, epoch = 'final'):\n",
    "    \"\"\"validation\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    device = opt.device\n",
    "    out_list = []\n",
    "    with torch.no_grad():\n",
    "        for idx, image in (enumerate(val_loader)):\n",
    "\n",
    "            images = image.float().to(device)\n",
    "\n",
    "            # forward\n",
    "            output = model(images)\n",
    "            output = torch.round(torch.sigmoid(output))\n",
    "            out_list.append(output.squeeze().detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    out_submisison = np.array(out_list)\n",
    "    np.save('output',out_submisison)\n",
    "    \n",
    "    output = np.load('/kaggle/working/output.npy')\n",
    "    submission = pd.read_csv(\"/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv\")\n",
    "    submission.iloc[:, 1:] = output\n",
    "    submission.to_csv(f\"/kaggle/working/submission{epoch}.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55454471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.644061Z",
     "iopub.status.busy": "2023-08-26T19:25:01.643346Z",
     "iopub.status.idle": "2023-08-26T19:25:01.652676Z",
     "shell.execute_reply": "2023-08-26T19:25:01.651754Z"
    },
    "papermill": {
     "duration": 0.025975,
     "end_time": "2023-08-26T19:25:01.654632",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.628657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_evaluation(val_loader, model, opt):\n",
    "    \"\"\"validation\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    device = opt.device\n",
    "    out_list = []\n",
    "    label_list = []\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (image,bio_tensor) in (enumerate(val_loader)):\n",
    "\n",
    "            images = image.float().to(device)\n",
    "            labels = bio_tensor.float().to(device)\n",
    "\n",
    "            labels = labels.float()\n",
    "\n",
    "            label_list.append(labels.squeeze().detach().cpu().numpy())\n",
    "            # forward\n",
    "            output = model(images)\n",
    "            output = torch.round(torch.sigmoid(output))\n",
    "            out_list.append(output.squeeze().detach().cpu().numpy())\n",
    "            \n",
    "            correct_count += (labels == output).sum().item()\n",
    "            total_count += len(labels) * 6\n",
    "        \n",
    "    print(\"Validation accuracy:\", (correct_count / total_count) * 100, \"%\")\n",
    "\n",
    "    label_array = np.array(label_list)\n",
    "    out_array = np.array(out_list)\n",
    "    f = f1_score(label_array,out_array,average='macro')\n",
    "    print(\"Validation F1:\", f)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48231759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.683646Z",
     "iopub.status.busy": "2023-08-26T19:25:01.683043Z",
     "iopub.status.idle": "2023-08-26T19:25:01.690828Z",
     "shell.execute_reply": "2023-08-26T19:25:01.689943Z"
    },
    "papermill": {
     "duration": 0.024549,
     "end_time": "2023-08-26T19:25:01.693016",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.668467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_evaluation_acc(val_loader, model, opt):\n",
    "    \"\"\"validation\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    device = opt.device\n",
    "    out_list = []\n",
    "    label_list = []\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (image,bio_tensor) in (enumerate(val_loader)):\n",
    "\n",
    "            images = image.float().to(device)\n",
    "            labels = bio_tensor.float().to(device)\n",
    "\n",
    "            labels = labels.float()\n",
    "\n",
    "            #label_list.append(labels.squeeze().detach().cpu().numpy())\n",
    "            # forward\n",
    "            output = model(images)\n",
    "            output = torch.round(torch.sigmoid(output))\n",
    "            #out_list.append(output.squeeze().detach().cpu().numpy())\n",
    "            \n",
    "            correct_count += (labels == output).sum().item()\n",
    "            total_count += len(labels) * 6\n",
    "        \n",
    "    print((correct_count / total_count) * 100, \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63d57f67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.721379Z",
     "iopub.status.busy": "2023-08-26T19:25:01.720585Z",
     "iopub.status.idle": "2023-08-26T19:25:01.725935Z",
     "shell.execute_reply": "2023-08-26T19:25:01.725098Z"
    },
    "papermill": {
     "duration": 0.02162,
     "end_time": "2023-08-26T19:25:01.728026",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.706406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model, load_file, key = 'model'):\n",
    "    print('==> Loading...')\n",
    "    checkpoint = torch.load(load_file)\n",
    "    model.load_state_dict(checkpoint[key])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1304039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.756872Z",
     "iopub.status.busy": "2023-08-26T19:25:01.755841Z",
     "iopub.status.idle": "2023-08-26T19:25:01.761882Z",
     "shell.execute_reply": "2023-08-26T19:25:01.760984Z"
    },
    "papermill": {
     "duration": 0.022387,
     "end_time": "2023-08-26T19:25:01.763913",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.741526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('/kaggle/working/supervised'):\n",
    "    os.makedirs('/kaggle/working/supervised')\n",
    "if not os.path.isdir('/kaggle/working/unsupervised'):\n",
    "    os.makedirs('/kaggle/working/unsupervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc19b38e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.792782Z",
     "iopub.status.busy": "2023-08-26T19:25:01.791924Z",
     "iopub.status.idle": "2023-08-26T19:25:01.801024Z",
     "shell.execute_reply": "2023-08-26T19:25:01.800116Z"
    },
    "papermill": {
     "duration": 0.026281,
     "end_time": "2023-08-26T19:25:01.803147",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.776866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = args = ['--batch_size', '64', '--model', \"resnet50\", '--dataset', 'OLIVES', '--epochs', '2', '--device', 'cuda:0', '--train_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TRAIN/OLIVES', '--test_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/', '--test_csv_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv', '--train_csv_path', '/kaggle/input/olives-training-labels/Training_Biomarker_Data.csv']\n",
    "opt = parse_option(args)\n",
    "\n",
    "# CSV paths\n",
    "csv_path_train = \"/kaggle/input/olives-training-labels/training_split_biomarker_data.csv\"\n",
    "csv_path_valid = \"/kaggle/input/olives-training-labels/validation_biomarker_data.csv\"\n",
    "csv_path_test = \"/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv\"\n",
    "csv_path_unlabelled = \"/kaggle/input/olives-training-labels/unlabelled_images.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86f0d7",
   "metadata": {
    "papermill": {
     "duration": 0.015095,
     "end_time": "2023-08-26T19:25:01.831359",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.816264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9818fc16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:01.859726Z",
     "iopub.status.busy": "2023-08-26T19:25:01.859371Z",
     "iopub.status.idle": "2023-08-26T19:25:02.190020Z",
     "shell.execute_reply": "2023-08-26T19:25:02.188834Z"
    },
    "papermill": {
     "duration": 0.348375,
     "end_time": "2023-08-26T19:25:02.193373",
     "exception": false,
     "start_time": "2023-08-26T19:25:01.844998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# build data loader\n",
    "unlabel_count = 20000\n",
    "train_loader, val_loader, test_loader, unlabelled_train_loader = set_loader(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea3b46",
   "metadata": {
    "papermill": {
     "duration": 0.013566,
     "end_time": "2023-08-26T19:25:02.221566",
     "exception": false,
     "start_time": "2023-08-26T19:25:02.208000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029b88b",
   "metadata": {
    "papermill": {
     "duration": 0.013495,
     "end_time": "2023-08-26T19:25:02.248540",
     "exception": false,
     "start_time": "2023-08-26T19:25:02.235045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61baaede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:02.278409Z",
     "iopub.status.busy": "2023-08-26T19:25:02.277334Z",
     "iopub.status.idle": "2023-08-26T19:25:02.282016Z",
     "shell.execute_reply": "2023-08-26T19:25:02.281333Z"
    },
    "papermill": {
     "duration": 0.021897,
     "end_time": "2023-08-26T19:25:02.284043",
     "exception": false,
     "start_time": "2023-08-26T19:25:02.262146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### UNSUPERVISED LEARNING PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c82896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:02.315611Z",
     "iopub.status.busy": "2023-08-26T19:25:02.314866Z",
     "iopub.status.idle": "2023-08-26T19:25:06.736523Z",
     "shell.execute_reply": "2023-08-26T19:25:06.735405Z"
    },
    "papermill": {
     "duration": 4.440301,
     "end_time": "2023-08-26T19:25:06.739035",
     "exception": false,
     "start_time": "2023-08-26T19:25:02.298734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 235MB/s]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda:0\" )\n",
    "Net = Encdr().to(device)\n",
    "projection_head = Prj_Head(512,128).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(Net.parameters())+list(projection_head.parameters()), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "#train(Net,projection_head,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfa596ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:06.770649Z",
     "iopub.status.busy": "2023-08-26T19:25:06.770283Z",
     "iopub.status.idle": "2023-08-26T19:25:07.045290Z",
     "shell.execute_reply": "2023-08-26T19:25:07.044323Z"
    },
    "papermill": {
     "duration": 0.293109,
     "end_time": "2023-08-26T19:25:07.047520",
     "exception": false,
     "start_time": "2023-08-26T19:25:06.754411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc37eed5d80>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epoch = 0\n",
    "loss_list = []\n",
    "best_loss = 10000\n",
    "save_file = os.path.join(opt.save_folder + 'models', 'last.pth')\n",
    "#save_model(model, optimizer, opt, opt.epochs, save_file)\n",
    "\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    loss = train_ss(Net,projection_head,unlabelled_train_loader, epoch)\n",
    "    print(f'Epoch {epoch:3d}, Loss: {loss:.4f}')\n",
    "    scheduler.step()\n",
    "    loss_list.append(loss)\n",
    "    if loss<best_loss:\n",
    "        best_loss = loss\n",
    "        #torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_best_loss.pth.tar') \n",
    "        save_model(Net, optimizer, opt, opt.epochs, '/kaggle/working/unsupervised/best_loss.pth')\n",
    "    elif epoch % 10 == 0:\n",
    "        #best_loss = loss\n",
    "        #torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_checkpoint.pth.tar') \n",
    "        save_model(Net, optimizer, opt, opt.epochs, f'/kaggle/working/unsupervised/epoch{epoch}.pth')\n",
    "#torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_final_checkpoint.pth.tar') \n",
    "plt.figure()\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32a15a9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:07.078831Z",
     "iopub.status.busy": "2023-08-26T19:25:07.078223Z",
     "iopub.status.idle": "2023-08-26T19:25:07.083088Z",
     "shell.execute_reply": "2023-08-26T19:25:07.082171Z"
    },
    "papermill": {
     "duration": 0.022812,
     "end_time": "2023-08-26T19:25:07.085275",
     "exception": false,
     "start_time": "2023-08-26T19:25:07.062463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### SUPERVISED LEARNING PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffb2dd73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:07.116784Z",
     "iopub.status.busy": "2023-08-26T19:25:07.115852Z",
     "iopub.status.idle": "2023-08-26T19:25:09.055687Z",
     "shell.execute_reply": "2023-08-26T19:25:09.054629Z"
    },
    "papermill": {
     "duration": 1.958872,
     "end_time": "2023-08-26T19:25:09.058717",
     "exception": false,
     "start_time": "2023-08-26T19:25:07.099845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading...\n"
     ]
    }
   ],
   "source": [
    "# If loading pretrained\n",
    "load_file = \"/kaggle/input/models/epoch30_changing.pth\"  # change to model path\n",
    "device = torch.device(\"cuda:0\" )\n",
    "Net = Encdr().to(device)\n",
    "Net = load_model(Net, load_file, \"net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afb8a961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:09.090232Z",
     "iopub.status.busy": "2023-08-26T19:25:09.089904Z",
     "iopub.status.idle": "2023-08-26T19:25:09.108267Z",
     "shell.execute_reply": "2023-08-26T19:25:09.107392Z"
    },
    "papermill": {
     "duration": 0.036272,
     "end_time": "2023-08-26T19:25:09.110314",
     "exception": false,
     "start_time": "2023-08-26T19:25:09.074042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Net.add_feature()\n",
    "model, criterion = set_model_st(opt, Net)    \n",
    "optimizer = set_optimizer(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d9bda07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:09.140826Z",
     "iopub.status.busy": "2023-08-26T19:25:09.140548Z",
     "iopub.status.idle": "2023-08-26T19:25:09.145675Z",
     "shell.execute_reply": "2023-08-26T19:25:09.144826Z"
    },
    "papermill": {
     "duration": 0.022577,
     "end_time": "2023-08-26T19:25:09.147761",
     "exception": false,
     "start_time": "2023-08-26T19:25:09.125184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To check performance without the pre training\n",
    "#model, criterion = set_model(opt)    \n",
    "#optimizer = set_optimizer(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1e409",
   "metadata": {
    "papermill": {
     "duration": 0.014535,
     "end_time": "2023-08-26T19:25:09.176738",
     "exception": false,
     "start_time": "2023-08-26T19:25:09.162203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f432175d",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:09.207250Z",
     "iopub.status.busy": "2023-08-26T19:25:09.206910Z",
     "iopub.status.idle": "2023-08-26T19:25:09.214183Z",
     "shell.execute_reply": "2023-08-26T19:25:09.213223Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.025037,
     "end_time": "2023-08-26T19:25:09.216307",
     "exception": false,
     "start_time": "2023-08-26T19:25:09.191270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training routine with freezing\n",
    "model[0].requires_grad_(False)\n",
    "opt.learning_rate = 0.05\n",
    "\n",
    "best_f1 = 0.6\n",
    "for epoch in range(1, 0+1):\n",
    "    train_supervised(train_loader, val_loader, model, criterion, optimizer, epoch, opt)\n",
    "    cur_f1 = sample_evaluation(val_loader, model, opt)\n",
    "    if cur_f1 > best_f1:\n",
    "        best_f1 = cur_f1\n",
    "        submission_generate(test_loader, model, opt, epoch)\n",
    "    elif epoch % 10 == 0:\n",
    "        submission_generate(test_loader, model, opt, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61f57f55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T19:25:09.247132Z",
     "iopub.status.busy": "2023-08-26T19:25:09.246838Z",
     "iopub.status.idle": "2023-08-26T20:47:53.695389Z",
     "shell.execute_reply": "2023-08-26T20:47:53.694121Z"
    },
    "papermill": {
     "duration": 4964.467042,
     "end_time": "2023-08-26T20:47:53.697932",
     "exception": false,
     "start_time": "2023-08-26T19:25:09.230890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [1][10/140]\t\n",
      "Train: [1][20/140]\t\n",
      "Train: [1][30/140]\t\n",
      "Train: [1][40/140]\t\n",
      "Train: [1][50/140]\t\n",
      "Train: [1][60/140]\t\n",
      "Train: [1][70/140]\t\n",
      "Train: [1][80/140]\t\n",
      "Train: [1][90/140]\t\n",
      "Train: [1][100/140]\t\n",
      "Train: [1][110/140]\t\n",
      "Train: [1][120/140]\t\n",
      "Train: [1][130/140]\t\n",
      "Train: [1][140/140]\t\n",
      "Training Accuracy: 71.12%\n",
      "Training loss: 0.5470158385906981\n",
      "Validation accuracy: 68.06122448979592 %\n",
      "Validation F1: 0.19807576202173652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [2][10/140]\t\n",
      "Train: [2][20/140]\t\n",
      "Train: [2][30/140]\t\n",
      "Train: [2][40/140]\t\n",
      "Train: [2][50/140]\t\n",
      "Train: [2][60/140]\t\n",
      "Train: [2][70/140]\t\n",
      "Train: [2][80/140]\t\n",
      "Train: [2][90/140]\t\n",
      "Train: [2][100/140]\t\n",
      "Train: [2][110/140]\t\n",
      "Train: [2][120/140]\t\n",
      "Train: [2][130/140]\t\n",
      "Train: [2][140/140]\t\n",
      "Training Accuracy: 78.08%\n",
      "Training loss: 0.450437026578339\n",
      "Validation accuracy: 69.21768707482994 %\n",
      "Validation F1: 0.43181567809858507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [3][10/140]\t\n",
      "Train: [3][20/140]\t\n",
      "Train: [3][30/140]\t\n",
      "Train: [3][40/140]\t\n",
      "Train: [3][50/140]\t\n",
      "Train: [3][60/140]\t\n",
      "Train: [3][70/140]\t\n",
      "Train: [3][80/140]\t\n",
      "Train: [3][90/140]\t\n",
      "Train: [3][100/140]\t\n",
      "Train: [3][110/140]\t\n",
      "Train: [3][120/140]\t\n",
      "Train: [3][130/140]\t\n",
      "Train: [3][140/140]\t\n",
      "Training Accuracy: 80.55%\n",
      "Training loss: 0.4109656302871178\n",
      "Validation accuracy: 73.26530612244898 %\n",
      "Validation F1: 0.424996707185191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [4][10/140]\t\n",
      "Train: [4][20/140]\t\n",
      "Train: [4][30/140]\t\n",
      "Train: [4][40/140]\t\n",
      "Train: [4][50/140]\t\n",
      "Train: [4][60/140]\t\n",
      "Train: [4][70/140]\t\n",
      "Train: [4][80/140]\t\n",
      "Train: [4][90/140]\t\n",
      "Train: [4][100/140]\t\n",
      "Train: [4][110/140]\t\n",
      "Train: [4][120/140]\t\n",
      "Train: [4][130/140]\t\n",
      "Train: [4][140/140]\t\n",
      "Training Accuracy: 81.76%\n",
      "Training loss: 0.3890121982460681\n",
      "Validation accuracy: 74.14965986394559 %\n",
      "Validation F1: 0.4670866000428302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [5][10/140]\t\n",
      "Train: [5][20/140]\t\n",
      "Train: [5][30/140]\t\n",
      "Train: [5][40/140]\t\n",
      "Train: [5][50/140]\t\n",
      "Train: [5][60/140]\t\n",
      "Train: [5][70/140]\t\n",
      "Train: [5][80/140]\t\n",
      "Train: [5][90/140]\t\n",
      "Train: [5][100/140]\t\n",
      "Train: [5][110/140]\t\n",
      "Train: [5][120/140]\t\n",
      "Train: [5][130/140]\t\n",
      "Train: [5][140/140]\t\n",
      "Training Accuracy: 82.87%\n",
      "Training loss: 0.37227338630005125\n",
      "Validation accuracy: 74.96598639455783 %\n",
      "Validation F1: 0.4682095075026384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [6][10/140]\t\n",
      "Train: [6][20/140]\t\n",
      "Train: [6][30/140]\t\n",
      "Train: [6][40/140]\t\n",
      "Train: [6][50/140]\t\n",
      "Train: [6][60/140]\t\n",
      "Train: [6][70/140]\t\n",
      "Train: [6][80/140]\t\n",
      "Train: [6][90/140]\t\n",
      "Train: [6][100/140]\t\n",
      "Train: [6][110/140]\t\n",
      "Train: [6][120/140]\t\n",
      "Train: [6][130/140]\t\n",
      "Train: [6][140/140]\t\n",
      "Training Accuracy: 83.49%\n",
      "Training loss: 0.36033413604145814\n",
      "Validation accuracy: 76.59863945578232 %\n",
      "Validation F1: 0.4967647090390965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [7][10/140]\t\n",
      "Train: [7][20/140]\t\n",
      "Train: [7][30/140]\t\n",
      "Train: [7][40/140]\t\n",
      "Train: [7][50/140]\t\n",
      "Train: [7][60/140]\t\n",
      "Train: [7][70/140]\t\n",
      "Train: [7][80/140]\t\n",
      "Train: [7][90/140]\t\n",
      "Train: [7][100/140]\t\n",
      "Train: [7][110/140]\t\n",
      "Train: [7][120/140]\t\n",
      "Train: [7][130/140]\t\n",
      "Train: [7][140/140]\t\n",
      "Training Accuracy: 84.17%\n",
      "Training loss: 0.34669277583919156\n",
      "Validation accuracy: 76.0204081632653 %\n",
      "Validation F1: 0.4592641092616779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [8][10/140]\t\n",
      "Train: [8][20/140]\t\n",
      "Train: [8][30/140]\t\n",
      "Train: [8][40/140]\t\n",
      "Train: [8][50/140]\t\n",
      "Train: [8][60/140]\t\n",
      "Train: [8][70/140]\t\n",
      "Train: [8][80/140]\t\n",
      "Train: [8][90/140]\t\n",
      "Train: [8][100/140]\t\n",
      "Train: [8][110/140]\t\n",
      "Train: [8][120/140]\t\n",
      "Train: [8][130/140]\t\n",
      "Train: [8][140/140]\t\n",
      "Training Accuracy: 84.55%\n",
      "Training loss: 0.3418175293929268\n",
      "Validation accuracy: 73.70748299319729 %\n",
      "Validation F1: 0.44811442881454383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [9][10/140]\t\n",
      "Train: [9][20/140]\t\n",
      "Train: [9][30/140]\t\n",
      "Train: [9][40/140]\t\n",
      "Train: [9][50/140]\t\n",
      "Train: [9][60/140]\t\n",
      "Train: [9][70/140]\t\n",
      "Train: [9][80/140]\t\n",
      "Train: [9][90/140]\t\n",
      "Train: [9][100/140]\t\n",
      "Train: [9][110/140]\t\n",
      "Train: [9][120/140]\t\n",
      "Train: [9][130/140]\t\n",
      "Train: [9][140/140]\t\n",
      "Training Accuracy: 84.89%\n",
      "Training loss: 0.3311761933662721\n",
      "Validation accuracy: 77.48299319727892 %\n",
      "Validation F1: 0.5027158945962064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [10][10/140]\t\n",
      "Train: [10][20/140]\t\n",
      "Train: [10][30/140]\t\n",
      "Train: [10][40/140]\t\n",
      "Train: [10][50/140]\t\n",
      "Train: [10][60/140]\t\n",
      "Train: [10][70/140]\t\n",
      "Train: [10][80/140]\t\n",
      "Train: [10][90/140]\t\n",
      "Train: [10][100/140]\t\n",
      "Train: [10][110/140]\t\n",
      "Train: [10][120/140]\t\n",
      "Train: [10][130/140]\t\n",
      "Train: [10][140/140]\t\n",
      "Training Accuracy: 85.84%\n",
      "Training loss: 0.3156056839619337\n",
      "Validation accuracy: 74.72789115646259 %\n",
      "Validation F1: 0.4793595126942516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [11][10/140]\t\n",
      "Train: [11][20/140]\t\n",
      "Train: [11][30/140]\t\n",
      "Train: [11][40/140]\t\n",
      "Train: [11][50/140]\t\n",
      "Train: [11][60/140]\t\n",
      "Train: [11][70/140]\t\n",
      "Train: [11][80/140]\t\n",
      "Train: [11][90/140]\t\n",
      "Train: [11][100/140]\t\n",
      "Train: [11][110/140]\t\n",
      "Train: [11][120/140]\t\n",
      "Train: [11][130/140]\t\n",
      "Train: [11][140/140]\t\n",
      "Training Accuracy: 85.85%\n",
      "Training loss: 0.3162682317623404\n",
      "Validation accuracy: 73.87755102040816 %\n",
      "Validation F1: 0.4545340887732838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [12][10/140]\t\n",
      "Train: [12][20/140]\t\n",
      "Train: [12][30/140]\t\n",
      "Train: [12][40/140]\t\n",
      "Train: [12][50/140]\t\n",
      "Train: [12][60/140]\t\n",
      "Train: [12][70/140]\t\n",
      "Train: [12][80/140]\t\n",
      "Train: [12][90/140]\t\n",
      "Train: [12][100/140]\t\n",
      "Train: [12][110/140]\t\n",
      "Train: [12][120/140]\t\n",
      "Train: [12][130/140]\t\n",
      "Train: [12][140/140]\t\n",
      "Training Accuracy: 86.39%\n",
      "Training loss: 0.3054400716469036\n",
      "Validation accuracy: 74.62585034013605 %\n",
      "Validation F1: 0.49298503539016963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [13][10/140]\t\n",
      "Train: [13][20/140]\t\n",
      "Train: [13][30/140]\t\n",
      "Train: [13][40/140]\t\n",
      "Train: [13][50/140]\t\n",
      "Train: [13][60/140]\t\n",
      "Train: [13][70/140]\t\n",
      "Train: [13][80/140]\t\n",
      "Train: [13][90/140]\t\n",
      "Train: [13][100/140]\t\n",
      "Train: [13][110/140]\t\n",
      "Train: [13][120/140]\t\n",
      "Train: [13][130/140]\t\n",
      "Train: [13][140/140]\t\n",
      "Training Accuracy: 86.74%\n",
      "Training loss: 0.3015335009260599\n",
      "Validation accuracy: 77.82312925170068 %\n",
      "Validation F1: 0.5792661836911249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [14][10/140]\t\n",
      "Train: [14][20/140]\t\n",
      "Train: [14][30/140]\t\n",
      "Train: [14][40/140]\t\n",
      "Train: [14][50/140]\t\n",
      "Train: [14][60/140]\t\n",
      "Train: [14][70/140]\t\n",
      "Train: [14][80/140]\t\n",
      "Train: [14][90/140]\t\n",
      "Train: [14][100/140]\t\n",
      "Train: [14][110/140]\t\n",
      "Train: [14][120/140]\t\n",
      "Train: [14][130/140]\t\n",
      "Train: [14][140/140]\t\n",
      "Training Accuracy: 87.00%\n",
      "Training loss: 0.2947095169109944\n",
      "Validation accuracy: 78.80952380952381 %\n",
      "Validation F1: 0.5680334955259033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [15][10/140]\t\n",
      "Train: [15][20/140]\t\n",
      "Train: [15][30/140]\t\n",
      "Train: [15][40/140]\t\n",
      "Train: [15][50/140]\t\n",
      "Train: [15][60/140]\t\n",
      "Train: [15][70/140]\t\n",
      "Train: [15][80/140]\t\n",
      "Train: [15][90/140]\t\n",
      "Train: [15][100/140]\t\n",
      "Train: [15][110/140]\t\n",
      "Train: [15][120/140]\t\n",
      "Train: [15][130/140]\t\n",
      "Train: [15][140/140]\t\n",
      "Training Accuracy: 87.25%\n",
      "Training loss: 0.291755621617459\n",
      "Validation accuracy: 74.65986394557824 %\n",
      "Validation F1: 0.5318333114439858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [16][10/140]\t\n",
      "Train: [16][20/140]\t\n",
      "Train: [16][30/140]\t\n",
      "Train: [16][40/140]\t\n",
      "Train: [16][50/140]\t\n",
      "Train: [16][60/140]\t\n",
      "Train: [16][70/140]\t\n",
      "Train: [16][80/140]\t\n",
      "Train: [16][90/140]\t\n",
      "Train: [16][100/140]\t\n",
      "Train: [16][110/140]\t\n",
      "Train: [16][120/140]\t\n",
      "Train: [16][130/140]\t\n",
      "Train: [16][140/140]\t\n",
      "Training Accuracy: 87.46%\n",
      "Training loss: 0.28302047596995833\n",
      "Validation accuracy: 72.38095238095238 %\n",
      "Validation F1: 0.46986691034239936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [17][10/140]\t\n",
      "Train: [17][20/140]\t\n",
      "Train: [17][30/140]\t\n",
      "Train: [17][40/140]\t\n",
      "Train: [17][50/140]\t\n",
      "Train: [17][60/140]\t\n",
      "Train: [17][70/140]\t\n",
      "Train: [17][80/140]\t\n",
      "Train: [17][90/140]\t\n",
      "Train: [17][100/140]\t\n",
      "Train: [17][110/140]\t\n",
      "Train: [17][120/140]\t\n",
      "Train: [17][130/140]\t\n",
      "Train: [17][140/140]\t\n",
      "Training Accuracy: 87.81%\n",
      "Training loss: 0.27902222018547723\n",
      "Validation accuracy: 78.36734693877551 %\n",
      "Validation F1: 0.6045969812932276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [18][10/140]\t\n",
      "Train: [18][20/140]\t\n",
      "Train: [18][30/140]\t\n",
      "Train: [18][40/140]\t\n",
      "Train: [18][50/140]\t\n",
      "Train: [18][60/140]\t\n",
      "Train: [18][70/140]\t\n",
      "Train: [18][80/140]\t\n",
      "Train: [18][90/140]\t\n",
      "Train: [18][100/140]\t\n",
      "Train: [18][110/140]\t\n",
      "Train: [18][120/140]\t\n",
      "Train: [18][130/140]\t\n",
      "Train: [18][140/140]\t\n",
      "Training Accuracy: 88.27%\n",
      "Training loss: 0.2695687993421873\n",
      "Validation accuracy: 77.24489795918367 %\n",
      "Validation F1: 0.5794515007786839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [19][10/140]\t\n",
      "Train: [19][20/140]\t\n",
      "Train: [19][30/140]\t\n",
      "Train: [19][40/140]\t\n",
      "Train: [19][50/140]\t\n",
      "Train: [19][60/140]\t\n",
      "Train: [19][70/140]\t\n",
      "Train: [19][80/140]\t\n",
      "Train: [19][90/140]\t\n",
      "Train: [19][100/140]\t\n",
      "Train: [19][110/140]\t\n",
      "Train: [19][120/140]\t\n",
      "Train: [19][130/140]\t\n",
      "Train: [19][140/140]\t\n",
      "Training Accuracy: 88.08%\n",
      "Training loss: 0.2708912130548003\n",
      "Validation accuracy: 72.31292517006803 %\n",
      "Validation F1: 0.54614001789575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [20][10/140]\t\n",
      "Train: [20][20/140]\t\n",
      "Train: [20][30/140]\t\n",
      "Train: [20][40/140]\t\n",
      "Train: [20][50/140]\t\n",
      "Train: [20][60/140]\t\n",
      "Train: [20][70/140]\t\n",
      "Train: [20][80/140]\t\n",
      "Train: [20][90/140]\t\n",
      "Train: [20][100/140]\t\n",
      "Train: [20][110/140]\t\n",
      "Train: [20][120/140]\t\n",
      "Train: [20][130/140]\t\n",
      "Train: [20][140/140]\t\n",
      "Training Accuracy: 88.34%\n",
      "Training loss: 0.26671171569655794\n",
      "Validation accuracy: 77.27891156462586 %\n",
      "Validation F1: 0.567632579354878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [21][10/140]\t\n",
      "Train: [21][20/140]\t\n",
      "Train: [21][30/140]\t\n",
      "Train: [21][40/140]\t\n",
      "Train: [21][50/140]\t\n",
      "Train: [21][60/140]\t\n",
      "Train: [21][70/140]\t\n",
      "Train: [21][80/140]\t\n",
      "Train: [21][90/140]\t\n",
      "Train: [21][100/140]\t\n",
      "Train: [21][110/140]\t\n",
      "Train: [21][120/140]\t\n",
      "Train: [21][130/140]\t\n",
      "Train: [21][140/140]\t\n",
      "Training Accuracy: 88.82%\n",
      "Training loss: 0.2607361055136957\n",
      "Validation accuracy: 76.25850340136054 %\n",
      "Validation F1: 0.459290226016025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [22][10/140]\t\n",
      "Train: [22][20/140]\t\n",
      "Train: [22][30/140]\t\n",
      "Train: [22][40/140]\t\n",
      "Train: [22][50/140]\t\n",
      "Train: [22][60/140]\t\n",
      "Train: [22][70/140]\t\n",
      "Train: [22][80/140]\t\n",
      "Train: [22][90/140]\t\n",
      "Train: [22][100/140]\t\n",
      "Train: [22][110/140]\t\n",
      "Train: [22][120/140]\t\n",
      "Train: [22][130/140]\t\n",
      "Train: [22][140/140]\t\n",
      "Training Accuracy: 88.61%\n",
      "Training loss: 0.26257337221090876\n",
      "Validation accuracy: 77.27891156462586 %\n",
      "Validation F1: 0.5529068228934956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [23][10/140]\t\n",
      "Train: [23][20/140]\t\n",
      "Train: [23][30/140]\t\n",
      "Train: [23][40/140]\t\n",
      "Train: [23][50/140]\t\n",
      "Train: [23][60/140]\t\n",
      "Train: [23][70/140]\t\n",
      "Train: [23][80/140]\t\n",
      "Train: [23][90/140]\t\n",
      "Train: [23][100/140]\t\n",
      "Train: [23][110/140]\t\n",
      "Train: [23][120/140]\t\n",
      "Train: [23][130/140]\t\n",
      "Train: [23][140/140]\t\n",
      "Training Accuracy: 88.90%\n",
      "Training loss: 0.25550965087567246\n",
      "Validation accuracy: 78.70748299319727 %\n",
      "Validation F1: 0.5603556015269476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [24][10/140]\t\n",
      "Train: [24][20/140]\t\n",
      "Train: [24][30/140]\t\n",
      "Train: [24][40/140]\t\n",
      "Train: [24][50/140]\t\n",
      "Train: [24][60/140]\t\n",
      "Train: [24][70/140]\t\n",
      "Train: [24][80/140]\t\n",
      "Train: [24][90/140]\t\n",
      "Train: [24][100/140]\t\n",
      "Train: [24][110/140]\t\n",
      "Train: [24][120/140]\t\n",
      "Train: [24][130/140]\t\n",
      "Train: [24][140/140]\t\n",
      "Training Accuracy: 89.25%\n",
      "Training loss: 0.2493532026100116\n",
      "Validation accuracy: 80.23809523809524 %\n",
      "Validation F1: 0.6151192321321196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [25][10/140]\t\n",
      "Train: [25][20/140]\t\n",
      "Train: [25][30/140]\t\n",
      "Train: [25][40/140]\t\n",
      "Train: [25][50/140]\t\n",
      "Train: [25][60/140]\t\n",
      "Train: [25][70/140]\t\n",
      "Train: [25][80/140]\t\n",
      "Train: [25][90/140]\t\n",
      "Train: [25][100/140]\t\n",
      "Train: [25][110/140]\t\n",
      "Train: [25][120/140]\t\n",
      "Train: [25][130/140]\t\n",
      "Train: [25][140/140]\t\n",
      "Training Accuracy: 89.21%\n",
      "Training loss: 0.250240393452485\n",
      "Validation accuracy: 76.39455782312925 %\n",
      "Validation F1: 0.48716257183476913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [26][10/140]\t\n",
      "Train: [26][20/140]\t\n",
      "Train: [26][30/140]\t\n",
      "Train: [26][40/140]\t\n",
      "Train: [26][50/140]\t\n",
      "Train: [26][60/140]\t\n",
      "Train: [26][70/140]\t\n",
      "Train: [26][80/140]\t\n",
      "Train: [26][90/140]\t\n",
      "Train: [26][100/140]\t\n",
      "Train: [26][110/140]\t\n",
      "Train: [26][120/140]\t\n",
      "Train: [26][130/140]\t\n",
      "Train: [26][140/140]\t\n",
      "Training Accuracy: 89.63%\n",
      "Training loss: 0.24162423682202136\n",
      "Validation accuracy: 79.59183673469387 %\n",
      "Validation F1: 0.627371502907223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [27][10/140]\t\n",
      "Train: [27][20/140]\t\n",
      "Train: [27][30/140]\t\n",
      "Train: [27][40/140]\t\n",
      "Train: [27][50/140]\t\n",
      "Train: [27][60/140]\t\n",
      "Train: [27][70/140]\t\n",
      "Train: [27][80/140]\t\n",
      "Train: [27][90/140]\t\n",
      "Train: [27][100/140]\t\n",
      "Train: [27][110/140]\t\n",
      "Train: [27][120/140]\t\n",
      "Train: [27][130/140]\t\n",
      "Train: [27][140/140]\t\n",
      "Training Accuracy: 89.55%\n",
      "Training loss: 0.2399387460040736\n",
      "Validation accuracy: 79.04761904761905 %\n",
      "Validation F1: 0.5489246460610634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [28][10/140]\t\n",
      "Train: [28][20/140]\t\n",
      "Train: [28][30/140]\t\n",
      "Train: [28][40/140]\t\n",
      "Train: [28][50/140]\t\n",
      "Train: [28][60/140]\t\n",
      "Train: [28][70/140]\t\n",
      "Train: [28][80/140]\t\n",
      "Train: [28][90/140]\t\n",
      "Train: [28][100/140]\t\n",
      "Train: [28][110/140]\t\n",
      "Train: [28][120/140]\t\n",
      "Train: [28][130/140]\t\n",
      "Train: [28][140/140]\t\n",
      "Training Accuracy: 89.83%\n",
      "Training loss: 0.23642596142271705\n",
      "Validation accuracy: 76.97278911564625 %\n",
      "Validation F1: 0.5613223831180544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [29][10/140]\t\n",
      "Train: [29][20/140]\t\n",
      "Train: [29][30/140]\t\n",
      "Train: [29][40/140]\t\n",
      "Train: [29][50/140]\t\n",
      "Train: [29][60/140]\t\n",
      "Train: [29][70/140]\t\n",
      "Train: [29][80/140]\t\n",
      "Train: [29][90/140]\t\n",
      "Train: [29][100/140]\t\n",
      "Train: [29][110/140]\t\n",
      "Train: [29][120/140]\t\n",
      "Train: [29][130/140]\t\n",
      "Train: [29][140/140]\t\n",
      "Training Accuracy: 90.28%\n",
      "Training loss: 0.22979540814463667\n",
      "Validation accuracy: 78.9795918367347 %\n",
      "Validation F1: 0.5726044580252075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [30][10/140]\t\n",
      "Train: [30][20/140]\t\n",
      "Train: [30][30/140]\t\n",
      "Train: [30][40/140]\t\n",
      "Train: [30][50/140]\t\n",
      "Train: [30][60/140]\t\n",
      "Train: [30][70/140]\t\n",
      "Train: [30][80/140]\t\n",
      "Train: [30][90/140]\t\n",
      "Train: [30][100/140]\t\n",
      "Train: [30][110/140]\t\n",
      "Train: [30][120/140]\t\n",
      "Train: [30][130/140]\t\n",
      "Train: [30][140/140]\t\n",
      "Training Accuracy: 90.13%\n",
      "Training loss: 0.23037089799439114\n",
      "Validation accuracy: 76.93877551020408 %\n",
      "Validation F1: 0.5725687344686223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [31][10/140]\t\n",
      "Train: [31][20/140]\t\n",
      "Train: [31][30/140]\t\n",
      "Train: [31][40/140]\t\n",
      "Train: [31][50/140]\t\n",
      "Train: [31][60/140]\t\n",
      "Train: [31][70/140]\t\n",
      "Train: [31][80/140]\t\n",
      "Train: [31][90/140]\t\n",
      "Train: [31][100/140]\t\n",
      "Train: [31][110/140]\t\n",
      "Train: [31][120/140]\t\n",
      "Train: [31][130/140]\t\n",
      "Train: [31][140/140]\t\n",
      "Training Accuracy: 90.20%\n",
      "Training loss: 0.2303076014073073\n",
      "Validation accuracy: 79.25170068027211 %\n",
      "Validation F1: 0.5438959181380484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [32][10/140]\t\n",
      "Train: [32][20/140]\t\n",
      "Train: [32][30/140]\t\n",
      "Train: [32][40/140]\t\n",
      "Train: [32][50/140]\t\n",
      "Train: [32][60/140]\t\n",
      "Train: [32][70/140]\t\n",
      "Train: [32][80/140]\t\n",
      "Train: [32][90/140]\t\n",
      "Train: [32][100/140]\t\n",
      "Train: [32][110/140]\t\n",
      "Train: [32][120/140]\t\n",
      "Train: [32][130/140]\t\n",
      "Train: [32][140/140]\t\n",
      "Training Accuracy: 90.37%\n",
      "Training loss: 0.22586918747740548\n",
      "Validation accuracy: 75.0 %\n",
      "Validation F1: 0.5490046816583747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [33][10/140]\t\n",
      "Train: [33][20/140]\t\n",
      "Train: [33][30/140]\t\n",
      "Train: [33][40/140]\t\n",
      "Train: [33][50/140]\t\n",
      "Train: [33][60/140]\t\n",
      "Train: [33][70/140]\t\n",
      "Train: [33][80/140]\t\n",
      "Train: [33][90/140]\t\n",
      "Train: [33][100/140]\t\n",
      "Train: [33][110/140]\t\n",
      "Train: [33][120/140]\t\n",
      "Train: [33][130/140]\t\n",
      "Train: [33][140/140]\t\n",
      "Training Accuracy: 90.46%\n",
      "Training loss: 0.22577178091592762\n",
      "Validation accuracy: 79.72789115646258 %\n",
      "Validation F1: 0.6332990773832284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [34][10/140]\t\n",
      "Train: [34][20/140]\t\n",
      "Train: [34][30/140]\t\n",
      "Train: [34][40/140]\t\n",
      "Train: [34][50/140]\t\n",
      "Train: [34][60/140]\t\n",
      "Train: [34][70/140]\t\n",
      "Train: [34][80/140]\t\n",
      "Train: [34][90/140]\t\n",
      "Train: [34][100/140]\t\n",
      "Train: [34][110/140]\t\n",
      "Train: [34][120/140]\t\n",
      "Train: [34][130/140]\t\n",
      "Train: [34][140/140]\t\n",
      "Training Accuracy: 90.63%\n",
      "Training loss: 0.22093397828395872\n",
      "Validation accuracy: 79.62585034013605 %\n",
      "Validation F1: 0.6058627489675136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [35][10/140]\t\n",
      "Train: [35][20/140]\t\n",
      "Train: [35][30/140]\t\n",
      "Train: [35][40/140]\t\n",
      "Train: [35][50/140]\t\n",
      "Train: [35][60/140]\t\n",
      "Train: [35][70/140]\t\n",
      "Train: [35][80/140]\t\n",
      "Train: [35][90/140]\t\n",
      "Train: [35][100/140]\t\n",
      "Train: [35][110/140]\t\n",
      "Train: [35][120/140]\t\n",
      "Train: [35][130/140]\t\n",
      "Train: [35][140/140]\t\n",
      "Training Accuracy: 90.60%\n",
      "Training loss: 0.2219404056751827\n",
      "Validation accuracy: 78.77551020408163 %\n",
      "Validation F1: 0.5909323158670836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [36][10/140]\t\n",
      "Train: [36][20/140]\t\n",
      "Train: [36][30/140]\t\n",
      "Train: [36][40/140]\t\n",
      "Train: [36][50/140]\t\n",
      "Train: [36][60/140]\t\n",
      "Train: [36][70/140]\t\n",
      "Train: [36][80/140]\t\n",
      "Train: [36][90/140]\t\n",
      "Train: [36][100/140]\t\n",
      "Train: [36][110/140]\t\n",
      "Train: [36][120/140]\t\n",
      "Train: [36][130/140]\t\n",
      "Train: [36][140/140]\t\n",
      "Training Accuracy: 90.82%\n",
      "Training loss: 0.21380728351198308\n",
      "Validation accuracy: 81.36054421768706 %\n",
      "Validation F1: 0.6101036541455217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [37][10/140]\t\n",
      "Train: [37][20/140]\t\n",
      "Train: [37][30/140]\t\n",
      "Train: [37][40/140]\t\n",
      "Train: [37][50/140]\t\n",
      "Train: [37][60/140]\t\n",
      "Train: [37][70/140]\t\n",
      "Train: [37][80/140]\t\n",
      "Train: [37][90/140]\t\n",
      "Train: [37][100/140]\t\n",
      "Train: [37][110/140]\t\n",
      "Train: [37][120/140]\t\n",
      "Train: [37][130/140]\t\n",
      "Train: [37][140/140]\t\n",
      "Training Accuracy: 90.89%\n",
      "Training loss: 0.2154258839196222\n",
      "Validation accuracy: 77.55102040816327 %\n",
      "Validation F1: 0.516055882636155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [38][10/140]\t\n",
      "Train: [38][20/140]\t\n",
      "Train: [38][30/140]\t\n",
      "Train: [38][40/140]\t\n",
      "Train: [38][50/140]\t\n",
      "Train: [38][60/140]\t\n",
      "Train: [38][70/140]\t\n",
      "Train: [38][80/140]\t\n",
      "Train: [38][90/140]\t\n",
      "Train: [38][100/140]\t\n",
      "Train: [38][110/140]\t\n",
      "Train: [38][120/140]\t\n",
      "Train: [38][130/140]\t\n",
      "Train: [38][140/140]\t\n",
      "Training Accuracy: 91.04%\n",
      "Training loss: 0.20987470264885558\n",
      "Validation accuracy: 78.77551020408163 %\n",
      "Validation F1: 0.5840535208664424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [39][10/140]\t\n",
      "Train: [39][20/140]\t\n",
      "Train: [39][30/140]\t\n",
      "Train: [39][40/140]\t\n",
      "Train: [39][50/140]\t\n",
      "Train: [39][60/140]\t\n",
      "Train: [39][70/140]\t\n",
      "Train: [39][80/140]\t\n",
      "Train: [39][90/140]\t\n",
      "Train: [39][100/140]\t\n",
      "Train: [39][110/140]\t\n",
      "Train: [39][120/140]\t\n",
      "Train: [39][130/140]\t\n",
      "Train: [39][140/140]\t\n",
      "Training Accuracy: 91.05%\n",
      "Training loss: 0.21300757162558656\n",
      "Validation accuracy: 80.20408163265306 %\n",
      "Validation F1: 0.5848661661862188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [40][10/140]\t\n",
      "Train: [40][20/140]\t\n",
      "Train: [40][30/140]\t\n",
      "Train: [40][40/140]\t\n",
      "Train: [40][50/140]\t\n",
      "Train: [40][60/140]\t\n",
      "Train: [40][70/140]\t\n",
      "Train: [40][80/140]\t\n",
      "Train: [40][90/140]\t\n",
      "Train: [40][100/140]\t\n",
      "Train: [40][110/140]\t\n",
      "Train: [40][120/140]\t\n",
      "Train: [40][130/140]\t\n",
      "Train: [40][140/140]\t\n",
      "Training Accuracy: 90.96%\n",
      "Training loss: 0.2109233014090068\n",
      "Validation accuracy: 78.63945578231292 %\n",
      "Validation F1: 0.5954691864914113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [41][10/140]\t\n",
      "Train: [41][20/140]\t\n",
      "Train: [41][30/140]\t\n",
      "Train: [41][40/140]\t\n",
      "Train: [41][50/140]\t\n",
      "Train: [41][60/140]\t\n",
      "Train: [41][70/140]\t\n",
      "Train: [41][80/140]\t\n",
      "Train: [41][90/140]\t\n",
      "Train: [41][100/140]\t\n",
      "Train: [41][110/140]\t\n",
      "Train: [41][120/140]\t\n",
      "Train: [41][130/140]\t\n",
      "Train: [41][140/140]\t\n",
      "Training Accuracy: 91.44%\n",
      "Training loss: 0.2052356945570073\n",
      "Validation accuracy: 77.7891156462585 %\n",
      "Validation F1: 0.5599281529142816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [42][10/140]\t\n",
      "Train: [42][20/140]\t\n",
      "Train: [42][30/140]\t\n",
      "Train: [42][40/140]\t\n",
      "Train: [42][50/140]\t\n",
      "Train: [42][60/140]\t\n",
      "Train: [42][70/140]\t\n",
      "Train: [42][80/140]\t\n",
      "Train: [42][90/140]\t\n",
      "Train: [42][100/140]\t\n",
      "Train: [42][110/140]\t\n",
      "Train: [42][120/140]\t\n",
      "Train: [42][130/140]\t\n",
      "Train: [42][140/140]\t\n",
      "Training Accuracy: 91.36%\n",
      "Training loss: 0.20557445207166683\n",
      "Validation accuracy: 81.83673469387756 %\n",
      "Validation F1: 0.6201479006947294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [43][10/140]\t\n",
      "Train: [43][20/140]\t\n",
      "Train: [43][30/140]\t\n",
      "Train: [43][40/140]\t\n",
      "Train: [43][50/140]\t\n",
      "Train: [43][60/140]\t\n",
      "Train: [43][70/140]\t\n",
      "Train: [43][80/140]\t\n",
      "Train: [43][90/140]\t\n",
      "Train: [43][100/140]\t\n",
      "Train: [43][110/140]\t\n",
      "Train: [43][120/140]\t\n",
      "Train: [43][130/140]\t\n",
      "Train: [43][140/140]\t\n",
      "Training Accuracy: 91.45%\n",
      "Training loss: 0.20262255506428348\n",
      "Validation accuracy: 79.6938775510204 %\n",
      "Validation F1: 0.565564573584802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [44][10/140]\t\n",
      "Train: [44][20/140]\t\n",
      "Train: [44][30/140]\t\n",
      "Train: [44][40/140]\t\n",
      "Train: [44][50/140]\t\n",
      "Train: [44][60/140]\t\n",
      "Train: [44][70/140]\t\n",
      "Train: [44][80/140]\t\n",
      "Train: [44][90/140]\t\n",
      "Train: [44][100/140]\t\n",
      "Train: [44][110/140]\t\n",
      "Train: [44][120/140]\t\n",
      "Train: [44][130/140]\t\n",
      "Train: [44][140/140]\t\n",
      "Training Accuracy: 91.49%\n",
      "Training loss: 0.199176706438857\n",
      "Validation accuracy: 78.74149659863946 %\n",
      "Validation F1: 0.5866673259629596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [45][10/140]\t\n",
      "Train: [45][20/140]\t\n",
      "Train: [45][30/140]\t\n",
      "Train: [45][40/140]\t\n",
      "Train: [45][50/140]\t\n",
      "Train: [45][60/140]\t\n",
      "Train: [45][70/140]\t\n",
      "Train: [45][80/140]\t\n",
      "Train: [45][90/140]\t\n",
      "Train: [45][100/140]\t\n",
      "Train: [45][110/140]\t\n",
      "Train: [45][120/140]\t\n",
      "Train: [45][130/140]\t\n",
      "Train: [45][140/140]\t\n",
      "Training Accuracy: 91.67%\n",
      "Training loss: 0.19844538399151393\n",
      "Validation accuracy: 77.27891156462586 %\n",
      "Validation F1: 0.5836675985722642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [46][10/140]\t\n",
      "Train: [46][20/140]\t\n",
      "Train: [46][30/140]\t\n",
      "Train: [46][40/140]\t\n",
      "Train: [46][50/140]\t\n",
      "Train: [46][60/140]\t\n",
      "Train: [46][70/140]\t\n",
      "Train: [46][80/140]\t\n",
      "Train: [46][90/140]\t\n",
      "Train: [46][100/140]\t\n",
      "Train: [46][110/140]\t\n",
      "Train: [46][120/140]\t\n",
      "Train: [46][130/140]\t\n",
      "Train: [46][140/140]\t\n",
      "Training Accuracy: 91.65%\n",
      "Training loss: 0.1977403190711283\n",
      "Validation accuracy: 80.03401360544218 %\n",
      "Validation F1: 0.6241956387563287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [47][10/140]\t\n",
      "Train: [47][20/140]\t\n",
      "Train: [47][30/140]\t\n",
      "Train: [47][40/140]\t\n",
      "Train: [47][50/140]\t\n",
      "Train: [47][60/140]\t\n",
      "Train: [47][70/140]\t\n",
      "Train: [47][80/140]\t\n",
      "Train: [47][90/140]\t\n",
      "Train: [47][100/140]\t\n",
      "Train: [47][110/140]\t\n",
      "Train: [47][120/140]\t\n",
      "Train: [47][130/140]\t\n",
      "Train: [47][140/140]\t\n",
      "Training Accuracy: 91.72%\n",
      "Training loss: 0.19788958052063393\n",
      "Validation accuracy: 81.3265306122449 %\n",
      "Validation F1: 0.5998076541909375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [48][10/140]\t\n",
      "Train: [48][20/140]\t\n",
      "Train: [48][30/140]\t\n",
      "Train: [48][40/140]\t\n",
      "Train: [48][50/140]\t\n",
      "Train: [48][60/140]\t\n",
      "Train: [48][70/140]\t\n",
      "Train: [48][80/140]\t\n",
      "Train: [48][90/140]\t\n",
      "Train: [48][100/140]\t\n",
      "Train: [48][110/140]\t\n",
      "Train: [48][120/140]\t\n",
      "Train: [48][130/140]\t\n",
      "Train: [48][140/140]\t\n",
      "Training Accuracy: 91.86%\n",
      "Training loss: 0.19438644587646073\n",
      "Validation accuracy: 80.54421768707482 %\n",
      "Validation F1: 0.5896617822328364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [49][10/140]\t\n",
      "Train: [49][20/140]\t\n",
      "Train: [49][30/140]\t\n",
      "Train: [49][40/140]\t\n",
      "Train: [49][50/140]\t\n",
      "Train: [49][60/140]\t\n",
      "Train: [49][70/140]\t\n",
      "Train: [49][80/140]\t\n",
      "Train: [49][90/140]\t\n",
      "Train: [49][100/140]\t\n",
      "Train: [49][110/140]\t\n",
      "Train: [49][120/140]\t\n",
      "Train: [49][130/140]\t\n",
      "Train: [49][140/140]\t\n",
      "Training Accuracy: 92.05%\n",
      "Training loss: 0.19462329736737613\n",
      "Validation accuracy: 78.74149659863946 %\n",
      "Validation F1: 0.5780386251983917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [50][10/140]\t\n",
      "Train: [50][20/140]\t\n",
      "Train: [50][30/140]\t\n",
      "Train: [50][40/140]\t\n",
      "Train: [50][50/140]\t\n",
      "Train: [50][60/140]\t\n",
      "Train: [50][70/140]\t\n",
      "Train: [50][80/140]\t\n",
      "Train: [50][90/140]\t\n",
      "Train: [50][100/140]\t\n",
      "Train: [50][110/140]\t\n",
      "Train: [50][120/140]\t\n",
      "Train: [50][130/140]\t\n",
      "Train: [50][140/140]\t\n",
      "Training Accuracy: 91.99%\n",
      "Training loss: 0.19030324028302373\n",
      "Validation accuracy: 80.47619047619048 %\n",
      "Validation F1: 0.5904151376193466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [51][10/140]\t\n",
      "Train: [51][20/140]\t\n",
      "Train: [51][30/140]\t\n",
      "Train: [51][40/140]\t\n",
      "Train: [51][50/140]\t\n",
      "Train: [51][60/140]\t\n",
      "Train: [51][70/140]\t\n",
      "Train: [51][80/140]\t\n",
      "Train: [51][90/140]\t\n",
      "Train: [51][100/140]\t\n",
      "Train: [51][110/140]\t\n",
      "Train: [51][120/140]\t\n",
      "Train: [51][130/140]\t\n",
      "Train: [51][140/140]\t\n",
      "Training Accuracy: 91.88%\n",
      "Training loss: 0.19306280934963024\n",
      "Validation accuracy: 80.71428571428572 %\n",
      "Validation F1: 0.617559352364669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [52][10/140]\t\n",
      "Train: [52][20/140]\t\n",
      "Train: [52][30/140]\t\n",
      "Train: [52][40/140]\t\n",
      "Train: [52][50/140]\t\n",
      "Train: [52][60/140]\t\n",
      "Train: [52][70/140]\t\n",
      "Train: [52][80/140]\t\n",
      "Train: [52][90/140]\t\n",
      "Train: [52][100/140]\t\n",
      "Train: [52][110/140]\t\n",
      "Train: [52][120/140]\t\n",
      "Train: [52][130/140]\t\n",
      "Train: [52][140/140]\t\n",
      "Training Accuracy: 92.13%\n",
      "Training loss: 0.18511046157754252\n",
      "Validation accuracy: 80.47619047619048 %\n",
      "Validation F1: 0.5798299782184969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [53][10/140]\t\n",
      "Train: [53][20/140]\t\n",
      "Train: [53][30/140]\t\n",
      "Train: [53][40/140]\t\n",
      "Train: [53][50/140]\t\n",
      "Train: [53][60/140]\t\n",
      "Train: [53][70/140]\t\n",
      "Train: [53][80/140]\t\n",
      "Train: [53][90/140]\t\n",
      "Train: [53][100/140]\t\n",
      "Train: [53][110/140]\t\n",
      "Train: [53][120/140]\t\n",
      "Train: [53][130/140]\t\n",
      "Train: [53][140/140]\t\n",
      "Training Accuracy: 92.04%\n",
      "Training loss: 0.19006740984413542\n",
      "Validation accuracy: 79.35374149659864 %\n",
      "Validation F1: 0.5893950980990627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [54][10/140]\t\n",
      "Train: [54][20/140]\t\n",
      "Train: [54][30/140]\t\n",
      "Train: [54][40/140]\t\n",
      "Train: [54][50/140]\t\n",
      "Train: [54][60/140]\t\n",
      "Train: [54][70/140]\t\n",
      "Train: [54][80/140]\t\n",
      "Train: [54][90/140]\t\n",
      "Train: [54][100/140]\t\n",
      "Train: [54][110/140]\t\n",
      "Train: [54][120/140]\t\n",
      "Train: [54][130/140]\t\n",
      "Train: [54][140/140]\t\n",
      "Training Accuracy: 92.17%\n",
      "Training loss: 0.1864017993378837\n",
      "Validation accuracy: 81.9047619047619 %\n",
      "Validation F1: 0.6333985389084542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [55][10/140]\t\n",
      "Train: [55][20/140]\t\n",
      "Train: [55][30/140]\t\n",
      "Train: [55][40/140]\t\n",
      "Train: [55][50/140]\t\n",
      "Train: [55][60/140]\t\n",
      "Train: [55][70/140]\t\n",
      "Train: [55][80/140]\t\n",
      "Train: [55][90/140]\t\n",
      "Train: [55][100/140]\t\n",
      "Train: [55][110/140]\t\n",
      "Train: [55][120/140]\t\n",
      "Train: [55][130/140]\t\n",
      "Train: [55][140/140]\t\n",
      "Training Accuracy: 92.25%\n",
      "Training loss: 0.18300199180500581\n",
      "Validation accuracy: 78.9795918367347 %\n",
      "Validation F1: 0.5448138471780826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [56][10/140]\t\n",
      "Train: [56][20/140]\t\n",
      "Train: [56][30/140]\t\n",
      "Train: [56][40/140]\t\n",
      "Train: [56][50/140]\t\n",
      "Train: [56][60/140]\t\n",
      "Train: [56][70/140]\t\n",
      "Train: [56][80/140]\t\n",
      "Train: [56][90/140]\t\n",
      "Train: [56][100/140]\t\n",
      "Train: [56][110/140]\t\n",
      "Train: [56][120/140]\t\n",
      "Train: [56][130/140]\t\n",
      "Train: [56][140/140]\t\n",
      "Training Accuracy: 92.25%\n",
      "Training loss: 0.18652375280041683\n",
      "Validation accuracy: 80.81632653061224 %\n",
      "Validation F1: 0.5642175634830378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [57][10/140]\t\n",
      "Train: [57][20/140]\t\n",
      "Train: [57][30/140]\t\n",
      "Train: [57][40/140]\t\n",
      "Train: [57][50/140]\t\n",
      "Train: [57][60/140]\t\n",
      "Train: [57][70/140]\t\n",
      "Train: [57][80/140]\t\n",
      "Train: [57][90/140]\t\n",
      "Train: [57][100/140]\t\n",
      "Train: [57][110/140]\t\n",
      "Train: [57][120/140]\t\n",
      "Train: [57][130/140]\t\n",
      "Train: [57][140/140]\t\n",
      "Training Accuracy: 92.23%\n",
      "Training loss: 0.1841966715140256\n",
      "Validation accuracy: 80.64625850340136 %\n",
      "Validation F1: 0.6139725703857479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [58][10/140]\t\n",
      "Train: [58][20/140]\t\n",
      "Train: [58][30/140]\t\n",
      "Train: [58][40/140]\t\n",
      "Train: [58][50/140]\t\n",
      "Train: [58][60/140]\t\n",
      "Train: [58][70/140]\t\n",
      "Train: [58][80/140]\t\n",
      "Train: [58][90/140]\t\n",
      "Train: [58][100/140]\t\n",
      "Train: [58][110/140]\t\n",
      "Train: [58][120/140]\t\n",
      "Train: [58][130/140]\t\n",
      "Train: [58][140/140]\t\n",
      "Training Accuracy: 92.46%\n",
      "Training loss: 0.17998738591915345\n",
      "Validation accuracy: 80.54421768707482 %\n",
      "Validation F1: 0.6079692423345779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [59][10/140]\t\n",
      "Train: [59][20/140]\t\n",
      "Train: [59][30/140]\t\n",
      "Train: [59][40/140]\t\n",
      "Train: [59][50/140]\t\n",
      "Train: [59][60/140]\t\n",
      "Train: [59][70/140]\t\n",
      "Train: [59][80/140]\t\n",
      "Train: [59][90/140]\t\n",
      "Train: [59][100/140]\t\n",
      "Train: [59][110/140]\t\n",
      "Train: [59][120/140]\t\n",
      "Train: [59][130/140]\t\n",
      "Train: [59][140/140]\t\n",
      "Training Accuracy: 92.50%\n",
      "Training loss: 0.176622422039843\n",
      "Validation accuracy: 78.2312925170068 %\n",
      "Validation F1: 0.5968342566559973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [60][10/140]\t\n",
      "Train: [60][20/140]\t\n",
      "Train: [60][30/140]\t\n",
      "Train: [60][40/140]\t\n",
      "Train: [60][50/140]\t\n",
      "Train: [60][60/140]\t\n",
      "Train: [60][70/140]\t\n",
      "Train: [60][80/140]\t\n",
      "Train: [60][90/140]\t\n",
      "Train: [60][100/140]\t\n",
      "Train: [60][110/140]\t\n",
      "Train: [60][120/140]\t\n",
      "Train: [60][130/140]\t\n",
      "Train: [60][140/140]\t\n",
      "Training Accuracy: 92.55%\n",
      "Training loss: 0.1759524994311939\n",
      "Validation accuracy: 80.78231292517006 %\n",
      "Validation F1: 0.6206944742656796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [61][10/140]\t\n",
      "Train: [61][20/140]\t\n",
      "Train: [61][30/140]\t\n",
      "Train: [61][40/140]\t\n",
      "Train: [61][50/140]\t\n",
      "Train: [61][60/140]\t\n",
      "Train: [61][70/140]\t\n",
      "Train: [61][80/140]\t\n",
      "Train: [61][90/140]\t\n",
      "Train: [61][100/140]\t\n",
      "Train: [61][110/140]\t\n",
      "Train: [61][120/140]\t\n",
      "Train: [61][130/140]\t\n",
      "Train: [61][140/140]\t\n",
      "Training Accuracy: 92.49%\n",
      "Training loss: 0.17990271620457418\n",
      "Validation accuracy: 79.35374149659864 %\n",
      "Validation F1: 0.577281529876946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [62][10/140]\t\n",
      "Train: [62][20/140]\t\n",
      "Train: [62][30/140]\t\n",
      "Train: [62][40/140]\t\n",
      "Train: [62][50/140]\t\n",
      "Train: [62][60/140]\t\n",
      "Train: [62][70/140]\t\n",
      "Train: [62][80/140]\t\n",
      "Train: [62][90/140]\t\n",
      "Train: [62][100/140]\t\n",
      "Train: [62][110/140]\t\n",
      "Train: [62][120/140]\t\n",
      "Train: [62][130/140]\t\n",
      "Train: [62][140/140]\t\n",
      "Training Accuracy: 92.75%\n",
      "Training loss: 0.17262305616062568\n",
      "Validation accuracy: 81.15646258503402 %\n",
      "Validation F1: 0.6179508289578929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [63][10/140]\t\n",
      "Train: [63][20/140]\t\n",
      "Train: [63][30/140]\t\n",
      "Train: [63][40/140]\t\n",
      "Train: [63][50/140]\t\n",
      "Train: [63][60/140]\t\n",
      "Train: [63][70/140]\t\n",
      "Train: [63][80/140]\t\n",
      "Train: [63][90/140]\t\n",
      "Train: [63][100/140]\t\n",
      "Train: [63][110/140]\t\n",
      "Train: [63][120/140]\t\n",
      "Train: [63][130/140]\t\n",
      "Train: [63][140/140]\t\n",
      "Training Accuracy: 92.53%\n",
      "Training loss: 0.17649366174091216\n",
      "Validation accuracy: 77.72108843537416 %\n",
      "Validation F1: 0.5439630453810721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [64][10/140]\t\n",
      "Train: [64][20/140]\t\n",
      "Train: [64][30/140]\t\n",
      "Train: [64][40/140]\t\n",
      "Train: [64][50/140]\t\n",
      "Train: [64][60/140]\t\n",
      "Train: [64][70/140]\t\n",
      "Train: [64][80/140]\t\n",
      "Train: [64][90/140]\t\n",
      "Train: [64][100/140]\t\n",
      "Train: [64][110/140]\t\n",
      "Train: [64][120/140]\t\n",
      "Train: [64][130/140]\t\n",
      "Train: [64][140/140]\t\n",
      "Training Accuracy: 92.74%\n",
      "Training loss: 0.17327564644971272\n",
      "Validation accuracy: 78.87755102040816 %\n",
      "Validation F1: 0.5589525609601634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [65][10/140]\t\n",
      "Train: [65][20/140]\t\n",
      "Train: [65][30/140]\t\n",
      "Train: [65][40/140]\t\n",
      "Train: [65][50/140]\t\n",
      "Train: [65][60/140]\t\n",
      "Train: [65][70/140]\t\n",
      "Train: [65][80/140]\t\n",
      "Train: [65][90/140]\t\n",
      "Train: [65][100/140]\t\n",
      "Train: [65][110/140]\t\n",
      "Train: [65][120/140]\t\n",
      "Train: [65][130/140]\t\n",
      "Train: [65][140/140]\t\n",
      "Training Accuracy: 93.01%\n",
      "Training loss: 0.17188881984284224\n",
      "Validation accuracy: 81.08843537414965 %\n",
      "Validation F1: 0.5847372460718644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [66][10/140]\t\n",
      "Train: [66][20/140]\t\n",
      "Train: [66][30/140]\t\n",
      "Train: [66][40/140]\t\n",
      "Train: [66][50/140]\t\n",
      "Train: [66][60/140]\t\n",
      "Train: [66][70/140]\t\n",
      "Train: [66][80/140]\t\n",
      "Train: [66][90/140]\t\n",
      "Train: [66][100/140]\t\n",
      "Train: [66][110/140]\t\n",
      "Train: [66][120/140]\t\n",
      "Train: [66][130/140]\t\n",
      "Train: [66][140/140]\t\n",
      "Training Accuracy: 92.81%\n",
      "Training loss: 0.17048063786020726\n",
      "Validation accuracy: 82.44897959183673 %\n",
      "Validation F1: 0.6230754935455812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [67][10/140]\t\n",
      "Train: [67][20/140]\t\n",
      "Train: [67][30/140]\t\n",
      "Train: [67][40/140]\t\n",
      "Train: [67][50/140]\t\n",
      "Train: [67][60/140]\t\n",
      "Train: [67][70/140]\t\n",
      "Train: [67][80/140]\t\n",
      "Train: [67][90/140]\t\n",
      "Train: [67][100/140]\t\n",
      "Train: [67][110/140]\t\n",
      "Train: [67][120/140]\t\n",
      "Train: [67][130/140]\t\n",
      "Train: [67][140/140]\t\n",
      "Training Accuracy: 92.63%\n",
      "Training loss: 0.1748346937868354\n",
      "Validation accuracy: 79.08163265306123 %\n",
      "Validation F1: 0.5986570759641704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [68][10/140]\t\n",
      "Train: [68][20/140]\t\n",
      "Train: [68][30/140]\t\n",
      "Train: [68][40/140]\t\n",
      "Train: [68][50/140]\t\n",
      "Train: [68][60/140]\t\n",
      "Train: [68][70/140]\t\n",
      "Train: [68][80/140]\t\n",
      "Train: [68][90/140]\t\n",
      "Train: [68][100/140]\t\n",
      "Train: [68][110/140]\t\n",
      "Train: [68][120/140]\t\n",
      "Train: [68][130/140]\t\n",
      "Train: [68][140/140]\t\n",
      "Training Accuracy: 92.92%\n",
      "Training loss: 0.1671315905965746\n",
      "Validation accuracy: 80.17006802721089 %\n",
      "Validation F1: 0.6003774614127658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [69][10/140]\t\n",
      "Train: [69][20/140]\t\n",
      "Train: [69][30/140]\t\n",
      "Train: [69][40/140]\t\n",
      "Train: [69][50/140]\t\n",
      "Train: [69][60/140]\t\n",
      "Train: [69][70/140]\t\n",
      "Train: [69][80/140]\t\n",
      "Train: [69][90/140]\t\n",
      "Train: [69][100/140]\t\n",
      "Train: [69][110/140]\t\n",
      "Train: [69][120/140]\t\n",
      "Train: [69][130/140]\t\n",
      "Train: [69][140/140]\t\n",
      "Training Accuracy: 93.03%\n",
      "Training loss: 0.1693831222555006\n",
      "Validation accuracy: 77.89115646258503 %\n",
      "Validation F1: 0.5796923357697862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [70][10/140]\t\n",
      "Train: [70][20/140]\t\n",
      "Train: [70][30/140]\t\n",
      "Train: [70][40/140]\t\n",
      "Train: [70][50/140]\t\n",
      "Train: [70][60/140]\t\n",
      "Train: [70][70/140]\t\n",
      "Train: [70][80/140]\t\n",
      "Train: [70][90/140]\t\n",
      "Train: [70][100/140]\t\n",
      "Train: [70][110/140]\t\n",
      "Train: [70][120/140]\t\n",
      "Train: [70][130/140]\t\n",
      "Train: [70][140/140]\t\n",
      "Training Accuracy: 93.03%\n",
      "Training loss: 0.16836387671598754\n",
      "Validation accuracy: 77.10884353741496 %\n",
      "Validation F1: 0.5274149378029375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [71][10/140]\t\n",
      "Train: [71][20/140]\t\n",
      "Train: [71][30/140]\t\n",
      "Train: [71][40/140]\t\n",
      "Train: [71][50/140]\t\n",
      "Train: [71][60/140]\t\n",
      "Train: [71][70/140]\t\n",
      "Train: [71][80/140]\t\n",
      "Train: [71][90/140]\t\n",
      "Train: [71][100/140]\t\n",
      "Train: [71][110/140]\t\n",
      "Train: [71][120/140]\t\n",
      "Train: [71][130/140]\t\n",
      "Train: [71][140/140]\t\n",
      "Training Accuracy: 93.14%\n",
      "Training loss: 0.1654956615787901\n",
      "Validation accuracy: 81.12244897959184 %\n",
      "Validation F1: 0.6300173004851619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [72][10/140]\t\n",
      "Train: [72][20/140]\t\n",
      "Train: [72][30/140]\t\n",
      "Train: [72][40/140]\t\n",
      "Train: [72][50/140]\t\n",
      "Train: [72][60/140]\t\n",
      "Train: [72][70/140]\t\n",
      "Train: [72][80/140]\t\n",
      "Train: [72][90/140]\t\n",
      "Train: [72][100/140]\t\n",
      "Train: [72][110/140]\t\n",
      "Train: [72][120/140]\t\n",
      "Train: [72][130/140]\t\n",
      "Train: [72][140/140]\t\n",
      "Training Accuracy: 93.10%\n",
      "Training loss: 0.1679847508294616\n",
      "Validation accuracy: 80.06802721088435 %\n",
      "Validation F1: 0.5967047672100508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [73][10/140]\t\n",
      "Train: [73][20/140]\t\n",
      "Train: [73][30/140]\t\n",
      "Train: [73][40/140]\t\n",
      "Train: [73][50/140]\t\n",
      "Train: [73][60/140]\t\n",
      "Train: [73][70/140]\t\n",
      "Train: [73][80/140]\t\n",
      "Train: [73][90/140]\t\n",
      "Train: [73][100/140]\t\n",
      "Train: [73][110/140]\t\n",
      "Train: [73][120/140]\t\n",
      "Train: [73][130/140]\t\n",
      "Train: [73][140/140]\t\n",
      "Training Accuracy: 93.22%\n",
      "Training loss: 0.16314203505506428\n",
      "Validation accuracy: 82.0408163265306 %\n",
      "Validation F1: 0.6164420016450415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [74][10/140]\t\n",
      "Train: [74][20/140]\t\n",
      "Train: [74][30/140]\t\n",
      "Train: [74][40/140]\t\n",
      "Train: [74][50/140]\t\n",
      "Train: [74][60/140]\t\n",
      "Train: [74][70/140]\t\n",
      "Train: [74][80/140]\t\n",
      "Train: [74][90/140]\t\n",
      "Train: [74][100/140]\t\n",
      "Train: [74][110/140]\t\n",
      "Train: [74][120/140]\t\n",
      "Train: [74][130/140]\t\n",
      "Train: [74][140/140]\t\n",
      "Training Accuracy: 93.24%\n",
      "Training loss: 0.1644249767011579\n",
      "Validation accuracy: 79.1156462585034 %\n",
      "Validation F1: 0.5679731331917285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [75][10/140]\t\n",
      "Train: [75][20/140]\t\n",
      "Train: [75][30/140]\t\n",
      "Train: [75][40/140]\t\n",
      "Train: [75][50/140]\t\n",
      "Train: [75][60/140]\t\n",
      "Train: [75][70/140]\t\n",
      "Train: [75][80/140]\t\n",
      "Train: [75][90/140]\t\n",
      "Train: [75][100/140]\t\n",
      "Train: [75][110/140]\t\n",
      "Train: [75][120/140]\t\n",
      "Train: [75][130/140]\t\n",
      "Train: [75][140/140]\t\n",
      "Training Accuracy: 93.16%\n",
      "Training loss: 0.16497063664851047\n",
      "Validation accuracy: 77.99319727891157 %\n",
      "Validation F1: 0.5736211036453043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [76][10/140]\t\n",
      "Train: [76][20/140]\t\n",
      "Train: [76][30/140]\t\n",
      "Train: [76][40/140]\t\n",
      "Train: [76][50/140]\t\n",
      "Train: [76][60/140]\t\n",
      "Train: [76][70/140]\t\n",
      "Train: [76][80/140]\t\n",
      "Train: [76][90/140]\t\n",
      "Train: [76][100/140]\t\n",
      "Train: [76][110/140]\t\n",
      "Train: [76][120/140]\t\n",
      "Train: [76][130/140]\t\n",
      "Train: [76][140/140]\t\n",
      "Training Accuracy: 93.33%\n",
      "Training loss: 0.16286876211187234\n",
      "Validation accuracy: 78.46938775510203 %\n",
      "Validation F1: 0.5731946413808964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [77][10/140]\t\n",
      "Train: [77][20/140]\t\n",
      "Train: [77][30/140]\t\n",
      "Train: [77][40/140]\t\n",
      "Train: [77][50/140]\t\n",
      "Train: [77][60/140]\t\n",
      "Train: [77][70/140]\t\n",
      "Train: [77][80/140]\t\n",
      "Train: [77][90/140]\t\n",
      "Train: [77][100/140]\t\n",
      "Train: [77][110/140]\t\n",
      "Train: [77][120/140]\t\n",
      "Train: [77][130/140]\t\n",
      "Train: [77][140/140]\t\n",
      "Training Accuracy: 93.31%\n",
      "Training loss: 0.16137041159878132\n",
      "Validation accuracy: 76.7687074829932 %\n",
      "Validation F1: 0.556453872156266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [78][10/140]\t\n",
      "Train: [78][20/140]\t\n",
      "Train: [78][30/140]\t\n",
      "Train: [78][40/140]\t\n",
      "Train: [78][50/140]\t\n",
      "Train: [78][60/140]\t\n",
      "Train: [78][70/140]\t\n",
      "Train: [78][80/140]\t\n",
      "Train: [78][90/140]\t\n",
      "Train: [78][100/140]\t\n",
      "Train: [78][110/140]\t\n",
      "Train: [78][120/140]\t\n",
      "Train: [78][130/140]\t\n",
      "Train: [78][140/140]\t\n",
      "Training Accuracy: 93.18%\n",
      "Training loss: 0.16195058682943894\n",
      "Validation accuracy: 77.7891156462585 %\n",
      "Validation F1: 0.5899594361698363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [79][10/140]\t\n",
      "Train: [79][20/140]\t\n",
      "Train: [79][30/140]\t\n",
      "Train: [79][40/140]\t\n",
      "Train: [79][50/140]\t\n",
      "Train: [79][60/140]\t\n",
      "Train: [79][70/140]\t\n",
      "Train: [79][80/140]\t\n",
      "Train: [79][90/140]\t\n",
      "Train: [79][100/140]\t\n",
      "Train: [79][110/140]\t\n",
      "Train: [79][120/140]\t\n",
      "Train: [79][130/140]\t\n",
      "Train: [79][140/140]\t\n",
      "Training Accuracy: 93.40%\n",
      "Training loss: 0.16040515579846334\n",
      "Validation accuracy: 80.34013605442178 %\n",
      "Validation F1: 0.605316262875189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [80][10/140]\t\n",
      "Train: [80][20/140]\t\n",
      "Train: [80][30/140]\t\n",
      "Train: [80][40/140]\t\n",
      "Train: [80][50/140]\t\n",
      "Train: [80][60/140]\t\n",
      "Train: [80][70/140]\t\n",
      "Train: [80][80/140]\t\n",
      "Train: [80][90/140]\t\n",
      "Train: [80][100/140]\t\n",
      "Train: [80][110/140]\t\n",
      "Train: [80][120/140]\t\n",
      "Train: [80][130/140]\t\n",
      "Train: [80][140/140]\t\n",
      "Training Accuracy: 93.25%\n",
      "Training loss: 0.16083118422439371\n",
      "Validation accuracy: 81.42857142857143 %\n",
      "Validation F1: 0.6087230586346773\n"
     ]
    }
   ],
   "source": [
    "# training routine without freezing\n",
    "model[0].requires_grad_(True)\n",
    "opt.learning_rate = 0.001\n",
    "\n",
    "best_f1 = 0.5\n",
    "for epoch in range(1, 81):\n",
    "    train_supervised(train_loader, val_loader, model, criterion, optimizer, epoch, opt)\n",
    "    cur_f1 = sample_evaluation(val_loader, model, opt)\n",
    "    if cur_f1 > best_f1:\n",
    "        best_f1 = cur_f1\n",
    "        submission_generate(test_loader, model, opt, epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        submission_generate(test_loader, model, opt, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f966a817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T20:47:53.937430Z",
     "iopub.status.busy": "2023-08-26T20:47:53.937054Z",
     "iopub.status.idle": "2023-08-26T20:47:54.308149Z",
     "shell.execute_reply": "2023-08-26T20:47:54.307088Z"
    },
    "papermill": {
     "duration": 0.489757,
     "end_time": "2023-08-26T20:47:54.310876",
     "exception": false,
     "start_time": "2023-08-26T20:47:53.821119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving...\n"
     ]
    }
   ],
   "source": [
    "save_file = os.path.join('/kaggle/working/supervised/last.pth')\n",
    "save_model(model, optimizer, opt, opt.epochs, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab8117b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T20:47:54.547685Z",
     "iopub.status.busy": "2023-08-26T20:47:54.546634Z",
     "iopub.status.idle": "2023-08-26T20:47:59.955004Z",
     "shell.execute_reply": "2023-08-26T20:47:59.954052Z"
    },
    "papermill": {
     "duration": 5.529485,
     "end_time": "2023-08-26T20:47:59.957290",
     "exception": false,
     "start_time": "2023-08-26T20:47:54.427805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 81.42857142857143 %\n",
      "Validation F1: 0.6087230586346773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6087230586346773"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation\n",
    "sample_evaluation(val_loader, model, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f561ee02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T20:48:00.194597Z",
     "iopub.status.busy": "2023-08-26T20:48:00.194234Z",
     "iopub.status.idle": "2023-08-26T20:48:41.831965Z",
     "shell.execute_reply": "2023-08-26T20:48:41.830945Z"
    },
    "papermill": {
     "duration": 41.76045,
     "end_time": "2023-08-26T20:48:41.834496",
     "exception": false,
     "start_time": "2023-08-26T20:48:00.074046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_generate(test_loader, model, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928d9fc",
   "metadata": {
    "papermill": {
     "duration": 0.11772,
     "end_time": "2023-08-26T20:48:42.066875",
     "exception": false,
     "start_time": "2023-08-26T20:48:41.949155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea71dab",
   "metadata": {
    "papermill": {
     "duration": 0.11633,
     "end_time": "2023-08-26T20:48:42.353970",
     "exception": false,
     "start_time": "2023-08-26T20:48:42.237640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eceb1b3",
   "metadata": {
    "papermill": {
     "duration": 0.119755,
     "end_time": "2023-08-26T20:48:42.588951",
     "exception": false,
     "start_time": "2023-08-26T20:48:42.469196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be98a12",
   "metadata": {
    "papermill": {
     "duration": 0.166829,
     "end_time": "2023-08-26T20:48:42.923572",
     "exception": false,
     "start_time": "2023-08-26T20:48:42.756743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a606c",
   "metadata": {
    "papermill": {
     "duration": 0.186004,
     "end_time": "2023-08-26T20:48:43.279783",
     "exception": false,
     "start_time": "2023-08-26T20:48:43.093779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb4a39",
   "metadata": {
    "papermill": {
     "duration": 0.17936,
     "end_time": "2023-08-26T20:48:43.652537",
     "exception": false,
     "start_time": "2023-08-26T20:48:43.473177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcebe6",
   "metadata": {
    "papermill": {
     "duration": 0.173415,
     "end_time": "2023-08-26T20:48:44.004358",
     "exception": false,
     "start_time": "2023-08-26T20:48:43.830943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892bc3b",
   "metadata": {
    "papermill": {
     "duration": 0.141563,
     "end_time": "2023-08-26T20:48:44.317722",
     "exception": false,
     "start_time": "2023-08-26T20:48:44.176159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ef485",
   "metadata": {
    "papermill": {
     "duration": 0.117595,
     "end_time": "2023-08-26T20:48:44.561222",
     "exception": false,
     "start_time": "2023-08-26T20:48:44.443627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ad3e8",
   "metadata": {
    "papermill": {
     "duration": 0.115629,
     "end_time": "2023-08-26T20:48:44.794933",
     "exception": false,
     "start_time": "2023-08-26T20:48:44.679304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27f7cd",
   "metadata": {
    "papermill": {
     "duration": 0.120241,
     "end_time": "2023-08-26T20:48:45.037108",
     "exception": false,
     "start_time": "2023-08-26T20:48:44.916867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64857259",
   "metadata": {
    "papermill": {
     "duration": 0.117177,
     "end_time": "2023-08-26T20:48:45.274384",
     "exception": false,
     "start_time": "2023-08-26T20:48:45.157207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551001b8",
   "metadata": {
    "papermill": {
     "duration": 0.115861,
     "end_time": "2023-08-26T20:48:45.508735",
     "exception": false,
     "start_time": "2023-08-26T20:48:45.392874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e3f6b",
   "metadata": {
    "papermill": {
     "duration": 0.115334,
     "end_time": "2023-08-26T20:48:45.740124",
     "exception": false,
     "start_time": "2023-08-26T20:48:45.624790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76fa01",
   "metadata": {
    "papermill": {
     "duration": 0.113151,
     "end_time": "2023-08-26T20:48:45.967635",
     "exception": false,
     "start_time": "2023-08-26T20:48:45.854484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5057.444664,
   "end_time": "2023-08-26T20:48:49.236782",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-26T19:24:31.792118",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
