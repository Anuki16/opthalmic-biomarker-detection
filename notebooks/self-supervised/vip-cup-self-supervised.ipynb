{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport sys\nimport time\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport random\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-20T03:00:23.202879Z","iopub.execute_input":"2023-08-20T03:00:23.203836Z","iopub.status.idle":"2023-08-20T03:00:24.273493Z","shell.execute_reply.started":"2023-08-20T03:00:23.203789Z","shell.execute_reply":"2023-08-20T03:00:24.272524Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"\"\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""},"metadata":{}}]},{"cell_type":"code","source":"!pip install pytorch-metric-learning","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:24.275349Z","iopub.execute_input":"2023-08-20T03:00:24.276274Z","iopub.status.idle":"2023-08-20T03:00:36.847351Z","shell.execute_reply.started":"2023-08-20T03:00:24.276236Z","shell.execute_reply":"2023-08-20T03:00:36.846140Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pytorch-metric-learning\n  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (1.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (4.65.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (1.11.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\nInstalling collected packages: pytorch-metric-learning\nSuccessfully installed pytorch-metric-learning-2.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# model.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision\n\nclass ResNet(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=2):\n        super(ResNet, self).__init__()\n        if (name == 'resnet50'):\n            self.encoder = torchvision.models.resnet50(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(2048, num_classes)\n        else:\n            self.encoder = torchvision.models.resnet18(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(512, num_classes)\n    def forward(self, x):\n\n        return self.fc(self.encoder(x))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:36.849926Z","iopub.execute_input":"2023-08-20T03:00:36.851077Z","iopub.status.idle":"2023-08-20T03:00:40.396875Z","shell.execute_reply.started":"2023-08-20T03:00:36.851037Z","shell.execute_reply":"2023-08-20T03:00:40.395919Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Prj_Head(nn.Module):\n    def __init__(self,in_dim,feature_dim):\n        super(Prj_Head, self).__init__()\n        \n        self.g1 = nn.Sequential(nn.Linear(in_dim, 1024, bias=False),\n                               nn.BatchNorm1d(1024),\n                               nn.ReLU(inplace=True)\n                               )\n        self.g2 = nn.Sequential(nn.Linear(1024, 512, bias=False),\n                                nn.BatchNorm1d(512),\n                                nn.ReLU(inplace=True)\n                                )\n        self.g3=nn.Linear(512, feature_dim, bias=True)\n    def forward(self, x):\n        # print(x.shape)\n        x = torch.flatten(x, start_dim=1, end_dim=- 1) \n        x = self.g1(x)\n        x = self.g2(x)\n        x = self.g3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.399882Z","iopub.execute_input":"2023-08-20T03:00:40.401266Z","iopub.status.idle":"2023-08-20T03:00:40.409093Z","shell.execute_reply.started":"2023-08-20T03:00:40.401229Z","shell.execute_reply":"2023-08-20T03:00:40.408016Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Encdr(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=2):\n        super(Encdr, self).__init__()\n        self.encoder = torchvision.models.resnet50(pretrained=True, zero_init_residual=True)\n        self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n        self.encoder.fc = nn.Identity()\n        self.fc = nn.Linear(2048, 512)\n\n    def forward(self, x):\n\n        return self.fc(self.encoder(x))\n    \n    def add_feature(self):\n        self.fc1=nn.Linear(512,2)\n\n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.410410Z","iopub.execute_input":"2023-08-20T03:00:40.411062Z","iopub.status.idle":"2023-08-20T03:00:40.421031Z","shell.execute_reply.started":"2023-08-20T03:00:40.411026Z","shell.execute_reply":"2023-08-20T03:00:40.420090Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# datasets.py\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport os\n\nclass OLIVES(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        return image, bio_tensor\n\n\n\n\nclass RECOVERY(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        return image\n\n\n\nclass RECOVERY_TEST(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        return image, bio_tensor\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.422431Z","iopub.execute_input":"2023-08-20T03:00:40.422755Z","iopub.status.idle":"2023-08-20T03:00:40.440840Z","shell.execute_reply.started":"2023-08-20T03:00:40.422724Z","shell.execute_reply":"2023-08-20T03:00:40.439745Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# data_preprocessing.py\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport glob\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef combine_excel(csv_dir):\n    filenames = glob.glob(csv_dir + \"/*.xlsx\")\n    outputxlsx = pd.DataFrame()\n\n    for file in filenames:\n        df = pd.concat(pd.read_excel(file, sheet_name=None), ignore_index=True, sort=False)\n        outputxlsx = outputxlsx.append(df, ignore_index=True)\n\n    outputxlsx.to_csv('test_set_labels.csv',index=False)\n\ndef analyze_dataframe(csv_dir):\n    pass\n\ndef process_images(csv_dir):\n    df = pd.read_csv(csv_dir)\n\n    for i in tqdm(range(0,len(df))):\n        path = df.iloc[i,0]\n        im = Image.open(path).convert('L')\n\n\ndef numpy_submission(sub_dir,np_dir):\n    np_file  = np.load(np_dir)\n    print(len(np_file))\n    sub_dir = pd.read_csv(sub_dir)\n    print(len(sub_dir))\n    for i in range(0,len(sub_dir)):\n        sub_dir.iloc[i,1] = np_file[i,0]\n        sub_dir.iloc[i, 2] = np_file[i, 1]\n        sub_dir.iloc[i, 3] = np_file[i, 2]\n        sub_dir.iloc[i, 4] = np_file[i, 3]\n        sub_dir.iloc[i, 5] = np_file[i, 4]\n        sub_dir.iloc[i, 6] = np_file[i, 5]\n    print(sub_dir.head())\n    sub_dir.to_csv('baseline_result.csv',index=False)\n\n\n\n    #process_images(csv_dir)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.442183Z","iopub.execute_input":"2023-08-20T03:00:40.442511Z","iopub.status.idle":"2023-08-20T03:00:40.456532Z","shell.execute_reply.started":"2023-08-20T03:00:40.442480Z","shell.execute_reply":"2023-08-20T03:00:40.455663Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\n\nimport math\nimport numpy as np\nimport torch.optim as optim\nimport os\nfrom sklearn.metrics import f1_score\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import random_split, Subset\n\nimport torch.nn as nn\ndef set_model(opt, freeze = False):\n\n\n    device = opt.device\n    model = ResNet(name=opt.model,num_classes = opt.ncls)\n    if freeze:\n        model.encoder.requires_grad_(False)\n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n\n    return model, criterion\n\n\n# model for self supervised training\n\ndef set_model_st(opt,Net):\n\n\n    device = opt.device\n    #model = Encdr(name=opt.model,num_classes = opt.ncls)\n    model = nn.Sequential(\n    Net, \n    nn.ReLU(),\n    nn.Linear(512, 6))\n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n\n    return model, criterion\n\n\n\n\ndef set_loader(opt):\n    # construct data loader\n    if opt.dataset == 'OLIVES' or opt.dataset == 'RECOVERY':\n        mean = (.1706)\n        std = (.2112)\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n\n    normalize = transforms.Normalize(mean=mean, std=std)\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=224, scale=(0.2, 1.)),\n        transforms.RandomHorizontalFlip(),\n\n        transforms.RandomApply([\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n        ], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n\n    if opt.dataset =='OLIVES':\n        csv_path_train = opt.train_csv_path\n        csv_path_test = opt.test_csv_path\n        data_path_train = opt.train_image_path\n        data_path_test = opt.test_image_path\n        train_dataset = OLIVES(csv_path_train,data_path_train,transforms = train_transform)\n        unlabelled_train_dataset = RECOVERY(csv_path_unlabelled,data_path_train,transforms = val_transform)\n        test_dataset = RECOVERY(csv_path_test,data_path_test,transforms = val_transform)\n        train_dataset, val_dataset = random_split(train_dataset, [0.95, 0.05], generator=torch.Generator().manual_seed(42))\n        unlabelled_train_dataset = Subset(unlabelled_train_dataset, range(unlabel_count))\n    else:\n        raise ValueError(opt.dataset)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=opt.batch_size, shuffle=True,\n        num_workers=opt.num_workers, pin_memory=True)\n    \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n    \n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n    \n    unlabelled_train_loader = torch.utils.data.DataLoader(\n        unlabelled_train_dataset, batch_size=opt.batch_size, shuffle=True,\n        num_workers=opt.num_workers, pin_memory=True)\n\n    return train_loader, val_loader, test_loader, unlabelled_train_loader\n\n\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef adjust_learning_rate(args, optimizer, epoch):\n    lr = args.learning_rate\n    if args.cosine:\n        eta_min = lr * (args.lr_decay_rate ** 3)\n        lr = eta_min + (lr - eta_min) * (\n                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n    else:\n        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n        if steps > 0:\n            lr = lr * (args.lr_decay_rate ** steps)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n    if args.warm and epoch <= args.warm_epochs:\n        p = (batch_id + (epoch - 1) * total_batches) / \\\n            (args.warm_epochs * total_batches)\n        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n\ndef set_optimizer(opt, model):\n\n    optimizer = optim.SGD(model.parameters(),\n                          lr=opt.learning_rate,\n                          momentum=opt.momentum,\n                          weight_decay=opt.weight_decay)\n\n\n    return optimizer\n\n\ndef save_model(model, optimizer, opt, epoch, save_file):\n    print('==> Saving...')\n    state = {\n        'opt': opt,\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'epoch': epoch,\n    }\n    torch.save(state, save_file)\n    del state","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.457914Z","iopub.execute_input":"2023-08-20T03:00:40.458386Z","iopub.status.idle":"2023-08-20T03:00:40.489092Z","shell.execute_reply.started":"2023-08-20T03:00:40.458354Z","shell.execute_reply":"2023-08-20T03:00:40.488081Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#----------------------------------------------------------------------------------------------------\n# Augmentations\nfrom torchvision import transforms\nclass GaussianBlur(object):\n    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n    \"\"\"Borrowed from MoCo implementation\"\"\"\n\n    def __init__(self, sigma=[.1, 2.]):\n        self.sigma = sigma\n\n    def __call__(self, x):\n        sigma = random.uniform(self.sigma[0], self.sigma[1])\n        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n        return x\n    \nclass FixedRandomRotation:\n    \"\"\"Rotate by one of the given angles.\"\"\"\n    def __init__(self, angles):\n        self.angles = angles\n\n    def __call__(self, x):\n        angle = random.choice(self.angles)\n        return transforms.functional.rotate(x, angle)\n    \ndef torchvision_transforms(eval=False, aug=None):\n\n    trans = []\n\n    if aug[\"resize\"]:\n        trans.append(transforms.Resize(aug[\"resize\"]))\n\n    if aug[\"randcrop\"] and aug[\"scale\"] and not eval:\n        trans.append(transforms.RandomResizedCrop(aug[\"randcrop\"], scale=aug[\"scale\"]))\n\n    if aug[\"randcrop\"] and eval:\n        trans.append(transforms.CenterCrop(aug[\"randcrop\"]))\n\n    if aug[\"flip\"] and not eval:\n        trans.append(transforms.RandomHorizontalFlip(p=0.5))\n        trans.append(transforms.RandomVerticalFlip(p=0.5))\n\n    if aug[\"jitter_d\"] and not eval:\n        trans.append(transforms.RandomApply(\n            [transforms.ColorJitter(0.8*aug[\"jitter_d\"], 0.8*aug[\"jitter_d\"], 0.8*aug[\"jitter_d\"], 0.2*aug[\"jitter_d\"])],\n             p=aug[\"jitter_p\"]))\n\n    if aug[\"gaussian_blur\"] and not eval:\n        trans.append(transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1,.2))], p=aug[\"gaussian_blur\"]))\n\n    if aug[\"rotation\"] and not eval:\n        # rotation_transform = FixedRandomRotation(angles=[0, 90, 180, 270])\n        trans.append(FixedRandomRotation(angles=[0, 90, 180, 270]))\n\n\n    trans = transforms.Compose(trans)\n   \n    return trans\naug = {\"resize\":0,\n    \"randcrop\":224,\n      \"scale\": (0.25, 1.0),\n      \"flip\":0,\n      \"jitter_d\":0.3,\n       \"jitter_p\":0.3,\n       \"gaussian_blur\":0.5,\n       \"rotation\":1\n      }\naugmentations = torchvision_transforms(aug = aug)\nprint(augmentations)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.490337Z","iopub.execute_input":"2023-08-20T03:00:40.491500Z","iopub.status.idle":"2023-08-20T03:00:40.509214Z","shell.execute_reply.started":"2023-08-20T03:00:40.491468Z","shell.execute_reply":"2023-08-20T03:00:40.508332Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Compose(\n    RandomResizedCrop(size=(224, 224), scale=(0.25, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=warn)\n    RandomApply(\n    p=0.3\n    ColorJitter(brightness=(0.76, 1.24), contrast=(0.76, 1.24), saturation=(0.76, 1.24), hue=(-0.06, 0.06))\n)\n    RandomApply(\n    p=0.5\n    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.2))\n)\n    <__main__.FixedRandomRotation object at 0x795f480b5c30>\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# config.py\n\nimport argparse\nimport math\nimport os\n\ndef parse_option(string):\n    parser = argparse.ArgumentParser('argument for training')\n\n    parser.add_argument('--print_freq', type=int, default=10,\n                        help='print frequency')\n    parser.add_argument('--save_freq', type=int, default=50,\n                        help='save frequency')\n    parser.add_argument('--batch_size', type=int, default=128,\n                        help='batch_size')\n    parser.add_argument('--num_workers', type=int, default=8,\n                        help='num of workers to use')\n    parser.add_argument('--epochs', type=int, default=100,\n                        help='number of training epochs')\n    parser.add_argument('--device', type=str, default='cuda:0')\n    # optimization\n    parser.add_argument('--learning_rate', type=float, default=0.05,\n                        help='learning rate')\n    parser.add_argument('--patient_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--cluster_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--lr_decay_epochs', type=str, default='100',\n                        help='where to decay lr, can be a list')\n    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n                        help='decay rate for learning rate')\n    parser.add_argument('--weight_decay', type=float, default=1e-4,\n                        help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='momentum')\n    parser.add_argument('--train_csv_path', type=str, default='train data csv')\n    parser.add_argument('--test_csv_path', type=str, default='test data csv')\n    parser.add_argument('--train_image_path', type=str, default='train data csv')\n    parser.add_argument('--test_image_path', type=str, default='test data csv')\n\n    parser.add_argument('--parallel', type=int, default=1, help='data parallel')\n    parser.add_argument('--ncls', type=int, default=6, help='Number of Classes')\n    # model dataset\n    parser.add_argument('--model', type=str, default='resnet50')\n    parser.add_argument('--dataset', type=str, default='TREX_DME',\n                        choices=[ 'OLIVES'], help='dataset')\n    parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n    parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n    parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n    parser.add_argument('--size', type=int, default=128, help='parameter for RandomResizedCrop')\n\n    # temperature\n    parser.add_argument('--temp', type=float, default=0.07,\n                        help='temperature for loss function')\n\n\n\n    opt = parser.parse_args(string)\n\n    # check if dataset is path that passed required arguments\n    if opt.dataset == 'path':\n        assert opt.data_folder is not None \\\n               and opt.mean is not None \\\n               and opt.std is not None\n\n    # set the path according to the environment\n    if opt.data_folder is None:\n        opt.data_folder = './datasets/'\n    opt.model_path = './save/{}_models'.format(opt.dataset)\n\n    iterations = opt.lr_decay_epochs.split(',')\n    opt.lr_decay_epochs = list([])\n    for it in iterations:\n        opt.lr_decay_epochs.append(int(it))\n\n    opt.model_name = '{}_lr_{}_decay_{}_bsz_{}_temp_{}'. \\\n        format(opt.model, opt.learning_rate,\n               opt.weight_decay, opt.batch_size, opt.temp)\n\n\n    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n    if not os.path.isdir(opt.save_folder):\n        os.makedirs(opt.save_folder)\n\n    return opt","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.514140Z","iopub.execute_input":"2023-08-20T03:00:40.514907Z","iopub.status.idle":"2023-08-20T03:00:40.532801Z","shell.execute_reply.started":"2023-08-20T03:00:40.514875Z","shell.execute_reply":"2023-08-20T03:00:40.531869Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from pytorch_metric_learning.losses import NTXentLoss\nss_loss_func = NTXentLoss(temperature=0.10)\n\ndef train_ss(Net,projection_head,data_loader, epoch):\n    Net.train()\n    projection_head.train()\n    total_loss = AverageMeter()\n    for idx, x in enumerate(data_loader): \n        # print(batch_idx)\n        optimizer.zero_grad()\n        x = x.to(device)\n        # Get data representations\n        \n        A1 = augmentations(x)\n        A2 = augmentations(x)\n        \n        h1 = Net(A1)\n        z1 = projection_head(h1)\n        \n        h2 = Net(A2)\n        z2 = projection_head(h2)\n        \n        # Prepare for loss\n        embeddings = torch.cat((z1, z2))\n        # The same index corresponds to a positive pair\n        indices = torch.arange(0, z1.size(0), device=z2.device)\n        labels = torch.cat((indices, indices))\n        loss = ss_loss_func(embeddings, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss.update(loss.data.item())\n        \n        # print info\n        if (idx + 1) % 50 == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(\n                epoch, idx + 1, len(data_loader)))\n            \n    return total_loss.avg","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.534054Z","iopub.execute_input":"2023-08-20T03:00:40.535104Z","iopub.status.idle":"2023-08-20T03:00:40.565897Z","shell.execute_reply.started":"2023-08-20T03:00:40.535071Z","shell.execute_reply":"2023-08-20T03:00:40.564917Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def train_supervised(train_loader, model,criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    device = opt.device\n    end = time.time()\n    correct_predictions = 0\n\n    for idx, (image, bio_tensor) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        images = image.to(device)\n\n        labels = bio_tensor.float()\n\n        labels = labels.to(device)\n        bsz = labels.shape[0]\n\n        # compute loss\n        output = model(images)\n        loss = criterion(output, labels)\n        \n        # Calculate training accuracy\n        predicted_labels = torch.round(torch.sigmoid(output)) \n        correct_predictions += (predicted_labels == labels).sum().item()\n\n        # update metric\n        losses.update(loss.item(), bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % 10 == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(\n                epoch, idx + 1, len(train_loader)))\n\n            sys.stdout.flush()\n\n    total_values = len(train_loader.dataset) * 6\n    training_accuracy = (correct_predictions / total_values) * 100.0\n    print(f\"Training Accuracy: {training_accuracy:.2f}%\")\n    print(losses.avg)\n    return losses.avg\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.569255Z","iopub.execute_input":"2023-08-20T03:00:40.570990Z","iopub.status.idle":"2023-08-20T03:00:40.581131Z","shell.execute_reply.started":"2023-08-20T03:00:40.570957Z","shell.execute_reply":"2023-08-20T03:00:40.580241Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def submission_generate(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    with torch.no_grad():\n        for idx, image in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n\n            # forward\n            output = model(images)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n\n\n    out_submisison = np.array(out_list)\n    np.save('output',out_submisison)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.583492Z","iopub.execute_input":"2023-08-20T03:00:40.584286Z","iopub.status.idle":"2023-08-20T03:00:40.595985Z","shell.execute_reply.started":"2023-08-20T03:00:40.584261Z","shell.execute_reply":"2023-08-20T03:00:40.595137Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def sample_evaluation(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    label_list = []\n    correct_count = 0\n    total_count = 0\n\n    with torch.no_grad():\n        for idx, (image,bio_tensor) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            labels = bio_tensor.float().to(device)\n\n            labels = labels.float()\n\n            label_list.append(labels.squeeze().detach().cpu().numpy())\n            # forward\n            output = model(images)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n            \n            correct_count += (labels == output).sum().item()\n            total_count += len(labels) * 6\n        \n    print((correct_count / total_count) * 100, \"%\")\n\n    label_array = np.array(label_list)\n    out_array = np.array(out_list)\n    f = f1_score(label_array,out_array,average='macro')\n    print(f)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.597439Z","iopub.execute_input":"2023-08-20T03:00:40.598356Z","iopub.status.idle":"2023-08-20T03:00:40.608721Z","shell.execute_reply.started":"2023-08-20T03:00:40.598324Z","shell.execute_reply":"2023-08-20T03:00:40.607766Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = args = ['--batch_size', '64', '--model', \"resnet50\", '--dataset', 'OLIVES', '--epochs', '2', '--device', 'cuda:0', '--train_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TRAIN/OLIVES', '--test_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/', '--test_csv_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv', '--train_csv_path', '/kaggle/input/olives-training-labels/Training_Biomarker_Data.csv']\nopt = parse_option(args)\ncsv_path_unlabelled = \"/kaggle/input/olives-training-labels/unlabelled_images.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.610291Z","iopub.execute_input":"2023-08-20T03:00:40.610693Z","iopub.status.idle":"2023-08-20T03:00:40.626702Z","shell.execute_reply.started":"2023-08-20T03:00:40.610593Z","shell.execute_reply":"2023-08-20T03:00:40.625822Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# build data loader\nunlabel_count = 20000\ntrain_loader, val_loader, test_loader, unlabelled_train_loader = set_loader(opt)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.628026Z","iopub.execute_input":"2023-08-20T03:00:40.628663Z","iopub.status.idle":"2023-08-20T03:00:40.920382Z","shell.execute_reply.started":"2023-08-20T03:00:40.628631Z","shell.execute_reply":"2023-08-20T03:00:40.919454Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"len(unlabelled_train_loader)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.921851Z","iopub.execute_input":"2023-08-20T03:00:40.922394Z","iopub.status.idle":"2023-08-20T03:00:40.929378Z","shell.execute_reply.started":"2023-08-20T03:00:40.922357Z","shell.execute_reply":"2023-08-20T03:00:40.928246Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"313"},"metadata":{}}]},{"cell_type":"code","source":"# build optimizer\n#optimizer = set_optimizer(opt, model)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.931088Z","iopub.execute_input":"2023-08-20T03:00:40.931447Z","iopub.status.idle":"2023-08-20T03:00:40.939240Z","shell.execute_reply.started":"2023-08-20T03:00:40.931415Z","shell.execute_reply":"2023-08-20T03:00:40.938358Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#--------------------------------------------------------------------------\ndevice = torch.device(\"cuda:0\" )\nNet = Encdr().to(device)\nprojection_head = Prj_Head(512,128).to(device)\n\noptimizer = torch.optim.Adam(list(Net.parameters())+list(projection_head.parameters()), lr=0.001)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n\n#train(Net,projection_head,train_loader)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:40.940474Z","iopub.execute_input":"2023-08-20T03:00:40.940964Z","iopub.status.idle":"2023-08-20T03:00:46.096910Z","shell.execute_reply.started":"2023-08-20T03:00:40.940932Z","shell.execute_reply":"2023-08-20T03:00:46.095886Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:01<00:00, 69.2MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"n_epoch = 20\nloss_list = []\nbest_loss = 10000\nsave_file = os.path.join(opt.save_folder, 'last.pth')\n#save_model(model, optimizer, opt, opt.epochs, save_file)\n\nfor epoch in range(1, n_epoch):\n    loss = train_ss(Net,projection_head,unlabelled_train_loader, epoch)\n    print(f'Epoch {epoch:3d}, Loss: {loss:.4f}')\n    scheduler.step()\n    loss_list.append(loss)\n    if loss<best_loss:\n        best_loss = loss\n        #torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_best_loss.pth.tar') \n        save_model(Net, optimizer, opt, opt.epochs, save_file)\n    elif n_epoch%10 ==0:\n        best_loss = loss\n        #torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_checkpoint.pth.tar') \n        save_model(Net, optimizer, opt, opt.epochs, save_file)\n#torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_final_checkpoint.pth.tar') \nplt.figure()\nplt.plot(loss_list)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T03:00:46.098382Z","iopub.execute_input":"2023-08-20T03:00:46.098716Z","iopub.status.idle":"2023-08-20T04:01:51.445217Z","shell.execute_reply.started":"2023-08-20T03:00:46.098679Z","shell.execute_reply":"2023-08-20T04:01:51.444176Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Train: [1][50/313]\t\nTrain: [1][100/313]\t\nTrain: [1][150/313]\t\nTrain: [1][200/313]\t\nTrain: [1][250/313]\t\nTrain: [1][300/313]\t\nEpoch   1, Loss: 2.6353\n==> Saving...\nTrain: [2][50/313]\t\nTrain: [2][100/313]\t\nTrain: [2][150/313]\t\nTrain: [2][200/313]\t\nTrain: [2][250/313]\t\nTrain: [2][300/313]\t\nEpoch   2, Loss: 1.7904\n==> Saving...\nTrain: [3][50/313]\t\nTrain: [3][100/313]\t\nTrain: [3][150/313]\t\nTrain: [3][200/313]\t\nTrain: [3][250/313]\t\nTrain: [3][300/313]\t\nEpoch   3, Loss: 1.4043\n==> Saving...\nTrain: [4][50/313]\t\nTrain: [4][100/313]\t\nTrain: [4][150/313]\t\nTrain: [4][200/313]\t\nTrain: [4][250/313]\t\nTrain: [4][300/313]\t\nEpoch   4, Loss: 0.9617\n==> Saving...\nTrain: [5][50/313]\t\nTrain: [5][100/313]\t\nTrain: [5][150/313]\t\nTrain: [5][200/313]\t\nTrain: [5][250/313]\t\nTrain: [5][300/313]\t\nEpoch   5, Loss: 0.8988\n==> Saving...\nTrain: [6][50/313]\t\nTrain: [6][100/313]\t\nTrain: [6][150/313]\t\nTrain: [6][200/313]\t\nTrain: [6][250/313]\t\nTrain: [6][300/313]\t\nEpoch   6, Loss: 0.7550\n==> Saving...\nTrain: [7][50/313]\t\nTrain: [7][100/313]\t\nTrain: [7][150/313]\t\nTrain: [7][200/313]\t\nTrain: [7][250/313]\t\nTrain: [7][300/313]\t\nEpoch   7, Loss: 0.5644\n==> Saving...\nTrain: [8][50/313]\t\nTrain: [8][100/313]\t\nTrain: [8][150/313]\t\nTrain: [8][200/313]\t\nTrain: [8][250/313]\t\nTrain: [8][300/313]\t\nEpoch   8, Loss: 0.6043\n==> Saving...\nTrain: [9][50/313]\t\nTrain: [9][100/313]\t\nTrain: [9][150/313]\t\nTrain: [9][200/313]\t\nTrain: [9][250/313]\t\nTrain: [9][300/313]\t\nEpoch   9, Loss: 0.4728\n==> Saving...\nTrain: [10][50/313]\t\nTrain: [10][100/313]\t\nTrain: [10][150/313]\t\nTrain: [10][200/313]\t\nTrain: [10][250/313]\t\nTrain: [10][300/313]\t\nEpoch  10, Loss: 0.5004\n==> Saving...\nTrain: [11][50/313]\t\nTrain: [11][100/313]\t\nTrain: [11][150/313]\t\nTrain: [11][200/313]\t\nTrain: [11][250/313]\t\nTrain: [11][300/313]\t\nEpoch  11, Loss: 0.5404\n==> Saving...\nTrain: [12][50/313]\t\nTrain: [12][100/313]\t\nTrain: [12][150/313]\t\nTrain: [12][200/313]\t\nTrain: [12][250/313]\t\nTrain: [12][300/313]\t\nEpoch  12, Loss: 0.4126\n==> Saving...\nTrain: [13][50/313]\t\nTrain: [13][100/313]\t\nTrain: [13][150/313]\t\nTrain: [13][200/313]\t\nTrain: [13][250/313]\t\nTrain: [13][300/313]\t\nEpoch  13, Loss: 0.4316\n==> Saving...\nTrain: [14][50/313]\t\nTrain: [14][100/313]\t\nTrain: [14][150/313]\t\nTrain: [14][200/313]\t\nTrain: [14][250/313]\t\nTrain: [14][300/313]\t\nEpoch  14, Loss: 0.3589\n==> Saving...\nTrain: [15][50/313]\t\nTrain: [15][100/313]\t\nTrain: [15][150/313]\t\nTrain: [15][200/313]\t\nTrain: [15][250/313]\t\nTrain: [15][300/313]\t\nEpoch  15, Loss: 0.3674\n==> Saving...\nTrain: [16][50/313]\t\nTrain: [16][100/313]\t\nTrain: [16][150/313]\t\nTrain: [16][200/313]\t\nTrain: [16][250/313]\t\nTrain: [16][300/313]\t\nEpoch  16, Loss: 0.3526\n==> Saving...\nTrain: [17][50/313]\t\nTrain: [17][100/313]\t\nTrain: [17][150/313]\t\nTrain: [17][200/313]\t\nTrain: [17][250/313]\t\nTrain: [17][300/313]\t\nEpoch  17, Loss: 0.3140\n==> Saving...\nTrain: [18][50/313]\t\nTrain: [18][100/313]\t\nTrain: [18][150/313]\t\nTrain: [18][200/313]\t\nTrain: [18][250/313]\t\nTrain: [18][300/313]\t\nEpoch  18, Loss: 0.3183\n==> Saving...\nTrain: [19][50/313]\t\nTrain: [19][100/313]\t\nTrain: [19][150/313]\t\nTrain: [19][200/313]\t\nTrain: [19][250/313]\t\nTrain: [19][300/313]\t\nEpoch  19, Loss: 0.2778\n==> Saving...\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x795f47e6a8c0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"#Net.add_feature()\nmodel, criterion = set_model_st(opt, Net)    \noptimizer = set_optimizer(opt, model)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T04:01:51.447145Z","iopub.execute_input":"2023-08-20T04:01:51.447515Z","iopub.status.idle":"2023-08-20T04:01:51.460888Z","shell.execute_reply.started":"2023-08-20T04:01:51.447475Z","shell.execute_reply":"2023-08-20T04:01:51.459824Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# To check performance without the pre training\nnew_model, new_criterion = set_model(opt)    \nnew_optimizer = set_optimizer(opt, new_model)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T04:01:51.462188Z","iopub.execute_input":"2023-08-20T04:01:51.463078Z","iopub.status.idle":"2023-08-20T04:01:51.888386Z","shell.execute_reply.started":"2023-08-20T04:01:51.463043Z","shell.execute_reply":"2023-08-20T04:01:51.887382Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# training routine\nfor epoch in range(1, 100+1):\n    train_supervised(train_loader, model, criterion, optimizer, epoch, opt)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T04:01:51.889885Z","iopub.execute_input":"2023-08-20T04:01:51.890352Z","iopub.status.idle":"2023-08-20T05:15:53.406870Z","shell.execute_reply.started":"2023-08-20T04:01:51.890306Z","shell.execute_reply":"2023-08-20T05:15:53.405549Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Train: [1][10/140]\t\nTrain: [1][20/140]\t\nTrain: [1][30/140]\t\nTrain: [1][40/140]\t\nTrain: [1][50/140]\t\nTrain: [1][60/140]\t\nTrain: [1][70/140]\t\nTrain: [1][80/140]\t\nTrain: [1][90/140]\t\nTrain: [1][100/140]\t\nTrain: [1][110/140]\t\nTrain: [1][120/140]\t\nTrain: [1][130/140]\t\nTrain: [1][140/140]\t\nTraining Accuracy: 67.84%\n0.6697520944170036\nTrain: [2][10/140]\t\nTrain: [2][20/140]\t\nTrain: [2][30/140]\t\nTrain: [2][40/140]\t\nTrain: [2][50/140]\t\nTrain: [2][60/140]\t\nTrain: [2][70/140]\t\nTrain: [2][80/140]\t\nTrain: [2][90/140]\t\nTrain: [2][100/140]\t\nTrain: [2][110/140]\t\nTrain: [2][120/140]\t\nTrain: [2][130/140]\t\nTrain: [2][140/140]\t\nTraining Accuracy: 73.39%\n0.5235951840410725\nTrain: [3][10/140]\t\nTrain: [3][20/140]\t\nTrain: [3][30/140]\t\nTrain: [3][40/140]\t\nTrain: [3][50/140]\t\nTrain: [3][60/140]\t\nTrain: [3][70/140]\t\nTrain: [3][80/140]\t\nTrain: [3][90/140]\t\nTrain: [3][100/140]\t\nTrain: [3][110/140]\t\nTrain: [3][120/140]\t\nTrain: [3][130/140]\t\nTrain: [3][140/140]\t\nTraining Accuracy: 75.76%\n0.4893760000947178\nTrain: [4][10/140]\t\nTrain: [4][20/140]\t\nTrain: [4][30/140]\t\nTrain: [4][40/140]\t\nTrain: [4][50/140]\t\nTrain: [4][60/140]\t\nTrain: [4][70/140]\t\nTrain: [4][80/140]\t\nTrain: [4][90/140]\t\nTrain: [4][100/140]\t\nTrain: [4][110/140]\t\nTrain: [4][120/140]\t\nTrain: [4][130/140]\t\nTrain: [4][140/140]\t\nTraining Accuracy: 76.85%\n0.47328239179185694\nTrain: [5][10/140]\t\nTrain: [5][20/140]\t\nTrain: [5][30/140]\t\nTrain: [5][40/140]\t\nTrain: [5][50/140]\t\nTrain: [5][60/140]\t\nTrain: [5][70/140]\t\nTrain: [5][80/140]\t\nTrain: [5][90/140]\t\nTrain: [5][100/140]\t\nTrain: [5][110/140]\t\nTrain: [5][120/140]\t\nTrain: [5][130/140]\t\nTrain: [5][140/140]\t\nTraining Accuracy: 77.72%\n0.459815921666612\nTrain: [6][10/140]\t\nTrain: [6][20/140]\t\nTrain: [6][30/140]\t\nTrain: [6][40/140]\t\nTrain: [6][50/140]\t\nTrain: [6][60/140]\t\nTrain: [6][70/140]\t\nTrain: [6][80/140]\t\nTrain: [6][90/140]\t\nTrain: [6][100/140]\t\nTrain: [6][110/140]\t\nTrain: [6][120/140]\t\nTrain: [6][130/140]\t\nTrain: [6][140/140]\t\nTraining Accuracy: 78.00%\n0.4568573200521279\nTrain: [7][10/140]\t\nTrain: [7][20/140]\t\nTrain: [7][30/140]\t\nTrain: [7][40/140]\t\nTrain: [7][50/140]\t\nTrain: [7][60/140]\t\nTrain: [7][70/140]\t\nTrain: [7][80/140]\t\nTrain: [7][90/140]\t\nTrain: [7][100/140]\t\nTrain: [7][110/140]\t\nTrain: [7][120/140]\t\nTrain: [7][130/140]\t\nTrain: [7][140/140]\t\nTraining Accuracy: 78.43%\n0.4501062581325223\nTrain: [8][10/140]\t\nTrain: [8][20/140]\t\nTrain: [8][30/140]\t\nTrain: [8][40/140]\t\nTrain: [8][50/140]\t\nTrain: [8][60/140]\t\nTrain: [8][70/140]\t\nTrain: [8][80/140]\t\nTrain: [8][90/140]\t\nTrain: [8][100/140]\t\nTrain: [8][110/140]\t\nTrain: [8][120/140]\t\nTrain: [8][130/140]\t\nTrain: [8][140/140]\t\nTraining Accuracy: 79.02%\n0.4407864825107769\nTrain: [9][10/140]\t\nTrain: [9][20/140]\t\nTrain: [9][30/140]\t\nTrain: [9][40/140]\t\nTrain: [9][50/140]\t\nTrain: [9][60/140]\t\nTrain: [9][70/140]\t\nTrain: [9][80/140]\t\nTrain: [9][90/140]\t\nTrain: [9][100/140]\t\nTrain: [9][110/140]\t\nTrain: [9][120/140]\t\nTrain: [9][130/140]\t\nTrain: [9][140/140]\t\nTraining Accuracy: 79.58%\n0.4354670208808586\nTrain: [10][10/140]\t\nTrain: [10][20/140]\t\nTrain: [10][30/140]\t\nTrain: [10][40/140]\t\nTrain: [10][50/140]\t\nTrain: [10][60/140]\t\nTrain: [10][70/140]\t\nTrain: [10][80/140]\t\nTrain: [10][90/140]\t\nTrain: [10][100/140]\t\nTrain: [10][110/140]\t\nTrain: [10][120/140]\t\nTrain: [10][130/140]\t\nTrain: [10][140/140]\t\nTraining Accuracy: 80.07%\n0.42488086685074816\nTrain: [11][10/140]\t\nTrain: [11][20/140]\t\nTrain: [11][30/140]\t\nTrain: [11][40/140]\t\nTrain: [11][50/140]\t\nTrain: [11][60/140]\t\nTrain: [11][70/140]\t\nTrain: [11][80/140]\t\nTrain: [11][90/140]\t\nTrain: [11][100/140]\t\nTrain: [11][110/140]\t\nTrain: [11][120/140]\t\nTrain: [11][130/140]\t\nTrain: [11][140/140]\t\nTraining Accuracy: 80.83%\n0.41715432179670364\nTrain: [12][10/140]\t\nTrain: [12][20/140]\t\nTrain: [12][30/140]\t\nTrain: [12][40/140]\t\nTrain: [12][50/140]\t\nTrain: [12][60/140]\t\nTrain: [12][70/140]\t\nTrain: [12][80/140]\t\nTrain: [12][90/140]\t\nTrain: [12][100/140]\t\nTrain: [12][110/140]\t\nTrain: [12][120/140]\t\nTrain: [12][130/140]\t\nTrain: [12][140/140]\t\nTraining Accuracy: 81.23%\n0.40784360898850774\nTrain: [13][10/140]\t\nTrain: [13][20/140]\t\nTrain: [13][30/140]\t\nTrain: [13][40/140]\t\nTrain: [13][50/140]\t\nTrain: [13][60/140]\t\nTrain: [13][70/140]\t\nTrain: [13][80/140]\t\nTrain: [13][90/140]\t\nTrain: [13][100/140]\t\nTrain: [13][110/140]\t\nTrain: [13][120/140]\t\nTrain: [13][130/140]\t\nTrain: [13][140/140]\t\nTraining Accuracy: 81.39%\n0.40479144425993935\nTrain: [14][10/140]\t\nTrain: [14][20/140]\t\nTrain: [14][30/140]\t\nTrain: [14][40/140]\t\nTrain: [14][50/140]\t\nTrain: [14][60/140]\t\nTrain: [14][70/140]\t\nTrain: [14][80/140]\t\nTrain: [14][90/140]\t\nTrain: [14][100/140]\t\nTrain: [14][110/140]\t\nTrain: [14][120/140]\t\nTrain: [14][130/140]\t\nTrain: [14][140/140]\t\nTraining Accuracy: 81.45%\n0.40324519989100527\nTrain: [15][10/140]\t\nTrain: [15][20/140]\t\nTrain: [15][30/140]\t\nTrain: [15][40/140]\t\nTrain: [15][50/140]\t\nTrain: [15][60/140]\t\nTrain: [15][70/140]\t\nTrain: [15][80/140]\t\nTrain: [15][90/140]\t\nTrain: [15][100/140]\t\nTrain: [15][110/140]\t\nTrain: [15][120/140]\t\nTrain: [15][130/140]\t\nTrain: [15][140/140]\t\nTraining Accuracy: 81.82%\n0.3947777289436231\nTrain: [16][10/140]\t\nTrain: [16][20/140]\t\nTrain: [16][30/140]\t\nTrain: [16][40/140]\t\nTrain: [16][50/140]\t\nTrain: [16][60/140]\t\nTrain: [16][70/140]\t\nTrain: [16][80/140]\t\nTrain: [16][90/140]\t\nTrain: [16][100/140]\t\nTrain: [16][110/140]\t\nTrain: [16][120/140]\t\nTrain: [16][130/140]\t\nTrain: [16][140/140]\t\nTraining Accuracy: 81.79%\n0.3967629548993306\nTrain: [17][10/140]\t\nTrain: [17][20/140]\t\nTrain: [17][30/140]\t\nTrain: [17][40/140]\t\nTrain: [17][50/140]\t\nTrain: [17][60/140]\t\nTrain: [17][70/140]\t\nTrain: [17][80/140]\t\nTrain: [17][90/140]\t\nTrain: [17][100/140]\t\nTrain: [17][110/140]\t\nTrain: [17][120/140]\t\nTrain: [17][130/140]\t\nTrain: [17][140/140]\t\nTraining Accuracy: 81.96%\n0.39358191135799786\nTrain: [18][10/140]\t\nTrain: [18][20/140]\t\nTrain: [18][30/140]\t\nTrain: [18][40/140]\t\nTrain: [18][50/140]\t\nTrain: [18][60/140]\t\nTrain: [18][70/140]\t\nTrain: [18][80/140]\t\nTrain: [18][90/140]\t\nTrain: [18][100/140]\t\nTrain: [18][110/140]\t\nTrain: [18][120/140]\t\nTrain: [18][130/140]\t\nTrain: [18][140/140]\t\nTraining Accuracy: 82.31%\n0.3922046632088119\nTrain: [19][10/140]\t\nTrain: [19][20/140]\t\nTrain: [19][30/140]\t\nTrain: [19][40/140]\t\nTrain: [19][50/140]\t\nTrain: [19][60/140]\t\nTrain: [19][70/140]\t\nTrain: [19][80/140]\t\nTrain: [19][90/140]\t\nTrain: [19][100/140]\t\nTrain: [19][110/140]\t\nTrain: [19][120/140]\t\nTrain: [19][130/140]\t\nTrain: [19][140/140]\t\nTraining Accuracy: 82.26%\n0.38783587733753366\nTrain: [20][10/140]\t\nTrain: [20][20/140]\t\nTrain: [20][30/140]\t\nTrain: [20][40/140]\t\nTrain: [20][50/140]\t\nTrain: [20][60/140]\t\nTrain: [20][70/140]\t\nTrain: [20][80/140]\t\nTrain: [20][90/140]\t\nTrain: [20][100/140]\t\nTrain: [20][110/140]\t\nTrain: [20][120/140]\t\nTrain: [20][130/140]\t\nTrain: [20][140/140]\t\nTraining Accuracy: 82.54%\n0.3865614043876719\nTrain: [21][10/140]\t\nTrain: [21][20/140]\t\nTrain: [21][30/140]\t\nTrain: [21][40/140]\t\nTrain: [21][50/140]\t\nTrain: [21][60/140]\t\nTrain: [21][70/140]\t\nTrain: [21][80/140]\t\nTrain: [21][90/140]\t\nTrain: [21][100/140]\t\nTrain: [21][110/140]\t\nTrain: [21][120/140]\t\nTrain: [21][130/140]\t\nTrain: [21][140/140]\t\nTraining Accuracy: 83.03%\n0.37485675794162265\nTrain: [22][10/140]\t\nTrain: [22][20/140]\t\nTrain: [22][30/140]\t\nTrain: [22][40/140]\t\nTrain: [22][50/140]\t\nTrain: [22][60/140]\t\nTrain: [22][70/140]\t\nTrain: [22][80/140]\t\nTrain: [22][90/140]\t\nTrain: [22][100/140]\t\nTrain: [22][110/140]\t\nTrain: [22][120/140]\t\nTrain: [22][130/140]\t\nTrain: [22][140/140]\t\nTraining Accuracy: 82.70%\n0.3811810411751257\nTrain: [23][10/140]\t\nTrain: [23][20/140]\t\nTrain: [23][30/140]\t\nTrain: [23][40/140]\t\nTrain: [23][50/140]\t\nTrain: [23][60/140]\t\nTrain: [23][70/140]\t\nTrain: [23][80/140]\t\nTrain: [23][90/140]\t\nTrain: [23][100/140]\t\nTrain: [23][110/140]\t\nTrain: [23][120/140]\t\nTrain: [23][130/140]\t\nTrain: [23][140/140]\t\nTraining Accuracy: 83.04%\n0.3736355869221618\nTrain: [24][10/140]\t\nTrain: [24][20/140]\t\nTrain: [24][30/140]\t\nTrain: [24][40/140]\t\nTrain: [24][50/140]\t\nTrain: [24][60/140]\t\nTrain: [24][70/140]\t\nTrain: [24][80/140]\t\nTrain: [24][90/140]\t\nTrain: [24][100/140]\t\nTrain: [24][110/140]\t\nTrain: [24][120/140]\t\nTrain: [24][130/140]\t\nTrain: [24][140/140]\t\nTraining Accuracy: 83.27%\n0.3700911033124192\nTrain: [25][10/140]\t\nTrain: [25][20/140]\t\nTrain: [25][30/140]\t\nTrain: [25][40/140]\t\nTrain: [25][50/140]\t\nTrain: [25][60/140]\t\nTrain: [25][70/140]\t\nTrain: [25][80/140]\t\nTrain: [25][90/140]\t\nTrain: [25][100/140]\t\nTrain: [25][110/140]\t\nTrain: [25][120/140]\t\nTrain: [25][130/140]\t\nTrain: [25][140/140]\t\nTraining Accuracy: 82.96%\n0.3728867539305312\nTrain: [26][10/140]\t\nTrain: [26][20/140]\t\nTrain: [26][30/140]\t\nTrain: [26][40/140]\t\nTrain: [26][50/140]\t\nTrain: [26][60/140]\t\nTrain: [26][70/140]\t\nTrain: [26][80/140]\t\nTrain: [26][90/140]\t\nTrain: [26][100/140]\t\nTrain: [26][110/140]\t\nTrain: [26][120/140]\t\nTrain: [26][130/140]\t\nTrain: [26][140/140]\t\nTraining Accuracy: 83.45%\n0.3675434000341651\nTrain: [27][10/140]\t\nTrain: [27][20/140]\t\nTrain: [27][30/140]\t\nTrain: [27][40/140]\t\nTrain: [27][50/140]\t\nTrain: [27][60/140]\t\nTrain: [27][70/140]\t\nTrain: [27][80/140]\t\nTrain: [27][90/140]\t\nTrain: [27][100/140]\t\nTrain: [27][110/140]\t\nTrain: [27][120/140]\t\nTrain: [27][130/140]\t\nTrain: [27][140/140]\t\nTraining Accuracy: 83.54%\n0.3648436962624582\nTrain: [28][10/140]\t\nTrain: [28][20/140]\t\nTrain: [28][30/140]\t\nTrain: [28][40/140]\t\nTrain: [28][50/140]\t\nTrain: [28][60/140]\t\nTrain: [28][70/140]\t\nTrain: [28][80/140]\t\nTrain: [28][90/140]\t\nTrain: [28][100/140]\t\nTrain: [28][110/140]\t\nTrain: [28][120/140]\t\nTrain: [28][130/140]\t\nTrain: [28][140/140]\t\nTraining Accuracy: 83.69%\n0.36366864846813834\nTrain: [29][10/140]\t\nTrain: [29][20/140]\t\nTrain: [29][30/140]\t\nTrain: [29][40/140]\t\nTrain: [29][50/140]\t\nTrain: [29][60/140]\t\nTrain: [29][70/140]\t\nTrain: [29][80/140]\t\nTrain: [29][90/140]\t\nTrain: [29][100/140]\t\nTrain: [29][110/140]\t\nTrain: [29][120/140]\t\nTrain: [29][130/140]\t\nTrain: [29][140/140]\t\nTraining Accuracy: 83.48%\n0.36556816007045856\nTrain: [30][10/140]\t\nTrain: [30][20/140]\t\nTrain: [30][30/140]\t\nTrain: [30][40/140]\t\nTrain: [30][50/140]\t\nTrain: [30][60/140]\t\nTrain: [30][70/140]\t\nTrain: [30][80/140]\t\nTrain: [30][90/140]\t\nTrain: [30][100/140]\t\nTrain: [30][110/140]\t\nTrain: [30][120/140]\t\nTrain: [30][130/140]\t\nTrain: [30][140/140]\t\nTraining Accuracy: 83.64%\n0.3598675483237126\nTrain: [31][10/140]\t\nTrain: [31][20/140]\t\nTrain: [31][30/140]\t\nTrain: [31][40/140]\t\nTrain: [31][50/140]\t\nTrain: [31][60/140]\t\nTrain: [31][70/140]\t\nTrain: [31][80/140]\t\nTrain: [31][90/140]\t\nTrain: [31][100/140]\t\nTrain: [31][110/140]\t\nTrain: [31][120/140]\t\nTrain: [31][130/140]\t\nTrain: [31][140/140]\t\nTraining Accuracy: 84.12%\n0.3563104677477734\nTrain: [32][10/140]\t\nTrain: [32][20/140]\t\nTrain: [32][30/140]\t\nTrain: [32][40/140]\t\nTrain: [32][50/140]\t\nTrain: [32][60/140]\t\nTrain: [32][70/140]\t\nTrain: [32][80/140]\t\nTrain: [32][90/140]\t\nTrain: [32][100/140]\t\nTrain: [32][110/140]\t\nTrain: [32][120/140]\t\nTrain: [32][130/140]\t\nTrain: [32][140/140]\t\nTraining Accuracy: 84.13%\n0.3546245620604942\nTrain: [33][10/140]\t\nTrain: [33][20/140]\t\nTrain: [33][30/140]\t\nTrain: [33][40/140]\t\nTrain: [33][50/140]\t\nTrain: [33][60/140]\t\nTrain: [33][70/140]\t\nTrain: [33][80/140]\t\nTrain: [33][90/140]\t\nTrain: [33][100/140]\t\nTrain: [33][110/140]\t\nTrain: [33][120/140]\t\nTrain: [33][130/140]\t\nTrain: [33][140/140]\t\nTraining Accuracy: 84.13%\n0.3529439454351823\nTrain: [34][10/140]\t\nTrain: [34][20/140]\t\nTrain: [34][30/140]\t\nTrain: [34][40/140]\t\nTrain: [34][50/140]\t\nTrain: [34][60/140]\t\nTrain: [34][70/140]\t\nTrain: [34][80/140]\t\nTrain: [34][90/140]\t\nTrain: [34][100/140]\t\nTrain: [34][110/140]\t\nTrain: [34][120/140]\t\nTrain: [34][130/140]\t\nTrain: [34][140/140]\t\nTraining Accuracy: 84.33%\n0.3507762166058145\nTrain: [35][10/140]\t\nTrain: [35][20/140]\t\nTrain: [35][30/140]\t\nTrain: [35][40/140]\t\nTrain: [35][50/140]\t\nTrain: [35][60/140]\t\nTrain: [35][70/140]\t\nTrain: [35][80/140]\t\nTrain: [35][90/140]\t\nTrain: [35][100/140]\t\nTrain: [35][110/140]\t\nTrain: [35][120/140]\t\nTrain: [35][130/140]\t\nTrain: [35][140/140]\t\nTraining Accuracy: 84.36%\n0.35043314948901416\nTrain: [36][10/140]\t\nTrain: [36][20/140]\t\nTrain: [36][30/140]\t\nTrain: [36][40/140]\t\nTrain: [36][50/140]\t\nTrain: [36][60/140]\t\nTrain: [36][70/140]\t\nTrain: [36][80/140]\t\nTrain: [36][90/140]\t\nTrain: [36][100/140]\t\nTrain: [36][110/140]\t\nTrain: [36][120/140]\t\nTrain: [36][130/140]\t\nTrain: [36][140/140]\t\nTraining Accuracy: 84.38%\n0.3498138558827249\nTrain: [37][10/140]\t\nTrain: [37][20/140]\t\nTrain: [37][30/140]\t\nTrain: [37][40/140]\t\nTrain: [37][50/140]\t\nTrain: [37][60/140]\t\nTrain: [37][70/140]\t\nTrain: [37][80/140]\t\nTrain: [37][90/140]\t\nTrain: [37][100/140]\t\nTrain: [37][110/140]\t\nTrain: [37][120/140]\t\nTrain: [37][130/140]\t\nTrain: [37][140/140]\t\nTraining Accuracy: 84.47%\n0.3460394804057298\nTrain: [38][10/140]\t\nTrain: [38][20/140]\t\nTrain: [38][30/140]\t\nTrain: [38][40/140]\t\nTrain: [38][50/140]\t\nTrain: [38][60/140]\t\nTrain: [38][70/140]\t\nTrain: [38][80/140]\t\nTrain: [38][90/140]\t\nTrain: [38][100/140]\t\nTrain: [38][110/140]\t\nTrain: [38][120/140]\t\nTrain: [38][130/140]\t\nTrain: [38][140/140]\t\nTraining Accuracy: 84.44%\n0.3456389389867136\nTrain: [39][10/140]\t\nTrain: [39][20/140]\t\nTrain: [39][30/140]\t\nTrain: [39][40/140]\t\nTrain: [39][50/140]\t\nTrain: [39][60/140]\t\nTrain: [39][70/140]\t\nTrain: [39][80/140]\t\nTrain: [39][90/140]\t\nTrain: [39][100/140]\t\nTrain: [39][110/140]\t\nTrain: [39][120/140]\t\nTrain: [39][130/140]\t\nTrain: [39][140/140]\t\nTraining Accuracy: 84.38%\n0.3453613030187607\nTrain: [40][10/140]\t\nTrain: [40][20/140]\t\nTrain: [40][30/140]\t\nTrain: [40][40/140]\t\nTrain: [40][50/140]\t\nTrain: [40][60/140]\t\nTrain: [40][70/140]\t\nTrain: [40][80/140]\t\nTrain: [40][90/140]\t\nTrain: [40][100/140]\t\nTrain: [40][110/140]\t\nTrain: [40][120/140]\t\nTrain: [40][130/140]\t\nTrain: [40][140/140]\t\nTraining Accuracy: 84.81%\n0.3405697250238172\nTrain: [41][10/140]\t\nTrain: [41][20/140]\t\nTrain: [41][30/140]\t\nTrain: [41][40/140]\t\nTrain: [41][50/140]\t\nTrain: [41][60/140]\t\nTrain: [41][70/140]\t\nTrain: [41][80/140]\t\nTrain: [41][90/140]\t\nTrain: [41][100/140]\t\nTrain: [41][110/140]\t\nTrain: [41][120/140]\t\nTrain: [41][130/140]\t\nTrain: [41][140/140]\t\nTraining Accuracy: 84.71%\n0.34106416677863144\nTrain: [42][10/140]\t\nTrain: [42][20/140]\t\nTrain: [42][30/140]\t\nTrain: [42][40/140]\t\nTrain: [42][50/140]\t\nTrain: [42][60/140]\t\nTrain: [42][70/140]\t\nTrain: [42][80/140]\t\nTrain: [42][90/140]\t\nTrain: [42][100/140]\t\nTrain: [42][110/140]\t\nTrain: [42][120/140]\t\nTrain: [42][130/140]\t\nTrain: [42][140/140]\t\nTraining Accuracy: 84.76%\n0.33937199418111036\nTrain: [43][10/140]\t\nTrain: [43][20/140]\t\nTrain: [43][30/140]\t\nTrain: [43][40/140]\t\nTrain: [43][50/140]\t\nTrain: [43][60/140]\t\nTrain: [43][70/140]\t\nTrain: [43][80/140]\t\nTrain: [43][90/140]\t\nTrain: [43][100/140]\t\nTrain: [43][110/140]\t\nTrain: [43][120/140]\t\nTrain: [43][130/140]\t\nTrain: [43][140/140]\t\nTraining Accuracy: 85.10%\n0.3341827769958618\nTrain: [44][10/140]\t\nTrain: [44][20/140]\t\nTrain: [44][30/140]\t\nTrain: [44][40/140]\t\nTrain: [44][50/140]\t\nTrain: [44][60/140]\t\nTrain: [44][70/140]\t\nTrain: [44][80/140]\t\nTrain: [44][90/140]\t\nTrain: [44][100/140]\t\nTrain: [44][110/140]\t\nTrain: [44][120/140]\t\nTrain: [44][130/140]\t\nTrain: [44][140/140]\t\nTraining Accuracy: 85.02%\n0.33544177749221504\nTrain: [45][10/140]\t\nTrain: [45][20/140]\t\nTrain: [45][30/140]\t\nTrain: [45][40/140]\t\nTrain: [45][50/140]\t\nTrain: [45][60/140]\t\nTrain: [45][70/140]\t\nTrain: [45][80/140]\t\nTrain: [45][90/140]\t\nTrain: [45][100/140]\t\nTrain: [45][110/140]\t\nTrain: [45][120/140]\t\nTrain: [45][130/140]\t\nTrain: [45][140/140]\t\nTraining Accuracy: 85.06%\n0.33347608990305405\nTrain: [46][10/140]\t\nTrain: [46][20/140]\t\nTrain: [46][30/140]\t\nTrain: [46][40/140]\t\nTrain: [46][50/140]\t\nTrain: [46][60/140]\t\nTrain: [46][70/140]\t\nTrain: [46][80/140]\t\nTrain: [46][90/140]\t\nTrain: [46][100/140]\t\nTrain: [46][110/140]\t\nTrain: [46][120/140]\t\nTrain: [46][130/140]\t\nTrain: [46][140/140]\t\nTraining Accuracy: 85.17%\n0.3346848987071246\nTrain: [47][10/140]\t\nTrain: [47][20/140]\t\nTrain: [47][30/140]\t\nTrain: [47][40/140]\t\nTrain: [47][50/140]\t\nTrain: [47][60/140]\t\nTrain: [47][70/140]\t\nTrain: [47][80/140]\t\nTrain: [47][90/140]\t\nTrain: [47][100/140]\t\nTrain: [47][110/140]\t\nTrain: [47][120/140]\t\nTrain: [47][130/140]\t\nTrain: [47][140/140]\t\nTraining Accuracy: 85.02%\n0.3353743348633264\nTrain: [48][10/140]\t\nTrain: [48][20/140]\t\nTrain: [48][30/140]\t\nTrain: [48][40/140]\t\nTrain: [48][50/140]\t\nTrain: [48][60/140]\t\nTrain: [48][70/140]\t\nTrain: [48][80/140]\t\nTrain: [48][90/140]\t\nTrain: [48][100/140]\t\nTrain: [48][110/140]\t\nTrain: [48][120/140]\t\nTrain: [48][130/140]\t\nTrain: [48][140/140]\t\nTraining Accuracy: 85.12%\n0.3324630523480193\nTrain: [49][10/140]\t\nTrain: [49][20/140]\t\nTrain: [49][30/140]\t\nTrain: [49][40/140]\t\nTrain: [49][50/140]\t\nTrain: [49][60/140]\t\nTrain: [49][70/140]\t\nTrain: [49][80/140]\t\nTrain: [49][90/140]\t\nTrain: [49][100/140]\t\nTrain: [49][110/140]\t\nTrain: [49][120/140]\t\nTrain: [49][130/140]\t\nTrain: [49][140/140]\t\nTraining Accuracy: 85.16%\n0.32931742693626204\nTrain: [50][10/140]\t\nTrain: [50][20/140]\t\nTrain: [50][30/140]\t\nTrain: [50][40/140]\t\nTrain: [50][50/140]\t\nTrain: [50][60/140]\t\nTrain: [50][70/140]\t\nTrain: [50][80/140]\t\nTrain: [50][90/140]\t\nTrain: [50][100/140]\t\nTrain: [50][110/140]\t\nTrain: [50][120/140]\t\nTrain: [50][130/140]\t\nTrain: [50][140/140]\t\nTraining Accuracy: 85.29%\n0.32801220390481367\nTrain: [51][10/140]\t\nTrain: [51][20/140]\t\nTrain: [51][30/140]\t\nTrain: [51][40/140]\t\nTrain: [51][50/140]\t\nTrain: [51][60/140]\t\nTrain: [51][70/140]\t\nTrain: [51][80/140]\t\nTrain: [51][90/140]\t\nTrain: [51][100/140]\t\nTrain: [51][110/140]\t\nTrain: [51][120/140]\t\nTrain: [51][130/140]\t\nTrain: [51][140/140]\t\nTraining Accuracy: 85.15%\n0.33008516350964034\nTrain: [52][10/140]\t\nTrain: [52][20/140]\t\nTrain: [52][30/140]\t\nTrain: [52][40/140]\t\nTrain: [52][50/140]\t\nTrain: [52][60/140]\t\nTrain: [52][70/140]\t\nTrain: [52][80/140]\t\nTrain: [52][90/140]\t\nTrain: [52][100/140]\t\nTrain: [52][110/140]\t\nTrain: [52][120/140]\t\nTrain: [52][130/140]\t\nTrain: [52][140/140]\t\nTraining Accuracy: 85.46%\n0.3251556948237463\nTrain: [53][10/140]\t\nTrain: [53][20/140]\t\nTrain: [53][30/140]\t\nTrain: [53][40/140]\t\nTrain: [53][50/140]\t\nTrain: [53][60/140]\t\nTrain: [53][70/140]\t\nTrain: [53][80/140]\t\nTrain: [53][90/140]\t\nTrain: [53][100/140]\t\nTrain: [53][110/140]\t\nTrain: [53][120/140]\t\nTrain: [53][130/140]\t\nTrain: [53][140/140]\t\nTraining Accuracy: 85.24%\n0.3303556027535652\nTrain: [54][10/140]\t\nTrain: [54][20/140]\t\nTrain: [54][30/140]\t\nTrain: [54][40/140]\t\nTrain: [54][50/140]\t\nTrain: [54][60/140]\t\nTrain: [54][70/140]\t\nTrain: [54][80/140]\t\nTrain: [54][90/140]\t\nTrain: [54][100/140]\t\nTrain: [54][110/140]\t\nTrain: [54][120/140]\t\nTrain: [54][130/140]\t\nTrain: [54][140/140]\t\nTraining Accuracy: 85.47%\n0.3284290929279703\nTrain: [55][10/140]\t\nTrain: [55][20/140]\t\nTrain: [55][30/140]\t\nTrain: [55][40/140]\t\nTrain: [55][50/140]\t\nTrain: [55][60/140]\t\nTrain: [55][70/140]\t\nTrain: [55][80/140]\t\nTrain: [55][90/140]\t\nTrain: [55][100/140]\t\nTrain: [55][110/140]\t\nTrain: [55][120/140]\t\nTrain: [55][130/140]\t\nTrain: [55][140/140]\t\nTraining Accuracy: 85.49%\n0.3230105592090029\nTrain: [56][10/140]\t\nTrain: [56][20/140]\t\nTrain: [56][30/140]\t\nTrain: [56][40/140]\t\nTrain: [56][50/140]\t\nTrain: [56][60/140]\t\nTrain: [56][70/140]\t\nTrain: [56][80/140]\t\nTrain: [56][90/140]\t\nTrain: [56][100/140]\t\nTrain: [56][110/140]\t\nTrain: [56][120/140]\t\nTrain: [56][130/140]\t\nTrain: [56][140/140]\t\nTraining Accuracy: 85.53%\n0.3239919328868296\nTrain: [57][10/140]\t\nTrain: [57][20/140]\t\nTrain: [57][30/140]\t\nTrain: [57][40/140]\t\nTrain: [57][50/140]\t\nTrain: [57][60/140]\t\nTrain: [57][70/140]\t\nTrain: [57][80/140]\t\nTrain: [57][90/140]\t\nTrain: [57][100/140]\t\nTrain: [57][110/140]\t\nTrain: [57][120/140]\t\nTrain: [57][130/140]\t\nTrain: [57][140/140]\t\nTraining Accuracy: 85.87%\n0.3176637814332506\nTrain: [58][10/140]\t\nTrain: [58][20/140]\t\nTrain: [58][30/140]\t\nTrain: [58][40/140]\t\nTrain: [58][50/140]\t\nTrain: [58][60/140]\t\nTrain: [58][70/140]\t\nTrain: [58][80/140]\t\nTrain: [58][90/140]\t\nTrain: [58][100/140]\t\nTrain: [58][110/140]\t\nTrain: [58][120/140]\t\nTrain: [58][130/140]\t\nTrain: [58][140/140]\t\nTraining Accuracy: 86.01%\n0.3169692419110915\nTrain: [59][10/140]\t\nTrain: [59][20/140]\t\nTrain: [59][30/140]\t\nTrain: [59][40/140]\t\nTrain: [59][50/140]\t\nTrain: [59][60/140]\t\nTrain: [59][70/140]\t\nTrain: [59][80/140]\t\nTrain: [59][90/140]\t\nTrain: [59][100/140]\t\nTrain: [59][110/140]\t\nTrain: [59][120/140]\t\nTrain: [59][130/140]\t\nTrain: [59][140/140]\t\nTraining Accuracy: 85.62%\n0.32312896237540867\nTrain: [60][10/140]\t\nTrain: [60][20/140]\t\nTrain: [60][30/140]\t\nTrain: [60][40/140]\t\nTrain: [60][50/140]\t\nTrain: [60][60/140]\t\nTrain: [60][70/140]\t\nTrain: [60][80/140]\t\nTrain: [60][90/140]\t\nTrain: [60][100/140]\t\nTrain: [60][110/140]\t\nTrain: [60][120/140]\t\nTrain: [60][130/140]\t\nTrain: [60][140/140]\t\nTraining Accuracy: 85.84%\n0.3196708144864718\nTrain: [61][10/140]\t\nTrain: [61][20/140]\t\nTrain: [61][30/140]\t\nTrain: [61][40/140]\t\nTrain: [61][50/140]\t\nTrain: [61][60/140]\t\nTrain: [61][70/140]\t\nTrain: [61][80/140]\t\nTrain: [61][90/140]\t\nTrain: [61][100/140]\t\nTrain: [61][110/140]\t\nTrain: [61][120/140]\t\nTrain: [61][130/140]\t\nTrain: [61][140/140]\t\nTraining Accuracy: 85.91%\n0.3169884059980802\nTrain: [62][10/140]\t\nTrain: [62][20/140]\t\nTrain: [62][30/140]\t\nTrain: [62][40/140]\t\nTrain: [62][50/140]\t\nTrain: [62][60/140]\t\nTrain: [62][70/140]\t\nTrain: [62][80/140]\t\nTrain: [62][90/140]\t\nTrain: [62][100/140]\t\nTrain: [62][110/140]\t\nTrain: [62][120/140]\t\nTrain: [62][130/140]\t\nTrain: [62][140/140]\t\nTraining Accuracy: 85.80%\n0.31687623629182693\nTrain: [63][10/140]\t\nTrain: [63][20/140]\t\nTrain: [63][30/140]\t\nTrain: [63][40/140]\t\nTrain: [63][50/140]\t\nTrain: [63][60/140]\t\nTrain: [63][70/140]\t\nTrain: [63][80/140]\t\nTrain: [63][90/140]\t\nTrain: [63][100/140]\t\nTrain: [63][110/140]\t\nTrain: [63][120/140]\t\nTrain: [63][130/140]\t\nTrain: [63][140/140]\t\nTraining Accuracy: 86.13%\n0.313659864501362\nTrain: [64][10/140]\t\nTrain: [64][20/140]\t\nTrain: [64][30/140]\t\nTrain: [64][40/140]\t\nTrain: [64][50/140]\t\nTrain: [64][60/140]\t\nTrain: [64][70/140]\t\nTrain: [64][80/140]\t\nTrain: [64][90/140]\t\nTrain: [64][100/140]\t\nTrain: [64][110/140]\t\nTrain: [64][120/140]\t\nTrain: [64][130/140]\t\nTrain: [64][140/140]\t\nTraining Accuracy: 85.86%\n0.3168099822547778\nTrain: [65][10/140]\t\nTrain: [65][20/140]\t\nTrain: [65][30/140]\t\nTrain: [65][40/140]\t\nTrain: [65][50/140]\t\nTrain: [65][60/140]\t\nTrain: [65][70/140]\t\nTrain: [65][80/140]\t\nTrain: [65][90/140]\t\nTrain: [65][100/140]\t\nTrain: [65][110/140]\t\nTrain: [65][120/140]\t\nTrain: [65][130/140]\t\nTrain: [65][140/140]\t\nTraining Accuracy: 86.17%\n0.31405311326741586\nTrain: [66][10/140]\t\nTrain: [66][20/140]\t\nTrain: [66][30/140]\t\nTrain: [66][40/140]\t\nTrain: [66][50/140]\t\nTrain: [66][60/140]\t\nTrain: [66][70/140]\t\nTrain: [66][80/140]\t\nTrain: [66][90/140]\t\nTrain: [66][100/140]\t\nTrain: [66][110/140]\t\nTrain: [66][120/140]\t\nTrain: [66][130/140]\t\nTrain: [66][140/140]\t\nTraining Accuracy: 85.95%\n0.31346947542237713\nTrain: [67][10/140]\t\nTrain: [67][20/140]\t\nTrain: [67][30/140]\t\nTrain: [67][40/140]\t\nTrain: [67][50/140]\t\nTrain: [67][60/140]\t\nTrain: [67][70/140]\t\nTrain: [67][80/140]\t\nTrain: [67][90/140]\t\nTrain: [67][100/140]\t\nTrain: [67][110/140]\t\nTrain: [67][120/140]\t\nTrain: [67][130/140]\t\nTrain: [67][140/140]\t\nTraining Accuracy: 85.94%\n0.31567608297177946\nTrain: [68][10/140]\t\nTrain: [68][20/140]\t\nTrain: [68][30/140]\t\nTrain: [68][40/140]\t\nTrain: [68][50/140]\t\nTrain: [68][60/140]\t\nTrain: [68][70/140]\t\nTrain: [68][80/140]\t\nTrain: [68][90/140]\t\nTrain: [68][100/140]\t\nTrain: [68][110/140]\t\nTrain: [68][120/140]\t\nTrain: [68][130/140]\t\nTrain: [68][140/140]\t\nTraining Accuracy: 86.06%\n0.31101138269269085\nTrain: [69][10/140]\t\nTrain: [69][20/140]\t\nTrain: [69][30/140]\t\nTrain: [69][40/140]\t\nTrain: [69][50/140]\t\nTrain: [69][60/140]\t\nTrain: [69][70/140]\t\nTrain: [69][80/140]\t\nTrain: [69][90/140]\t\nTrain: [69][100/140]\t\nTrain: [69][110/140]\t\nTrain: [69][120/140]\t\nTrain: [69][130/140]\t\nTrain: [69][140/140]\t\nTraining Accuracy: 86.36%\n0.31095846461140086\nTrain: [70][10/140]\t\nTrain: [70][20/140]\t\nTrain: [70][30/140]\t\nTrain: [70][40/140]\t\nTrain: [70][50/140]\t\nTrain: [70][60/140]\t\nTrain: [70][70/140]\t\nTrain: [70][80/140]\t\nTrain: [70][90/140]\t\nTrain: [70][100/140]\t\nTrain: [70][110/140]\t\nTrain: [70][120/140]\t\nTrain: [70][130/140]\t\nTrain: [70][140/140]\t\nTraining Accuracy: 86.07%\n0.31247024170422505\nTrain: [71][10/140]\t\nTrain: [71][20/140]\t\nTrain: [71][30/140]\t\nTrain: [71][40/140]\t\nTrain: [71][50/140]\t\nTrain: [71][60/140]\t\nTrain: [71][70/140]\t\nTrain: [71][80/140]\t\nTrain: [71][90/140]\t\nTrain: [71][100/140]\t\nTrain: [71][110/140]\t\nTrain: [71][120/140]\t\nTrain: [71][130/140]\t\nTrain: [71][140/140]\t\nTraining Accuracy: 86.31%\n0.30889329568975377\nTrain: [72][10/140]\t\nTrain: [72][20/140]\t\nTrain: [72][30/140]\t\nTrain: [72][40/140]\t\nTrain: [72][50/140]\t\nTrain: [72][60/140]\t\nTrain: [72][70/140]\t\nTrain: [72][80/140]\t\nTrain: [72][90/140]\t\nTrain: [72][100/140]\t\nTrain: [72][110/140]\t\nTrain: [72][120/140]\t\nTrain: [72][130/140]\t\nTrain: [72][140/140]\t\nTraining Accuracy: 86.37%\n0.306232146233567\nTrain: [73][10/140]\t\nTrain: [73][20/140]\t\nTrain: [73][30/140]\t\nTrain: [73][40/140]\t\nTrain: [73][50/140]\t\nTrain: [73][60/140]\t\nTrain: [73][70/140]\t\nTrain: [73][80/140]\t\nTrain: [73][90/140]\t\nTrain: [73][100/140]\t\nTrain: [73][110/140]\t\nTrain: [73][120/140]\t\nTrain: [73][130/140]\t\nTrain: [73][140/140]\t\nTraining Accuracy: 86.42%\n0.30659442261098135\nTrain: [74][10/140]\t\nTrain: [74][20/140]\t\nTrain: [74][30/140]\t\nTrain: [74][40/140]\t\nTrain: [74][50/140]\t\nTrain: [74][60/140]\t\nTrain: [74][70/140]\t\nTrain: [74][80/140]\t\nTrain: [74][90/140]\t\nTrain: [74][100/140]\t\nTrain: [74][110/140]\t\nTrain: [74][120/140]\t\nTrain: [74][130/140]\t\nTrain: [74][140/140]\t\nTraining Accuracy: 86.50%\n0.3066673663191199\nTrain: [75][10/140]\t\nTrain: [75][20/140]\t\nTrain: [75][30/140]\t\nTrain: [75][40/140]\t\nTrain: [75][50/140]\t\nTrain: [75][60/140]\t\nTrain: [75][70/140]\t\nTrain: [75][80/140]\t\nTrain: [75][90/140]\t\nTrain: [75][100/140]\t\nTrain: [75][110/140]\t\nTrain: [75][120/140]\t\nTrain: [75][130/140]\t\nTrain: [75][140/140]\t\nTraining Accuracy: 86.51%\n0.30650571480803535\nTrain: [76][10/140]\t\nTrain: [76][20/140]\t\nTrain: [76][30/140]\t\nTrain: [76][40/140]\t\nTrain: [76][50/140]\t\nTrain: [76][60/140]\t\nTrain: [76][70/140]\t\nTrain: [76][80/140]\t\nTrain: [76][90/140]\t\nTrain: [76][100/140]\t\nTrain: [76][110/140]\t\nTrain: [76][120/140]\t\nTrain: [76][130/140]\t\nTrain: [76][140/140]\t\nTraining Accuracy: 86.37%\n0.30582511971183046\nTrain: [77][10/140]\t\nTrain: [77][20/140]\t\nTrain: [77][30/140]\t\nTrain: [77][40/140]\t\nTrain: [77][50/140]\t\nTrain: [77][60/140]\t\nTrain: [77][70/140]\t\nTrain: [77][80/140]\t\nTrain: [77][90/140]\t\nTrain: [77][100/140]\t\nTrain: [77][110/140]\t\nTrain: [77][120/140]\t\nTrain: [77][130/140]\t\nTrain: [77][140/140]\t\nTraining Accuracy: 86.51%\n0.30523679779396945\nTrain: [78][10/140]\t\nTrain: [78][20/140]\t\nTrain: [78][30/140]\t\nTrain: [78][40/140]\t\nTrain: [78][50/140]\t\nTrain: [78][60/140]\t\nTrain: [78][70/140]\t\nTrain: [78][80/140]\t\nTrain: [78][90/140]\t\nTrain: [78][100/140]\t\nTrain: [78][110/140]\t\nTrain: [78][120/140]\t\nTrain: [78][130/140]\t\nTrain: [78][140/140]\t\nTraining Accuracy: 86.66%\n0.3028974297343284\nTrain: [79][10/140]\t\nTrain: [79][20/140]\t\nTrain: [79][30/140]\t\nTrain: [79][40/140]\t\nTrain: [79][50/140]\t\nTrain: [79][60/140]\t\nTrain: [79][70/140]\t\nTrain: [79][80/140]\t\nTrain: [79][90/140]\t\nTrain: [79][100/140]\t\nTrain: [79][110/140]\t\nTrain: [79][120/140]\t\nTrain: [79][130/140]\t\nTrain: [79][140/140]\t\nTraining Accuracy: 86.58%\n0.3030184155263792\nTrain: [80][10/140]\t\nTrain: [80][20/140]\t\nTrain: [80][30/140]\t\nTrain: [80][40/140]\t\nTrain: [80][50/140]\t\nTrain: [80][60/140]\t\nTrain: [80][70/140]\t\nTrain: [80][80/140]\t\nTrain: [80][90/140]\t\nTrain: [80][100/140]\t\nTrain: [80][110/140]\t\nTrain: [80][120/140]\t\nTrain: [80][130/140]\t\nTrain: [80][140/140]\t\nTraining Accuracy: 86.57%\n0.30549568735424854\nTrain: [81][10/140]\t\nTrain: [81][20/140]\t\nTrain: [81][30/140]\t\nTrain: [81][40/140]\t\nTrain: [81][50/140]\t\nTrain: [81][60/140]\t\nTrain: [81][70/140]\t\nTrain: [81][80/140]\t\nTrain: [81][90/140]\t\nTrain: [81][100/140]\t\nTrain: [81][110/140]\t\nTrain: [81][120/140]\t\nTrain: [81][130/140]\t\nTrain: [81][140/140]\t\nTraining Accuracy: 86.98%\n0.2963073363731467\nTrain: [82][10/140]\t\nTrain: [82][20/140]\t\nTrain: [82][30/140]\t\nTrain: [82][40/140]\t\nTrain: [82][50/140]\t\nTrain: [82][60/140]\t\nTrain: [82][70/140]\t\nTrain: [82][80/140]\t\nTrain: [82][90/140]\t\nTrain: [82][100/140]\t\nTrain: [82][110/140]\t\nTrain: [82][120/140]\t\nTrain: [82][130/140]\t\nTrain: [82][140/140]\t\nTraining Accuracy: 86.65%\n0.3006889842972443\nTrain: [83][10/140]\t\nTrain: [83][20/140]\t\nTrain: [83][30/140]\t\nTrain: [83][40/140]\t\nTrain: [83][50/140]\t\nTrain: [83][60/140]\t\nTrain: [83][70/140]\t\nTrain: [83][80/140]\t\nTrain: [83][90/140]\t\nTrain: [83][100/140]\t\nTrain: [83][110/140]\t\nTrain: [83][120/140]\t\nTrain: [83][130/140]\t\nTrain: [83][140/140]\t\nTraining Accuracy: 86.78%\n0.29847217036269874\nTrain: [84][10/140]\t\nTrain: [84][20/140]\t\nTrain: [84][30/140]\t\nTrain: [84][40/140]\t\nTrain: [84][50/140]\t\nTrain: [84][60/140]\t\nTrain: [84][70/140]\t\nTrain: [84][80/140]\t\nTrain: [84][90/140]\t\nTrain: [84][100/140]\t\nTrain: [84][110/140]\t\nTrain: [84][120/140]\t\nTrain: [84][130/140]\t\nTrain: [84][140/140]\t\nTraining Accuracy: 86.84%\n0.2983112254047266\nTrain: [85][10/140]\t\nTrain: [85][20/140]\t\nTrain: [85][30/140]\t\nTrain: [85][40/140]\t\nTrain: [85][50/140]\t\nTrain: [85][60/140]\t\nTrain: [85][70/140]\t\nTrain: [85][80/140]\t\nTrain: [85][90/140]\t\nTrain: [85][100/140]\t\nTrain: [85][110/140]\t\nTrain: [85][120/140]\t\nTrain: [85][130/140]\t\nTrain: [85][140/140]\t\nTraining Accuracy: 87.01%\n0.29537194228380365\nTrain: [86][10/140]\t\nTrain: [86][20/140]\t\nTrain: [86][30/140]\t\nTrain: [86][40/140]\t\nTrain: [86][50/140]\t\nTrain: [86][60/140]\t\nTrain: [86][70/140]\t\nTrain: [86][80/140]\t\nTrain: [86][90/140]\t\nTrain: [86][100/140]\t\nTrain: [86][110/140]\t\nTrain: [86][120/140]\t\nTrain: [86][130/140]\t\nTrain: [86][140/140]\t\nTraining Accuracy: 87.08%\n0.2930898538343217\nTrain: [87][10/140]\t\nTrain: [87][20/140]\t\nTrain: [87][30/140]\t\nTrain: [87][40/140]\t\nTrain: [87][50/140]\t\nTrain: [87][60/140]\t\nTrain: [87][70/140]\t\nTrain: [87][80/140]\t\nTrain: [87][90/140]\t\nTrain: [87][100/140]\t\nTrain: [87][110/140]\t\nTrain: [87][120/140]\t\nTrain: [87][130/140]\t\nTrain: [87][140/140]\t\nTraining Accuracy: 87.09%\n0.29179610479368834\nTrain: [88][10/140]\t\nTrain: [88][20/140]\t\nTrain: [88][30/140]\t\nTrain: [88][40/140]\t\nTrain: [88][50/140]\t\nTrain: [88][60/140]\t\nTrain: [88][70/140]\t\nTrain: [88][80/140]\t\nTrain: [88][90/140]\t\nTrain: [88][100/140]\t\nTrain: [88][110/140]\t\nTrain: [88][120/140]\t\nTrain: [88][130/140]\t\nTrain: [88][140/140]\t\nTraining Accuracy: 87.25%\n0.29165583135404693\nTrain: [89][10/140]\t\nTrain: [89][20/140]\t\nTrain: [89][30/140]\t\nTrain: [89][40/140]\t\nTrain: [89][50/140]\t\nTrain: [89][60/140]\t\nTrain: [89][70/140]\t\nTrain: [89][80/140]\t\nTrain: [89][90/140]\t\nTrain: [89][100/140]\t\nTrain: [89][110/140]\t\nTrain: [89][120/140]\t\nTrain: [89][130/140]\t\nTrain: [89][140/140]\t\nTraining Accuracy: 87.01%\n0.2947540673454167\nTrain: [90][10/140]\t\nTrain: [90][20/140]\t\nTrain: [90][30/140]\t\nTrain: [90][40/140]\t\nTrain: [90][50/140]\t\nTrain: [90][60/140]\t\nTrain: [90][70/140]\t\nTrain: [90][80/140]\t\nTrain: [90][90/140]\t\nTrain: [90][100/140]\t\nTrain: [90][110/140]\t\nTrain: [90][120/140]\t\nTrain: [90][130/140]\t\nTrain: [90][140/140]\t\nTraining Accuracy: 87.07%\n0.2936494867522649\nTrain: [91][10/140]\t\nTrain: [91][20/140]\t\nTrain: [91][30/140]\t\nTrain: [91][40/140]\t\nTrain: [91][50/140]\t\nTrain: [91][60/140]\t\nTrain: [91][70/140]\t\nTrain: [91][80/140]\t\nTrain: [91][90/140]\t\nTrain: [91][100/140]\t\nTrain: [91][110/140]\t\nTrain: [91][120/140]\t\nTrain: [91][130/140]\t\nTrain: [91][140/140]\t\nTraining Accuracy: 87.12%\n0.2888973883519266\nTrain: [92][10/140]\t\nTrain: [92][20/140]\t\nTrain: [92][30/140]\t\nTrain: [92][40/140]\t\nTrain: [92][50/140]\t\nTrain: [92][60/140]\t\nTrain: [92][70/140]\t\nTrain: [92][80/140]\t\nTrain: [92][90/140]\t\nTrain: [92][100/140]\t\nTrain: [92][110/140]\t\nTrain: [92][120/140]\t\nTrain: [92][130/140]\t\nTrain: [92][140/140]\t\nTraining Accuracy: 87.24%\n0.2895671892456892\nTrain: [93][10/140]\t\nTrain: [93][20/140]\t\nTrain: [93][30/140]\t\nTrain: [93][40/140]\t\nTrain: [93][50/140]\t\nTrain: [93][60/140]\t\nTrain: [93][70/140]\t\nTrain: [93][80/140]\t\nTrain: [93][90/140]\t\nTrain: [93][100/140]\t\nTrain: [93][110/140]\t\nTrain: [93][120/140]\t\nTrain: [93][130/140]\t\nTrain: [93][140/140]\t\nTraining Accuracy: 87.24%\n0.2906694112340992\nTrain: [94][10/140]\t\nTrain: [94][20/140]\t\nTrain: [94][30/140]\t\nTrain: [94][40/140]\t\nTrain: [94][50/140]\t\nTrain: [94][60/140]\t\nTrain: [94][70/140]\t\nTrain: [94][80/140]\t\nTrain: [94][90/140]\t\nTrain: [94][100/140]\t\nTrain: [94][110/140]\t\nTrain: [94][120/140]\t\nTrain: [94][130/140]\t\nTrain: [94][140/140]\t\nTraining Accuracy: 87.27%\n0.29001658523701584\nTrain: [95][10/140]\t\nTrain: [95][20/140]\t\nTrain: [95][30/140]\t\nTrain: [95][40/140]\t\nTrain: [95][50/140]\t\nTrain: [95][60/140]\t\nTrain: [95][70/140]\t\nTrain: [95][80/140]\t\nTrain: [95][90/140]\t\nTrain: [95][100/140]\t\nTrain: [95][110/140]\t\nTrain: [95][120/140]\t\nTrain: [95][130/140]\t\nTrain: [95][140/140]\t\nTraining Accuracy: 87.38%\n0.2868212462805293\nTrain: [96][10/140]\t\nTrain: [96][20/140]\t\nTrain: [96][30/140]\t\nTrain: [96][40/140]\t\nTrain: [96][50/140]\t\nTrain: [96][60/140]\t\nTrain: [96][70/140]\t\nTrain: [96][80/140]\t\nTrain: [96][90/140]\t\nTrain: [96][100/140]\t\nTrain: [96][110/140]\t\nTrain: [96][120/140]\t\nTrain: [96][130/140]\t\nTrain: [96][140/140]\t\nTraining Accuracy: 87.39%\n0.2855839289209671\nTrain: [97][10/140]\t\nTrain: [97][20/140]\t\nTrain: [97][30/140]\t\nTrain: [97][40/140]\t\nTrain: [97][50/140]\t\nTrain: [97][60/140]\t\nTrain: [97][70/140]\t\nTrain: [97][80/140]\t\nTrain: [97][90/140]\t\nTrain: [97][100/140]\t\nTrain: [97][110/140]\t\nTrain: [97][120/140]\t\nTrain: [97][130/140]\t\nTrain: [97][140/140]\t\nTraining Accuracy: 87.53%\n0.28448920104295033\nTrain: [98][10/140]\t\nTrain: [98][20/140]\t\nTrain: [98][30/140]\t\nTrain: [98][40/140]\t\nTrain: [98][50/140]\t\nTrain: [98][60/140]\t\nTrain: [98][70/140]\t\nTrain: [98][80/140]\t\nTrain: [98][90/140]\t\nTrain: [98][100/140]\t\nTrain: [98][110/140]\t\nTrain: [98][120/140]\t\nTrain: [98][130/140]\t\nTrain: [98][140/140]\t\nTraining Accuracy: 87.27%\n0.2857745369960062\nTrain: [99][10/140]\t\nTrain: [99][20/140]\t\nTrain: [99][30/140]\t\nTrain: [99][40/140]\t\nTrain: [99][50/140]\t\nTrain: [99][60/140]\t\nTrain: [99][70/140]\t\nTrain: [99][80/140]\t\nTrain: [99][90/140]\t\nTrain: [99][100/140]\t\nTrain: [99][110/140]\t\nTrain: [99][120/140]\t\nTrain: [99][130/140]\t\nTrain: [99][140/140]\t\nTraining Accuracy: 87.61%\n0.2828715550776802\nTrain: [100][10/140]\t\nTrain: [100][20/140]\t\nTrain: [100][30/140]\t\nTrain: [100][40/140]\t\nTrain: [100][50/140]\t\nTrain: [100][60/140]\t\nTrain: [100][70/140]\t\nTrain: [100][80/140]\t\nTrain: [100][90/140]\t\nTrain: [100][100/140]\t\nTrain: [100][110/140]\t\nTrain: [100][120/140]\t\nTrain: [100][130/140]\t\nTrain: [100][140/140]\t\nTraining Accuracy: 87.48%\n0.2827735163869941\n","output_type":"stream"}]},{"cell_type":"code","source":"save_file = os.path.join(opt.save_folder, 'last.pth')\nsave_model(model, optimizer, opt, opt.epochs, save_file)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T05:15:53.408733Z","iopub.execute_input":"2023-08-20T05:15:53.409167Z","iopub.status.idle":"2023-08-20T05:15:53.899509Z","shell.execute_reply.started":"2023-08-20T05:15:53.409126Z","shell.execute_reply":"2023-08-20T05:15:53.898715Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"==> Saving...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Validation\nsample_evaluation(val_loader, model, opt)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T05:15:53.901056Z","iopub.execute_input":"2023-08-20T05:15:53.901421Z","iopub.status.idle":"2023-08-20T05:16:03.967760Z","shell.execute_reply.started":"2023-08-20T05:15:53.901382Z","shell.execute_reply":"2023-08-20T05:16:03.966825Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"81.98581560283688 %\n0.758597110500522\n","output_type":"stream"}]},{"cell_type":"code","source":"submission_generate(test_loader, model, opt)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T05:16:03.969336Z","iopub.execute_input":"2023-08-20T05:16:03.969684Z","iopub.status.idle":"2023-08-20T05:17:12.366728Z","shell.execute_reply.started":"2023-08-20T05:16:03.969650Z","shell.execute_reply":"2023-08-20T05:17:12.365731Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"output = np.load('/kaggle/working/output.npy')\nsubmission = pd.read_csv(\"/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv\")\nsubmission.iloc[:, 1:] = output\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T05:17:12.368276Z","iopub.execute_input":"2023-08-20T05:17:12.368631Z","iopub.status.idle":"2023-08-20T05:17:12.435127Z","shell.execute_reply.started":"2023-08-20T05:17:12.368597Z","shell.execute_reply":"2023-08-20T05:17:12.434176Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}