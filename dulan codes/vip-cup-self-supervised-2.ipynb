{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport sys\nimport time\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport random\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-24T08:01:39.554030Z","iopub.execute_input":"2023-08-24T08:01:39.554419Z","iopub.status.idle":"2023-08-24T08:01:40.034693Z","shell.execute_reply.started":"2023-08-24T08:01:39.554383Z","shell.execute_reply":"2023-08-24T08:01:40.033692Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"\"\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""},"metadata":{}}]},{"cell_type":"code","source":"!pip install pytorch-metric-learning","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:40.137017Z","iopub.execute_input":"2023-08-24T08:01:40.137362Z","iopub.status.idle":"2023-08-24T08:01:54.551926Z","shell.execute_reply.started":"2023-08-24T08:01:40.137332Z","shell.execute_reply":"2023-08-24T08:01:54.550702Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting pytorch-metric-learning\n  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (1.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (4.65.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-metric-learning) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (1.11.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\nInstalling collected packages: pytorch-metric-learning\nSuccessfully installed pytorch-metric-learning-2.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# model.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision\n\nclass ResNet(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=2):\n        super(ResNet, self).__init__()\n        if (name == 'resnet50'):\n            self.encoder = torchvision.models.resnet50(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(2048, num_classes)\n        else:\n            self.encoder = torchvision.models.resnet18(zero_init_residual=True)\n            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n            self.encoder.fc = nn.Identity()\n            self.fc = nn.Linear(512, num_classes)\n    def forward(self, x):\n\n        return self.fc(self.encoder(x))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:54.557523Z","iopub.execute_input":"2023-08-24T08:01:54.560284Z","iopub.status.idle":"2023-08-24T08:01:58.609028Z","shell.execute_reply.started":"2023-08-24T08:01:54.560245Z","shell.execute_reply":"2023-08-24T08:01:58.607711Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Prj_Head(nn.Module):\n    def __init__(self,in_dim,feature_dim):\n        super(Prj_Head, self).__init__()\n        \n        self.g1 = nn.Sequential(nn.Linear(in_dim, 1024, bias=False),\n                               nn.BatchNorm1d(1024),\n                               nn.ReLU(inplace=True)\n                               )\n        self.g2 = nn.Sequential(nn.Linear(1024, 512, bias=False),\n                                nn.BatchNorm1d(512),\n                                nn.ReLU(inplace=True)\n                                )\n        self.g3=nn.Linear(512, feature_dim, bias=True)\n    def forward(self, x):\n        # print(x.shape)\n        x = torch.flatten(x, start_dim=1, end_dim=- 1) \n        x = self.g1(x)\n        x = self.g2(x)\n        x = self.g3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.610504Z","iopub.execute_input":"2023-08-24T08:01:58.611125Z","iopub.status.idle":"2023-08-24T08:01:58.620754Z","shell.execute_reply.started":"2023-08-24T08:01:58.611094Z","shell.execute_reply":"2023-08-24T08:01:58.619699Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Encdr(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=2):\n        super(Encdr, self).__init__()\n        self.encoder = torchvision.models.resnet50(pretrained=True, zero_init_residual=True)\n        self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n        self.encoder.fc = nn.Identity()\n        self.fc = nn.Linear(2048, 512)\n\n    def forward(self, x):\n\n        return self.fc(self.encoder(x))\n    \n    def add_feature(self):\n        self.fc1=nn.Linear(512,2)\n\n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.624203Z","iopub.execute_input":"2023-08-24T08:01:58.624483Z","iopub.status.idle":"2023-08-24T08:01:58.633330Z","shell.execute_reply.started":"2023-08-24T08:01:58.624458Z","shell.execute_reply":"2023-08-24T08:01:58.632377Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# datasets.py\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport os\n\nclass OLIVES(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        return image, bio_tensor\n\n\n\n\nclass RECOVERY(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        return image\n\n\n\nclass RECOVERY_TEST(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.img_dir + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        return image, bio_tensor\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.634671Z","iopub.execute_input":"2023-08-24T08:01:58.635203Z","iopub.status.idle":"2023-08-24T08:01:58.653999Z","shell.execute_reply.started":"2023-08-24T08:01:58.635170Z","shell.execute_reply":"2023-08-24T08:01:58.653109Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# data_preprocessing.py\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport glob\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef combine_excel(csv_dir):\n    filenames = glob.glob(csv_dir + \"/*.xlsx\")\n    outputxlsx = pd.DataFrame()\n\n    for file in filenames:\n        df = pd.concat(pd.read_excel(file, sheet_name=None), ignore_index=True, sort=False)\n        outputxlsx = outputxlsx.append(df, ignore_index=True)\n\n    outputxlsx.to_csv('test_set_labels.csv',index=False)\n\ndef analyze_dataframe(csv_dir):\n    pass\n\ndef process_images(csv_dir):\n    df = pd.read_csv(csv_dir)\n\n    for i in tqdm(range(0,len(df))):\n        path = df.iloc[i,0]\n        im = Image.open(path).convert('L')\n\n\ndef numpy_submission(sub_dir,np_dir):\n    np_file  = np.load(np_dir)\n    print(len(np_file))\n    sub_dir = pd.read_csv(sub_dir)\n    print(len(sub_dir))\n    for i in range(0,len(sub_dir)):\n        sub_dir.iloc[i,1] = np_file[i,0]\n        sub_dir.iloc[i, 2] = np_file[i, 1]\n        sub_dir.iloc[i, 3] = np_file[i, 2]\n        sub_dir.iloc[i, 4] = np_file[i, 3]\n        sub_dir.iloc[i, 5] = np_file[i, 4]\n        sub_dir.iloc[i, 6] = np_file[i, 5]\n    print(sub_dir.head())\n    sub_dir.to_csv('baseline_result.csv',index=False)\n\n\n\n    #process_images(csv_dir)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.655596Z","iopub.execute_input":"2023-08-24T08:01:58.656704Z","iopub.status.idle":"2023-08-24T08:01:58.670890Z","shell.execute_reply.started":"2023-08-24T08:01:58.656668Z","shell.execute_reply":"2023-08-24T08:01:58.669929Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\n\nimport math\nimport numpy as np\nimport torch.optim as optim\nimport os\nfrom sklearn.metrics import f1_score\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import random_split, Subset\n\nimport torch.nn as nn\ndef set_model(opt, freeze = False):\n\n\n    device = opt.device\n    model = ResNet(name=opt.model,num_classes = opt.ncls)\n    if freeze:\n        model.encoder.requires_grad_(False)\n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n\n    return model, criterion\n\n\n# model for self supervised training\n\ndef set_model_st(opt,Net):\n\n\n    device = opt.device\n    #model = Encdr(name=opt.model,num_classes = opt.ncls)\n    model = nn.Sequential(\n    Net, \n    nn.Linear(512, 1024, bias=False),\n    nn.BatchNorm1d(1024),\n    nn.ReLU(inplace=True),\n    nn.Linear(1024, 512, bias=False),\n    nn.BatchNorm1d(512),\n    nn.ReLU(inplace=True),\n    nn.Linear(512, 6, bias=True))\n    \n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n\n    return model, criterion\n\n\n\n\ndef set_loader(opt):\n    # construct data loader\n    if opt.dataset == 'OLIVES' or opt.dataset == 'RECOVERY':\n        mean = (.1706)\n        std = (.2112)\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n\n    normalize = transforms.Normalize(mean=mean, std=std)\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=224, scale=(0.2, 1.)),\n        transforms.RandomHorizontalFlip(),\n\n        transforms.RandomApply([\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n        ], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n\n    if opt.dataset =='OLIVES':\n        csv_path_train = opt.train_csv_path\n        csv_path_test = opt.test_csv_path\n        data_path_train = opt.train_image_path\n        data_path_test = opt.test_image_path\n        train_dataset = OLIVES(csv_path_train,data_path_train,transforms = train_transform)\n        unlabelled_train_dataset = RECOVERY(csv_path_unlabelled,data_path_train,transforms = val_transform)\n        test_dataset = RECOVERY(csv_path_test,data_path_test,transforms = val_transform)\n        train_dataset, val_dataset = random_split(train_dataset, [0.95, 0.05], generator=torch.Generator().manual_seed(42))\n        unlabelled_train_dataset = Subset(unlabelled_train_dataset, range(unlabel_count))\n    else:\n        raise ValueError(opt.dataset)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=opt.batch_size, shuffle=True,\n        num_workers=opt.num_workers, pin_memory=True)\n    \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n    \n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n    \n    unlabelled_train_loader = torch.utils.data.DataLoader(\n        unlabelled_train_dataset, batch_size=opt.batch_size, shuffle=True,\n        num_workers=opt.num_workers, pin_memory=True)\n\n    return train_loader, val_loader, test_loader, unlabelled_train_loader\n\n\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef adjust_learning_rate(args, optimizer, epoch):\n    lr = args.learning_rate\n    if args.cosine:\n        eta_min = lr * (args.lr_decay_rate ** 3)\n        lr = eta_min + (lr - eta_min) * (\n                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n    else:\n        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n        if steps > 0:\n            lr = lr * (args.lr_decay_rate ** steps)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n    if args.warm and epoch <= args.warm_epochs:\n        p = (batch_id + (epoch - 1) * total_batches) / \\\n            (args.warm_epochs * total_batches)\n        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n\ndef set_optimizer(opt, model):\n\n    optimizer = optim.SGD(model.parameters(),\n                          lr=opt.learning_rate,\n                          momentum=opt.momentum,\n                          weight_decay=opt.weight_decay)\n    #optimizer = torch.optim.Adam(model.parameters(), lr=opt.learning_rate)\n\n    return optimizer\n\n\ndef save_model(model, optimizer, opt, epoch, save_file):\n    print('==> Saving...')\n    state = {\n        'opt': opt,\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'epoch': epoch,\n    }\n    torch.save(state, save_file)\n    del state","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.674183Z","iopub.execute_input":"2023-08-24T08:01:58.674520Z","iopub.status.idle":"2023-08-24T08:01:58.872654Z","shell.execute_reply.started":"2023-08-24T08:01:58.674482Z","shell.execute_reply":"2023-08-24T08:01:58.871538Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#----------------------------------------------------------------------------------------------------\n# Augmentations\nfrom torchvision import transforms\nclass GaussianBlur(object):\n    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n    \"\"\"Borrowed from MoCo implementation\"\"\"\n\n    def __init__(self, sigma=[.1, 2.]):\n        self.sigma = sigma\n\n    def __call__(self, x):\n        sigma = random.uniform(self.sigma[0], self.sigma[1])\n        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n        return x\n    \nclass FixedRandomRotation:\n    \"\"\"Rotate by one of the given angles.\"\"\"\n    def __init__(self, angles):\n        self.angles = angles\n\n    def __call__(self, x):\n        angle = random.choice(self.angles)\n        return transforms.functional.rotate(x, angle)\n    \ndef torchvision_transforms(eval=False, aug=None):\n\n    trans = []\n\n    if aug[\"resize\"]:\n        trans.append(transforms.Resize(aug[\"resize\"]))\n\n    if aug[\"randcrop\"] and aug[\"scale\"] and not eval:\n        trans.append(transforms.RandomResizedCrop(aug[\"randcrop\"], scale=aug[\"scale\"]))\n\n    if aug[\"randcrop\"] and eval:\n        trans.append(transforms.CenterCrop(aug[\"randcrop\"]))\n\n    if aug[\"flip\"] and not eval:\n        trans.append(transforms.RandomHorizontalFlip(p=0.5))\n        trans.append(transforms.RandomVerticalFlip(p=0.5))\n\n    if aug[\"jitter_d\"] and not eval:\n        trans.append(transforms.RandomApply(\n            [transforms.ColorJitter(0.8*aug[\"jitter_d\"], 0.8*aug[\"jitter_d\"], 0.8*aug[\"jitter_d\"], 0.2*aug[\"jitter_d\"])],\n             p=aug[\"jitter_p\"]))\n\n    if aug[\"gaussian_blur\"] and not eval:\n        trans.append(transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1,.2))], p=aug[\"gaussian_blur\"]))\n\n    if aug[\"rotation\"] and not eval:\n        # rotation_transform = FixedRandomRotation(angles=[0, 90, 180, 270])\n        trans.append(FixedRandomRotation(angles=[0, 90, 180, 270]))\n\n\n    trans = transforms.Compose(trans)\n   \n    return trans\naug = {\"resize\":0,\n    \"randcrop\":224,\n      \"scale\": (0.25, 1.0),\n      \"flip\":0,\n      \"jitter_d\":0.3,\n       \"jitter_p\":0.3,\n       \"gaussian_blur\":0.5,\n       \"rotation\":1\n      }\naugmentations = torchvision_transforms(aug = aug)\nprint(augmentations)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.874385Z","iopub.execute_input":"2023-08-24T08:01:58.874951Z","iopub.status.idle":"2023-08-24T08:01:58.892538Z","shell.execute_reply.started":"2023-08-24T08:01:58.874917Z","shell.execute_reply":"2023-08-24T08:01:58.891378Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Compose(\n    RandomResizedCrop(size=(224, 224), scale=(0.25, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=warn)\n    RandomApply(\n    p=0.3\n    ColorJitter(brightness=(0.76, 1.24), contrast=(0.76, 1.24), saturation=(0.76, 1.24), hue=(-0.06, 0.06))\n)\n    RandomApply(\n    p=0.5\n    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.2))\n)\n    <__main__.FixedRandomRotation object at 0x7ac7209ec160>\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# config.py\n\nimport argparse\nimport math\nimport os\n\ndef parse_option(string):\n    parser = argparse.ArgumentParser('argument for training')\n\n    parser.add_argument('--print_freq', type=int, default=10,\n                        help='print frequency')\n    parser.add_argument('--save_freq', type=int, default=50,\n                        help='save frequency')\n    parser.add_argument('--batch_size', type=int, default=128,\n                        help='batch_size')\n    parser.add_argument('--num_workers', type=int, default=8,\n                        help='num of workers to use')\n    parser.add_argument('--epochs', type=int, default=100,\n                        help='number of training epochs')\n    parser.add_argument('--device', type=str, default='cuda:0')\n    # optimization\n    parser.add_argument('--learning_rate', type=float, default=0.05,\n                        help='learning rate')\n    parser.add_argument('--patient_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--cluster_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--lr_decay_epochs', type=str, default='100',\n                        help='where to decay lr, can be a list')\n    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n                        help='decay rate for learning rate')\n    parser.add_argument('--weight_decay', type=float, default=1e-4,\n                        help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='momentum')\n    parser.add_argument('--train_csv_path', type=str, default='train data csv')\n    parser.add_argument('--test_csv_path', type=str, default='test data csv')\n    parser.add_argument('--train_image_path', type=str, default='train data csv')\n    parser.add_argument('--test_image_path', type=str, default='test data csv')\n\n    parser.add_argument('--parallel', type=int, default=1, help='data parallel')\n    parser.add_argument('--ncls', type=int, default=6, help='Number of Classes')\n    # model dataset\n    parser.add_argument('--model', type=str, default='resnet50')\n    parser.add_argument('--dataset', type=str, default='TREX_DME',\n                        choices=[ 'OLIVES'], help='dataset')\n    parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n    parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n    parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n    parser.add_argument('--size', type=int, default=128, help='parameter for RandomResizedCrop')\n\n    # temperature\n    parser.add_argument('--temp', type=float, default=0.07,\n                        help='temperature for loss function')\n\n\n\n    opt = parser.parse_args(string)\n\n    # check if dataset is path that passed required arguments\n    if opt.dataset == 'path':\n        assert opt.data_folder is not None \\\n               and opt.mean is not None \\\n               and opt.std is not None\n\n    # set the path according to the environment\n    if opt.data_folder is None:\n        opt.data_folder = './datasets/'\n    opt.model_path = './save/{}_models'.format(opt.dataset)\n\n    iterations = opt.lr_decay_epochs.split(',')\n    opt.lr_decay_epochs = list([])\n    for it in iterations:\n        opt.lr_decay_epochs.append(int(it))\n\n    opt.model_name = '{}_lr_{}_decay_{}_bsz_{}_temp_{}'. \\\n        format(opt.model, opt.learning_rate,\n               opt.weight_decay, opt.batch_size, opt.temp)\n\n\n    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n    if not os.path.isdir(opt.save_folder):\n        os.makedirs(opt.save_folder)\n\n    return opt","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.894829Z","iopub.execute_input":"2023-08-24T08:01:58.895215Z","iopub.status.idle":"2023-08-24T08:01:58.914665Z","shell.execute_reply.started":"2023-08-24T08:01:58.895183Z","shell.execute_reply":"2023-08-24T08:01:58.913717Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from pytorch_metric_learning.losses import NTXentLoss\nss_loss_func = NTXentLoss(temperature=0.10)\n\ndef train_ss(Net,projection_head,data_loader, epoch):\n    Net.train()\n    projection_head.train()\n    total_loss = AverageMeter()\n    for idx, x in enumerate(data_loader): \n        # print(batch_idx)\n        optimizer.zero_grad()\n        x = x.to(device)\n        # Get data representations\n        \n        A1 = augmentations(x)\n        A2 = augmentations(x)\n        \n        h1 = Net(A1)\n        z1 = projection_head(h1)\n        \n        h2 = Net(A2)\n        z2 = projection_head(h2)\n        \n        # Prepare for loss\n        embeddings = torch.cat((z1, z2))\n        # The same index corresponds to a positive pair\n        indices = torch.arange(0, z1.size(0), device=z2.device)\n        labels = torch.cat((indices, indices))\n        loss = ss_loss_func(embeddings, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss.update(loss.data.item())\n        \n        # print info\n        if (idx + 1) % 50 == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(\n                epoch, idx + 1, len(data_loader)))\n            \n    return total_loss.avg","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.918580Z","iopub.execute_input":"2023-08-24T08:01:58.918857Z","iopub.status.idle":"2023-08-24T08:01:58.949513Z","shell.execute_reply.started":"2023-08-24T08:01:58.918833Z","shell.execute_reply":"2023-08-24T08:01:58.948575Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train_supervised(train_loader, val_loader, model,criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    device = opt.device\n    end = time.time()\n    correct_predictions = 0\n\n    for idx, (image, bio_tensor) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        images = image.to(device)\n\n        labels = bio_tensor.float()\n\n        labels = labels.to(device)\n        bsz = labels.shape[0]\n\n        # compute loss\n        output = model(images)\n        loss = criterion(output, labels)\n        \n        # Calculate training accuracy\n        predicted_labels = torch.round(torch.sigmoid(output)) \n        correct_predictions += (predicted_labels == labels).sum().item()\n\n        # update metric\n        losses.update(loss.item(), bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % 10 == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(\n                epoch, idx + 1, len(train_loader)))\n\n            sys.stdout.flush()\n\n    total_values = len(train_loader.dataset) * 6\n    training_accuracy = (correct_predictions / total_values) * 100.0\n    print(f\"Training Accuracy: {training_accuracy:.2f}%\")\n    print(losses.avg)\n    print(sample_evaluation_acc(val_loader, model, opt))\n    \n    return losses.avg\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.950932Z","iopub.execute_input":"2023-08-24T08:01:58.951261Z","iopub.status.idle":"2023-08-24T08:01:58.962136Z","shell.execute_reply.started":"2023-08-24T08:01:58.951218Z","shell.execute_reply":"2023-08-24T08:01:58.961100Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def submission_generate(val_loader, model, opt, epoch = 0):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    with torch.no_grad():\n        for idx, image in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n\n            # forward\n            output = model(images)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n\n\n    out_submisison = np.array(out_list)\n    np.save('output',out_submisison)\n    \n    output = np.load('/kaggle/working/output.npy')\n    submission = pd.read_csv(\"/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv\")\n    submission.iloc[:, 1:] = output\n    submission.to_csv(f\"/kaggle/working/submission{epoch}.csv\", index = False)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.963687Z","iopub.execute_input":"2023-08-24T08:01:58.964045Z","iopub.status.idle":"2023-08-24T08:01:58.977103Z","shell.execute_reply.started":"2023-08-24T08:01:58.964012Z","shell.execute_reply":"2023-08-24T08:01:58.976103Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def sample_evaluation(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    label_list = []\n    correct_count = 0\n    total_count = 0\n\n    with torch.no_grad():\n        for idx, (image,bio_tensor) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            labels = bio_tensor.float().to(device)\n\n            labels = labels.float()\n\n            label_list.append(labels.squeeze().detach().cpu().numpy())\n            # forward\n            output = model(images)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n            \n            correct_count += (labels == output).sum().item()\n            total_count += len(labels) * 6\n        \n    print((correct_count / total_count) * 100, \"%\")\n\n    label_array = np.array(label_list)\n    out_array = np.array(out_list)\n    f = f1_score(label_array,out_array,average='macro')\n    print(f)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.978455Z","iopub.execute_input":"2023-08-24T08:01:58.978811Z","iopub.status.idle":"2023-08-24T08:01:58.991486Z","shell.execute_reply.started":"2023-08-24T08:01:58.978778Z","shell.execute_reply":"2023-08-24T08:01:58.990480Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def sample_evaluation_acc(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    label_list = []\n    correct_count = 0\n    total_count = 0\n\n    with torch.no_grad():\n        for idx, (image,bio_tensor) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            labels = bio_tensor.float().to(device)\n\n            labels = labels.float()\n\n            #label_list.append(labels.squeeze().detach().cpu().numpy())\n            # forward\n            output = model(images)\n            output = torch.round(torch.sigmoid(output))\n            #out_list.append(output.squeeze().detach().cpu().numpy())\n            \n            correct_count += (labels == output).sum().item()\n            total_count += len(labels) * 6\n        \n    print((correct_count / total_count) * 100, \"%\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:58.994380Z","iopub.execute_input":"2023-08-24T08:01:58.994665Z","iopub.status.idle":"2023-08-24T08:01:59.004926Z","shell.execute_reply.started":"2023-08-24T08:01:58.994640Z","shell.execute_reply":"2023-08-24T08:01:59.003978Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def load_model(model, load_file, key = 'model'):\n    print('==> Loading...')\n    checkpoint = torch.load(load_file)\n    model.load_state_dict(checkpoint[key])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:59.006472Z","iopub.execute_input":"2023-08-24T08:01:59.007048Z","iopub.status.idle":"2023-08-24T08:01:59.019816Z","shell.execute_reply.started":"2023-08-24T08:01:59.007013Z","shell.execute_reply":"2023-08-24T08:01:59.018892Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"if not os.path.isdir('/kaggle/working/supervised'):\n    os.makedirs('/kaggle/working/supervised')\nif not os.path.isdir('/kaggle/working/unsupervised'):\n    os.makedirs('/kaggle/working/unsupervised')","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:59.020969Z","iopub.execute_input":"2023-08-24T08:01:59.021285Z","iopub.status.idle":"2023-08-24T08:01:59.030141Z","shell.execute_reply.started":"2023-08-24T08:01:59.021262Z","shell.execute_reply":"2023-08-24T08:01:59.029138Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"args = args = ['--batch_size', '64', '--model', \"resnet50\", '--dataset', 'OLIVES', '--epochs', '2', '--device', 'cuda:0', '--train_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TRAIN/OLIVES', '--test_image_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/', '--test_csv_path', '/kaggle/input/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv', '--train_csv_path', '/kaggle/input/olives-training-labels/Training_Biomarker_Data.csv']\nopt = parse_option(args)\ncsv_path_unlabelled = \"/kaggle/input/olives-training-labels/unlabelled_images.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:01:59.031405Z","iopub.execute_input":"2023-08-24T08:01:59.031804Z","iopub.status.idle":"2023-08-24T08:01:59.044120Z","shell.execute_reply.started":"2023-08-24T08:01:59.031771Z","shell.execute_reply":"2023-08-24T08:01:59.043252Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:51:29.41341Z","iopub.execute_input":"2023-08-24T05:51:29.413892Z","iopub.status.idle":"2023-08-24T05:51:29.421173Z","shell.execute_reply.started":"2023-08-24T05:51:29.413853Z","shell.execute_reply":"2023-08-24T05:51:29.419924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build data loader\nunlabel_count = 20000\ntrain_loader, val_loader, test_loader, unlabelled_train_loader = set_loader(opt)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:02:08.432902Z","iopub.execute_input":"2023-08-24T08:02:08.433273Z","iopub.status.idle":"2023-08-24T08:02:08.605076Z","shell.execute_reply.started":"2023-08-24T08:02:08.433233Z","shell.execute_reply":"2023-08-24T08:02:08.604138Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"len(unlabelled_train_loader)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:02:09.670601Z","iopub.execute_input":"2023-08-24T08:02:09.671268Z","iopub.status.idle":"2023-08-24T08:02:09.678492Z","shell.execute_reply.started":"2023-08-24T08:02:09.671225Z","shell.execute_reply":"2023-08-24T08:02:09.676845Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"313"},"metadata":{}}]},{"cell_type":"code","source":"# build optimizer\n#optimizer = set_optimizer(opt, model)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:02:10.994553Z","iopub.execute_input":"2023-08-24T08:02:10.994926Z","iopub.status.idle":"2023-08-24T08:02:10.999614Z","shell.execute_reply.started":"2023-08-24T08:02:10.994894Z","shell.execute_reply":"2023-08-24T08:02:10.998504Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"##### UNSUPERVISED LEARNING PART","metadata":{"execution":{"iopub.status.busy":"2023-08-24T05:59:48.120283Z","iopub.execute_input":"2023-08-24T05:59:48.120967Z","iopub.status.idle":"2023-08-24T05:59:48.12549Z","shell.execute_reply.started":"2023-08-24T05:59:48.120932Z","shell.execute_reply":"2023-08-24T05:59:48.124588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--------------------------------------------------------------------------\ndevice = torch.device(\"cuda:0\" )\nNet = Encdr().to(device)\nprojection_head = Prj_Head(512,128).to(device)\n\noptimizer = torch.optim.Adam(list(Net.parameters())+list(projection_head.parameters()), lr=0.001)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n\n#train(Net,projection_head,train_loader)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:02:14.990248Z","iopub.execute_input":"2023-08-24T08:02:14.990606Z","iopub.status.idle":"2023-08-24T08:02:20.028439Z","shell.execute_reply.started":"2023-08-24T08:02:14.990575Z","shell.execute_reply":"2023-08-24T08:02:20.027396Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:01<00:00, 83.7MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"n_epoch = 2\nloss_list = []\nbest_loss = 10000\nsave_file = os.path.join(opt.save_folder + 'models', 'last.pth')\n#save_model(model, optimizer, opt, opt.epochs, save_file)\n\nfor epoch in range(1, n_epoch):\n    loss = train_ss(Net,projection_head,unlabelled_train_loader, epoch)\n    print(f'Epoch {epoch:3d}, Loss: {loss:.4f}')\n    scheduler.step()\n    loss_list.append(loss)\n    if loss<best_loss:\n        best_loss = loss\n        #torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_best_loss.pth.tar') \n        save_model(Net, optimizer, opt, opt.epochs, '/kaggle/working/unsupervised/best_loss.pth')\n    elif epoch % 10 == 0:\n        #best_loss = loss\n        #torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_checkpoint.pth.tar') \n        save_model(Net, optimizer, opt, opt.epochs, f'/kaggle/working/unsupervised/epoch{epoch}.pth')\n#torch.save(Net, f'./model_checkpoints/simclr_ki67/resnet50_ki67_dapi_chan_pretraining_final_checkpoint.pth.tar') \nplt.figure()\nplt.plot(loss_list)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:02:47.673212Z","iopub.execute_input":"2023-08-24T08:02:47.673573Z","iopub.status.idle":"2023-08-24T08:06:07.028720Z","shell.execute_reply.started":"2023-08-24T08:02:47.673542Z","shell.execute_reply":"2023-08-24T08:06:07.027688Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Train: [1][50/313]\t\nTrain: [1][100/313]\t\nTrain: [1][150/313]\t\nTrain: [1][200/313]\t\nTrain: [1][250/313]\t\nTrain: [1][300/313]\t\nEpoch   1, Loss: 2.6596\n==> Saving...\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7ac67ea578e0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"###### SUPERVISED LEARNING PART","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If loading pretrained\nload_file = \"/kaggle/input/models/pretrain_unsupervised.pth\"  # change to model path\ndevice = torch.device(\"cuda:0\" )\nNet = Encdr().to(device)\nNet = load_model(Net, load_file, \"model\")","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:11:46.517244Z","iopub.execute_input":"2023-08-24T08:11:46.517641Z","iopub.status.idle":"2023-08-24T08:11:51.377121Z","shell.execute_reply.started":"2023-08-24T08:11:46.517608Z","shell.execute_reply":"2023-08-24T08:11:51.374809Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"==> Loading...\n","output_type":"stream"}]},{"cell_type":"code","source":"#Net.add_feature()\nmodel, criterion = set_model_st(opt, Net)    \noptimizer = set_optimizer(opt, model)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:11:51.383439Z","iopub.execute_input":"2023-08-24T08:11:51.384337Z","iopub.status.idle":"2023-08-24T08:11:51.403364Z","shell.execute_reply.started":"2023-08-24T08:11:51.384297Z","shell.execute_reply":"2023-08-24T08:11:51.402480Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# To check performance without the pre training\nnew_model, new_criterion = set_model(opt)    \nnew_optimizer = set_optimizer(opt, new_model)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:11:51.404587Z","iopub.execute_input":"2023-08-24T08:11:51.405034Z","iopub.status.idle":"2023-08-24T08:11:51.786171Z","shell.execute_reply.started":"2023-08-24T08:11:51.404998Z","shell.execute_reply":"2023-08-24T08:11:51.785180Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-23T05:41:18.145796Z","iopub.execute_input":"2023-08-23T05:41:18.146193Z","iopub.status.idle":"2023-08-23T05:41:18.157971Z","shell.execute_reply.started":"2023-08-23T05:41:18.14616Z","shell.execute_reply":"2023-08-23T05:41:18.156882Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training routine with freezing\n#model[0].requires_grad_(False)\n#opt.learning_rate = 0.05\n#for epoch in range(1, 15+1):\n#    train_supervised(train_loader, val_loader, model, criterion, optimizer, epoch, opt)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T05:41:32.33422Z","iopub.execute_input":"2023-08-23T05:41:32.334952Z","iopub.status.idle":"2023-08-23T05:50:08.178343Z","shell.execute_reply.started":"2023-08-23T05:41:32.334917Z","shell.execute_reply":"2023-08-23T05:50:08.176759Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training routine without freezing\nmodel[0].requires_grad_(True)\nopt.learning_rate = 0.05\nfor epoch in range(1, 2+1):\n    train_supervised(train_loader, val_loader, model, criterion, optimizer, epoch, opt)\n    if epoch % 10 == 0:\n        submission_generate(test_loader, model, opt, epoch)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-24T08:12:13.885679Z","iopub.execute_input":"2023-08-24T08:12:13.886110Z","iopub.status.idle":"2023-08-24T08:15:50.575303Z","shell.execute_reply.started":"2023-08-24T08:12:13.886077Z","shell.execute_reply":"2023-08-24T08:15:50.574142Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Train: [1][10/140]\t\nTrain: [1][20/140]\t\nTrain: [1][30/140]\t\nTrain: [1][40/140]\t\nTrain: [1][50/140]\t\nTrain: [1][60/140]\t\nTrain: [1][70/140]\t\nTrain: [1][80/140]\t\nTrain: [1][90/140]\t\nTrain: [1][100/140]\t\nTrain: [1][110/140]\t\nTrain: [1][120/140]\t\nTrain: [1][130/140]\t\nTrain: [1][140/140]\t\nTraining Accuracy: 68.75%\n0.5802758005967634\n71.48936170212767 %\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Train: [2][10/140]\t\nTrain: [2][20/140]\t\nTrain: [2][30/140]\t\nTrain: [2][40/140]\t\nTrain: [2][50/140]\t\nTrain: [2][60/140]\t\nTrain: [2][70/140]\t\nTrain: [2][80/140]\t\nTrain: [2][90/140]\t\nTrain: [2][100/140]\t\nTrain: [2][110/140]\t\nTrain: [2][120/140]\t\nTrain: [2][130/140]\t\nTrain: [2][140/140]\t\nTraining Accuracy: 72.94%\n0.5223019162222533\n74.53900709219859 %\nNone\n","output_type":"stream"}]},{"cell_type":"markdown","source":"if epochs > 0 and epochs % 10 == 0:\n    submission_generate(test_loader, model, opt, epoch)","metadata":{}},{"cell_type":"code","source":"save_file = os.path.join('/kaggle/working/supervised/last.pth')\nsave_model(model, optimizer, opt, opt.epochs, save_file)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:06:37.395674Z","iopub.execute_input":"2023-08-24T06:06:37.39603Z","iopub.status.idle":"2023-08-24T06:06:37.769367Z","shell.execute_reply.started":"2023-08-24T06:06:37.396Z","shell.execute_reply":"2023-08-24T06:06:37.76829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation\nsample_evaluation(val_loader, model, opt)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:06:40.356515Z","iopub.execute_input":"2023-08-24T06:06:40.356935Z","iopub.status.idle":"2023-08-24T06:06:46.764014Z","shell.execute_reply.started":"2023-08-24T06:06:40.356903Z","shell.execute_reply":"2023-08-24T06:06:46.761788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_generate(test_loader, model, opt)","metadata":{"execution":{"iopub.status.busy":"2023-08-24T06:07:35.069497Z","iopub.execute_input":"2023-08-24T06:07:35.069891Z","iopub.status.idle":"2023-08-24T06:08:19.693679Z","shell.execute_reply.started":"2023-08-24T06:07:35.069858Z","shell.execute_reply":"2023-08-24T06:08:19.692381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}